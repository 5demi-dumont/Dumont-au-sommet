\documentclass[8pt]{book}
\input{usepackage}

\usepackage{lmodern}

\linespread{1.05}
 \geometry{
 	paperwidth=17cm,
 	paperheight=23.9cm,
 	top=2.5cm,
 	bottom=2.5cm,
 	left=1.5cm,
 	right=1.5cm,
 	headheight=14pt,
 	headsep=1cm,
 	footskip=1.5cm
 }

\usepackage[Bjornstrup]{fncychap}
\input{fonction}

\AddToShipoutPictureBG*{
	\begin{tikzpicture}[remember picture,overlay]
	\fill[red!70] 
		([xshift=0cm,yshift=0cm]current page.south west) 
		rectangle 
		([xshift=1.5cm,yshift=0cm]current page.north west);
	\end{tikzpicture}
}
\begin{document}
	\thispagestyle{empty}
	
	\setcounter{page}{0}
	%page de garde
	\begin{center}
		
		
		
		{\huge \textbf{Liste non exhaustive de classiques}}\\[0.5cm]
		{\Large \textit{Tome II : Algèbre / Probabilités}}\\[0.8cm]
		
		{\normalsize Une compilation d'exercice mathématiques\\ pour étudiants de classe préparatoire MP, PSI et PC}\\[1.5cm]
		
		
		\includegraphics[width=0.80\textwidth]{The_Combat_of_Ares_and_Athena.jpg}\\[0.2cm]
		
		\vfill
		{\large \textbf{Auteur :} Marconot Lorenzo , Esteve Arthur}\\[1cm]
		{\Large \textit{Collection Prépas – Mathématiques}}\\[0.5cm]
	\end{center}

	\vspace*{0.5cm}
	\begin{flushright}
		\includegraphics[width=0.30\textwidth]{lycee_dumont_urville-1-copie.jpg}\\
	\end{flushright}
	\newpage
	\thispagestyle{empty}
	\setcounter{page}{0}
	\vspace*{7.5cm}
	\begin{flushright}
	A M. Garcin et M. Teyssier, \\
	à Tan, la légende,\\
	à Arsinoé, Félix et Célian qui nous ont inspiré,\\
	et aux 5/2 de la promo 2024-2025. \\
	"La prépa, c'est du sang, des larmes et de la sueur".
\end{flushright}
\newpage
\thispagestyle{empty}
\setcounter{page}{0}
\begin{center}
	\Large Avant Propos\\
\end{center}
Salut à toi jeune CPGEiste, alors si aujourd’hui je me permets de te contacter, c’est pour une raison très simple : savais-tu que 95 \% des étudiants en prépa ne savent pas comment aborder un problème ? Alors, est-ce que tu veux en faire partie ? Il faut que tu te poses les bonnes questions. Est-ce que tu préfères faire pitié ou commencer très rapidement à accumuler du savoir avec moi grâce à ton téléphone et pouvoir faire partie de l’élite ? Moi, je pense que la question, elle est vite répondue.\\

Bon, en vrai sans déconner, Arthur et moi-même sommes passés par-là, on a fait 3 ans de classe préparatoire au lycée Dumont d’Urville, donc on connaît très bien la prépa et ses enjeux. On a voulu transmettre un morceau de ce que l’on pense qui vous sera utile, à l’instar de nos 5/2 (s/o Arsinoé Payet, Félix Gauci et Célian Rosello) qui ont écrit un formulaire en Chimie et en SI (que je ne saurais trop vous conseiller de lire avant chaque DS pour revoir les formules à connaître). On souhaite que soit transmis un héritage au sein de Dumont pour que chaque année, les prédécesseurs aident les suivants.\\

Déjà, à qui servent-ils ? Il est utile à tout étudiant peu importe son niveau ou son ambition. Dans cet ouvrage, on a essayé de compiler tous les exercices classiques (vus en colle, en DS ou en TD) que l’on a trouvés pertinents, car ils sont soit omniprésents aux concours, soit demandant un raisonnement qu’il est impératif d’avoir vu pour acquérir des réflexes très utiles si l’on est bloqué. Il est vrai que ce recueil d’exercices contient énormément d’exercices difficiles, mais il ne faut pas avoir peur : il y a énormément d’exercices abordables et que vous devez impérativement faire et puis, pour réussir en prépa, il faut « avoir les dents qui rayent le parquet », faut vouloir tout déchirer pour obtenir ce que l’on veut et c’est à cela que servent ces ouvrages : vous proposer une liste d’exercices qu'il faut voir (et revoir autant de fois que nécessaire) avant les concours. Ces ouvrages sont également destinés aux étudiants ne souhaitant pas faire d’études d’ingénieur et qui souhaitent aller en licence. Ils permettent d’avoir toutes les bases nécessaires pour appréhender en toute confiance les chapitres de la L3.\\

Bon, cet ouvrage est uniquement là en complément de cours : il faut d’abord assimiler le cours avec le prof pour pouvoir s’en sortir en prépa, donc on privilégie les exos du TD. On a créé ce recueil pour que les théorèmes ou résultats classiques soient recensés au même endroit.\\

P.-S. : On essaie de compléter au fur et à mesure les corrections des deux livres, cela demande un temps de dingue mais on promet de proposer un maximum de corrigés avant les concours.\\

P.-P.-S. : Ce message est destiné à tous les futurs 5/2, je souhaiterais que pendant votre temps de révision, vous envisagiez de perpétuer l’héritage pour que d’ici quelques années Dumont d’Urville soit au sommet. Je ne vous demande pas de faire un aussi gros projet, mais cela serait vraiment super de pouvoir aider les futurs élèves à relever le défi qu’est la prépa en toute confiance. 
	\newpage
	\begin{center}\large{Légende et notations}\end{center}
	Un exercice est affilié d'un certain nombre d'étoiles $\etoile{1}$ relativement à sa difficulté :\\
	\begin{itemize}
		\item $\etoile{1}$ Démonstration ou application directe du cours (le plus souvent E3A, CCINP);
		\item $\etoile{2}$ Application du cours/de méthodes usuelles (le plus souvent CCINP, Mines-Télécom);
		\item $\etoile{3}$ Application du cours/de méthodes usuelles (le plus souvent Centrale, Mines-Ponts);
		\item $\etoile{4}$ Exercice exotique/peu ou pas guidé (Centrale, Mines-Pont, X-ENS);
		\item $\etoile{5}$ Exercice exotique/peu ou pas guidé (le plus souvent X-ENS).
	\end{itemize}
	Ces étoiles suivent un code couleur indicatif du concours auquel l'exercice aurait le plus de chance d'être posé, à l'écrit ou à l'oral :
	\begin{itemize}
		\item Aucun concours spécifique $\etoile{1}$
		\item E3A/CCINP $\ccinp{1}$
		\item Mines-Télécom $\telecom{1}$
		\item Mines-Ponts/Centrale $\centraleponts{1}$
		\item X-ENS $\xens{1}$
	\end{itemize}
	Pour des exercices considérés comme des "classiques" qui tombent régulièrement, le titre de l'exercice sera \underline{souligné}.\\
	Par exemple \underline{Série harmonique :} $\ccinp{3}$ indique que l'exercice nommé "Série harmonique" est un immanquable de tous les candidats qui se préparent à E3A/CCINP.\\
	Enfin, un exercice qui utilise ou traite des notions hors du programme (de MP) sera suivi d'un label (HP).\\
	Dernière chose, il a été mis en place un système d'hyperlien permettant une navigation rapide et simple, ainsi toute la table de matière est cliquable et depuis un lien direct entre l'énoncé et sa correction l'est également à l'aide des balises \textcolor{blue}{[Enoncé]} ou \textcolor{blue}{[Corrigé]}
	\setcounter{tocdepth}{1}
	\tableofcontents
	
	\newpage
\chapter{Algèbre linéaire}
	
	Dans toute cette section $n$ désigne un entier naturel non nul.\\
	
	\section{Extension de corps \etoile{2} (HP)}
	\label{Extension de corps}
	\textcolor{blue}{\hyperref[Extension de corps corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Donner un exemple de sous-espace vectoriel réel d'un $\C$-espace vectoriel qui n'est pas un $\C$-espace vectoriel.
		\item Soit $V$ un $\C$-espace vectoriel de dimension $n$.\\
		Démontrer que $V$ est un $\R$-espace vectoriel de dimension $2n$.
		\item On considère un corps commutatif $(\mathbb M,+,\times)$ et $\K\subset\mathbb L$ deux sous-corps de $\mathbb M$. \\
		Démontrer l'équivalence des deux assertions :
		\begin{enumerate}[label=(\roman*)]
			\item $\mathbb M$ est un $\K$-espace vectoriel de dimension finie $m$;
			\item $\mathbb M$ est un $\mathbb L$-espace vectoriel de dimension finie $k$ et $\mathbb L$ est un $\K$-espace vectoriel de dimension finie $p$.
		\end{enumerate}
		Montrer que dans ces conditions $m=kp$. (relation de multiplicativité des degrés)
	\end{enumerate}
	
	\section{Passage du complexe au réel \etoile{2}}
	\label{Passage du complexe au réel}
	\textcolor{blue}{\hyperref[Passage du complexe au réel corrigé]{[Corrigé]}}\\
	Soient $V$ un $\C$-espace vectoriel de dimension $n$ et $v\in \mathcal L(V)$. On note $W$ l'espace $V$ munit de sa structure naturelle de $\R$-espace vectoriel de dimension $2n$ et on note $u\in \mathcal L(W)$ l'endomorphisme de $W$ défini comme étant égal à $v$.\\
	Montrer que $\det(w)=|\det(v)|^2$.
	
	\section{Dimension de $\R$ en tant que $\Q$-espace vectoriel \etoile{2} (HP)}
	\label{Dimension de R en tant que Q espace vectoriel }
	\textcolor{blue}{\hyperref[Dimenseion de R en tant que Q espace vectoriel corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Montrer que $\R$ est un $\Q$-espace vectoriel.
		\item Montrer que si $p_1,\dots,p_n$ sont des nombres premiers distincts alors la famille $(\ln(p_1),\dots,\ln(p_n))$ est $\Q$-libre.
		\item Quelle est la dimension de $\R$ en tant que $\Q$-espace vectoriel ?
	\end{enumerate}
	
	\section{Fonctions indépendantes \xens{3}}
	\label{Fonctions indépendantes}
	\textcolor{blue}{\hyperref[Fonctions indépendantes corrigé]{[Corrigé]}}\\
	Soient $X$ un ensemble et $(f_1,\dots,f_n)$ une famille libre de $\C^X$.\\
	Montrer qu'il existe $x_1,\dots,x_n\in X$ tels que la matrice $(f_i(x_j))_{1\leq i,j\leq n}$ soit inversible.
	
	\section{L'ordre a son importance \telecom{2}}
	\label{L'ordre a son importance}
	\textcolor{blue}{\hyperref[L'ordre a son importance corrigé]{[Corrigé]}}\\
	Soit $M\in \M_n(\R)$.\\ Montrer l'équivalence entre les affirmations :
	\begin{itemize}
		\item $\forall A,B\in\M_n(\R), \,\Tr(MAB)=\Tr(MBA)$
		\item $M$ est une matrice d'homothétie.
	\end{itemize}
	
	\section{Isomorphisme de $\M_n(\K)$ qui respecte les matrices de rang 1}
	\label{Isomorphisme de M_n(K) qui respecte les matrices de rang 1}
	\textcolor{blue}{\hyperref[Isomorphisme de M_n(K) qui respecte les matrices de rang 1 corrigé]{[Corrigé]}}\\
	Soit $f\in \T{GL}(\M_n(\K))$ telle que $\forall M\in \M_n(\K),\ \rg(M)=1\implies \rg(f(M))=1$. On note $\mathcal C=\M_{n,1}(\K),\ \L=\M_{1,n}(\K)$ et $\mathcal R=\{CL,\ (C,L)\in \mathcal C\times\L\}$. On note aussi pour $i,j\in \crblanc{1}{n},\ E_{ij}$ la matrice élémentaire, dont tous les coefficients sont nuls sauf celui en position $(i,j)$ qui vaut $1$. Enfin on note $(E_i)_{1\leq i\leq n}$ la base canonique de $\M_{n,1}(\K)$.
	\begin{enumerate}
		\item Calculer le produit $E_j^\top E_i$ pour $i,j\in \crblanc{1}{n}$.
		\item Montrer que $\mathcal R$ est l'ensemble des matrices de rang inférieur ou égal à $1$. En déduire que pour tout $i,j\in \crblanc{1}{n},\ \mathcal R=\{AE_{ij}B,\ A,B\in \T{GL}_n(\K)\}$.
		\item Soient $(C_1,C_2)\in \mathcal C$ et $(L_1,L_2)\in \L$.\\
		Montrer que $\rg(C_1L_1+C_2L_2)\geq 2$ si et seulement si $(C_1,C_2)$ et $(L_1,L_2)$ sont libres.
		\item Soit $\{0\}\ne\mathcal F\subset \mathcal R$ un sous-espace vectoriel de $\M_n(\K)$. Déduire de ce qui précède l'existence d'une colonne non nulle $C$ telle que $\mathcal F\subset C\mathcal L$ ou bien d'une ligne non nulle $L$ telle que $\mathcal F\subset \mathcal CL$.
		\item Justifier la décomposition $\M_n(\K)=\displaystyle\bigoplus_{i=1}^nE_i\L$.
		\item En déduire l'existence de deux matrices $A,B\in \T{GL}_n(\K)$ vérifiant une des deux affirmations :
		\begin{itemize}
			\item $\forall i,j\in \crblanc{1}{n},\ f(E_{ij})=AE_{ij}B$
			\item $\forall i,j\in \crblanc{1}{n},\ f(E_{ij})=AE_{ji}B$.
		\end{itemize}
		\item Déterminer $f$.
	\end{enumerate}
	
	\section{\underline{Centre de $\M_n(\K)$} \centraleponts{2}}
	\label{Centre de Mn(K)}
	\textcolor{blue}{\hyperref[Centre de Mn(K) corrigé]{[Corrigé]}}\\
	Déterminer le centre de $\M_n(\K)$ : $Z=\{A\in \M_n(\K),\ \forall B\in \M_n(\K),\ AB=BA\}$.
	
	\section{$\M_n(\K)$ est violemment non commutatif}
	\label{Mn(K) est violemment non commutatif corrigé}
	\textcolor{blue}{\hyperref[Mn(K) est violemment non commutatif]{[Corrigé]}}\\
	\begin{enumerate}
		\item Déterminer les matrices de $\M_n(\K)$ qui commutent avec toutes les matrices inversibles.
		\item Déterminer les matrices de $\M_n(\K)$ qui commutent avec toutes les matrices de projecteur.
	\end{enumerate}
	
	\section{Racine carrée de la dérivation \xens{3}}
	\label{Racine carrée de la dérivation}
	\textcolor{blue}{\hyperref[Racine carrée de la dérivation corrigé]{[Corrigé]}}\\
	On note $E=\mathcal C^\infty(\R,\R)$ et $\fonction{\Delta}{E}{E}{f}{f'}$.\\
	Existe-t-il un endomorphisme $\delta\in \mathcal L(E)$ tel que $\delta^2=\Delta$ ?
	
	\section{\underline{Produit de matrices nilpotentes} \xens{5}}
	\label{Produit de matrices nilpotentes}
	\textcolor{blue}{\hyperref[Produit de matrices nilpotentes corrigé]{[Corrigé]}}\\
	Soient $N_1,\dots,N_n\in \M_n(\K)$ des matrices nilpotentes qui commutent toutes entre elles.\\
	Montrer que $\displaystyle\prod_{i=1}^nN_i=0$.
	
	\section{Suites périodiques \xens{4}}
	\label{Suites périodiques}
	\textcolor{blue}{\hyperref[Suites périodiques corrigé]{[Corrigé]}}\\
	On note $E$ le sous-ensemble de $\C^\N$ composé des suites périodiques.\\
	Montrer que $E$ est un espace vectoriel et en déterminer une base.
	
	\section{Dimension du commutant (1) \centraleponts{3}}
	\label{Dimension du commutant (1)}
	\textcolor{blue}{\hyperref[Dimension du commutant (1) corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\C)$.\\
	On pose $\fonction{f_A}{\M_n(\C)}{\M_n(\C)}{B}{AB-BA}$ et on note $\mathcal C(A)=\{B\in \M_n(\C),\ AB=BA\}$.\\
	On cherche à montrer que la dimension de $\mathcal C(A)$ est minorée par $n$.
	\begin{enumerate}
		\item Montrer que $f_A\in \L(\M_n(\C))$ et que $\mathcal C(A)$ est un sous-espace vectoriel de $\M_n(\C)$.
		\item Montrer que l'on peut supposer $A$ triangulaire supérieure.\\
		On le supposera dans la suite.
		\item Montrer que $f_A$ induit un endomorphisme $\tilde f_A$ sur l'ensemble des matrices triangulaires supérieures.
		\item En déduire que $\dim\mathcal C(A)\geq n$.
	\end{enumerate}
	
	\section{Dimension du commutant (2) \centraleponts{4}}
	\label{Dimension du commutant (2)}
	\textcolor{blue}{\hyperref[Dimension du commutant (2) corrigé]{[Corrigé]}}\\
	Soit $u$ un endomorphisme nilpotent d'un $\K$-espace vectoriel $E$ de dimension finie.\\
	Pour $k\in \N^*$ on note $e_k(u)=\dim(\Ker(u^k))-\dim(\Ker(u^{k-1}))$.\\
	On note aussi $\mathcal C(u)=\{v\in \L(E),\ u\circ v=v\circ u\}$.
	\begin{enumerate}
		\item Soit $v\in \mathcal C(u)$. Montrer que $v$ induit un endomorphisme $\tilde v$ sur l'image de $u$.
		\item Soit $k\in \N$. Montrer que $\T{Im}(u)\cap\Ker(u^k)=u(\Ker(u^{k+1}))$ puis que $e_k(\tilde u)=e_{k+1}(u)$.
		\item Montrer que
		$$\fonction{\Phi}{\mathcal C(u)}{\mathcal C(\tilde u)}{v}{\tilde v}$$
		est une application linéaire surjective.
		\item En déduire que $\dim\mathcal C(u)=\displaystyle\sum_{k=1}^{+\infty}e_k(u)^2$.
	\end{enumerate}
	
	\section{Etude de la comatrice \centraleponts{3}}
	\label{Etude de la comatrice}
	\textcolor{blue}{\hyperref[Etude de la comatrice corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Calculer $\Com(J_r)$ pour $0\leq r\leq n$.
		\item Démontrer que $\forall (A,B)\in \M_n(\K)^2,\ \Com(AB)=\Com(A)\Com(B)$.
		\item En déduire le rang de la comatrice de $A$ en fonction du rang de $A$.
		\item Soit $\fonction{\varphi}{\M_n(\K)}{\M_n(\K)}{A}{\Com(A)}$.\\
		$\varphi$ est-elle injective ? Surjective ? \\Déterminer $\text{Im}(\varphi)$ dans le cas $\K=\C$.
	\end{enumerate}

	\section{Caractérisation des homothéties (1) \ccinp{2}}
	\label{Caractérisation des homothéties (1)}
	\textcolor{blue}{\hyperref[Caractérisation des homothéties (1) corrigé]{[Corrigé]}}\\
	Soit $u$ un endomorphisme d'un $\K$-espace vectoriel $E$ tel que pour tout $x\in E$, $(x,u(x))$ est une famille liée.\\
	Montrer que $u$ est une homothétie.
	
	\section{Caractérisation des homothéties (2) \centraleponts{2}}
	\label{Caractérisation des homothéties (2)}
	\textcolor{blue}{\hyperref[Caractérisation des homothéties (2) corrigé]{[Enoncé]}}\\
	Soit $u$ un endomorphisme d'un $\K$-espace vectoriel $E$.\\
	Montrer que si $u$ stabilise tous les sous-espaces de $E$ de dimension $r$ alors $u$ est une homothétie.
	
	\section{Espaces engendrés par les matrices inversibles et orthogonales \centraleponts{3}}
	\label{Espaces engendrés par les matrices inversibles et orthogonales}
	\textcolor{blue}{\hyperref[Espaces engendrés par les matrice inversibles et orthogonales corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Déterminer $\Vect(\text{GL}_n(\K))$.
		\item Déterminer $\Vect(\O_n(\R))$.
	\end{enumerate}
	
	\section{Espace engendré par les matrices nilpotentes \centraleponts{3}}
	\label{Espace engendré par les matrices nilpotentes}
	\textcolor{blue}{\hyperref[Espace engendré par les matrices nilpotentes corrigé]{[Corrigé]}}\\
	Montrer que l'espace vectoriel engendré par les matrice nilpotentes est l'ensemble des matrices de trace nulle.
	
	\section{Transmission d'information \centraleponts{3}}
	\label{Transmission d'information}
	\textcolor{blue}{\hyperref[Transmission d'information corrigé]{[Corrigé]}}\\
	Soient $A,B\in \M_n(\K)$ telles que $\forall X\in \M_n(\K),\ AXB=0$.\\
	Montrer que $A=0$ ou $B=0$.
	
	\section{Noyau et Image supplémentaires \ccinp{2}}
	\label{Condition nécessaire et suffisante pour que l'image et le noyau soient supplémentaires}
	\textcolor{blue}{\hyperref[Condition nécessaire et suffisante pour que l'image et le noyau soient supplémentaires corrigé]{[Corrigé]}}\\
	Soit $f$ un endomorphisme d'un espace vectoriel $E$.
\\	Montrer que les trois propositions sont équivalentes :
	\begin{itemize}
		\item $E=\text{Im}(f)\oplus \text{Ker}(f)$
		\item $\text{Im}(f^2)=\text{Im}(f)$
		\item $\text{Ker}(f^2)=\text{Ker}(f)$
	\end{itemize}
	
	\section{Equation entre projecteurs \centraleponts{3}}
	\label{Equation entre projecteurs}
	\textcolor{blue}{\hyperref[Equation entre projecteurs corrigé]{[Corrigé]}}\\
	Existe-t-il des projecteurs $p,q,r$ d'un espace euclidien $E$ non tous nuls vérifiant $\sqrt{2}p+\sqrt{3}q=r$ ?
	
	\section{Théorème de Maschke}
	\label{Théorème de Maschke}
	\textcolor{blue}{\hyperref[Théorème de Maschke corrigé]{[Corrigé]}}\\
	Soit $E$ un espace vectoriel de dimension finie et soit $G$ un sous-groupe fini de $GL(E)$ de cardinal $n$.
	\begin{enumerate}
		\item Etudions un objet mathématique courant dans l'étude des groupes dans le but de comprendre la méthode utilisé pour la suite.
		\begin{enumerate}
			\item Montrer que si $p$ est un projecteur, alors $\rg(p)=\Tr(p)$.
			\item Soit $h\in G$. Montrer que $\varphi_h:g\in G\to h\circ g$ est une permutation de $G$.
		\end{enumerate}
		\item On pose $\displaystyle p=\frac{1}{n}\sum_{g\in G}g$.
		\begin{enumerate}
			\item Montrer que $p$ est un projecteur.
			\item Montrer que $$\dim\left(\bigcap_{g\in G}\Ker(g-Id_E)\right)=\frac{1}{n}\sum_{g\in G}\Tr(g)$$
		\end{enumerate}
		
		\item Soit $F$ un sous-espace vectoriel de $E$ stable par tous les éléments de $G$ et $H$ un supplémentaire de $F$. On note $q$ le projecteur sur $F$ parallèlement à $H$.
		\\ Montrer qu'il existe un supplémentaire de $F$ stable par tous les éléments de $G$.
		\\ \underline{Indication}: on étudiera $\displaystyle \frac{1}{n}\sum_{g\in G}g^{-1}\circ q\circ g$
	\end{enumerate}
	
	\newpage
\chapter{Réduction géométrique}
	Dans toute cette section $n$ désigne un entier supérieur ou égal à $1$.
	\section{Réduction des matrices de rang $1$ \ccinp{2}}
	\label{Réduction des matrices de rang 1}
	\textcolor{blue}{\hyperref[Réduction des matrices de rang 1 corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $M\in \M_n(\K)$.
		Montrer que :\[\rg(M)=1 \iff \exists (U,V)\in (\M_{n,1}(\K)\backslash\{0\})^2,\ M=UV^\top\]
		\item Soit $M\in \M_n(\K)$ une matrice de rang $1$. Montrer que $M^2=\Tr(M)M$.
		\item Donner une condition nécessaire et suffisante pour qu'une matrice de rang $1$ soit diagonalisable.
		\item Application : Résoudre l'équation $A^3=\begin{pmatrix}
			1 &-2 &1\\
			1 &-2 &1\\
			1 &-2 &1\\
		\end{pmatrix}$.
		\item Soit $E$ un $\K$-espace vectoriel de dimension finie et soit $u\in \L(E)$ un endomorphisme de rang $1$ et de trace nulle.
		\begin{enumerate}[label=\alph*.]
			\item justifier que $\T{Im}(u)\subset\Ker(u)$.
			\item En déduire que $\T{Im}(u)\cap\Ker(u)\ne\{0\}$.
			\item Montrer qu'il existe une base de $E$ dans laquelle la matrice de $u$ est
			$$\begin{pmatrix}
				0&0&0&&&\\
				1&0&0&&(0)&\\
				0&0&0&&&\\
				&&&\ddots&&\\
				&(0)&&&\ddots&\\
				&&&&&0
			\end{pmatrix}$$
		\end{enumerate}
		\item Montrer que deux matrices de rang $1$ sont semblables si et seulement si elles ont la même trace.
	\end{enumerate}
	
	\subsection{Application \ccinp{1}}
	\label{Application}
	\textcolor{blue}{\hyperref[Application corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\K)$ non nulle.\\
	Montrer que l'application $M\in \M_n(\K)\mapsto M+\Tr(M)A$ est diagonalisable si et seulement si $\Tr(A)\ne 0$.
	
	\subsection{Dimension du commutant d'une matrice de rang 1 \centraleponts{3}}
	\label{Dimension du commutant d'une matrice de rang 1}
	\textcolor{blue}{\hyperref[Dimension du commutant d'une matrice de rang 1 corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\K)$ de rang $1$.\\
	Déterminer la dimension du commutant de $A$ : $\mathcal C(A)=\{B\in M_n(\K),\ AB=BA\}$.
	
	\subsection{Diagonalisabilité d'une matrice \centraleponts{3}}
	\label{Diagonalisabilité d'une matrice}
	\textcolor{blue}{\hyperref[Diagonalisabilité d'une matrice corrigé]{[Corrigé]}}\\
	Soient $a_1\leq \dots\leq a_n$ et $b_1>\dots>b_n>0$ des réels. On considère la matrice
	$$M=\begin{pmatrix}
		a_1   &b_2   &\dots &b_{n-1}&b_n\\
		b_1   &a_2   &\ddots&\vdots &\vdots\\
		\vdots&b_2   &\ddots&b_{n-1}&b_n\\
		\vdots&\vdots&\ddots&a_{n-1}&b_n\\
		b_1   &b_2   &\dots &b_{n-1}&a_n
	\end{pmatrix}$$
	\begin{enumerate}
		\item Exprimer $\chi_M$ en fonction des $a_i$ et des $b_i$.\\ \textit{On pourra utiliser le lemme du déterminant} (cf. VIII-50 tome Analyse)
		\item Démontrer à l'aide du TVI que $M$ est diagonalisable dans $\R$.
	\end{enumerate}
	
	\section{\underline{Matrices réelles semblables} \centraleponts{2}}
	\label{Matrices réelles semblables}
	\textcolor{blue}{\hyperref[Matrices réelles semblables corrigé]{[Corrigé]}}\\
	Soient $A$ et $B$ deux matrices de $\M_n(\R)$ semblables sur $\M_n(\C)$.\\
	Montrer que $A$ et $B$ sont semblables sur $\M_n(\R)$.
	
	\section{Complément de Schur \telecom{2}}
	\label{Complément de Schur}
	\textcolor{blue}{\hyperref[Complément de Schur corrigé]{[Corrigé]}}\\
	Soit $p,q\in \N^*$.\\
	Soit $M=\left(
	\begin{array}{c|c}
		A&B\\
		\hline
		C&D
	\end{array}\right)$
	avec $A\in GL_p(\K)$ et $D\in \M_q(\K)$.\\
	On pose $S=D-CA^{-1}B$.
	\begin{enumerate}
		\item Montrer que $\det(M)=\det(A)\det(S)$.
		\item Montrer que $\rg(M)=\rg(A)+\rg(S)$.
	\end{enumerate}
	
 	\section{Produit de Kronecker \ccinp{2}}
 	\label{Produit de Kronecker}
 	\textcolor{blue}{\hyperref[Produit de Kronecker corrigé]{[Corrigé]}}\\
	Pour $A,B\in \M_2(\K)$ on note $A\otimes B=\left(
	\begin{array}{c|c}
		a_{11}B&a_{12}B\\
		\hline
		a_{21}B&a_{22}B
	\end{array}\right)$.
	\begin{enumerate}
		\item Soit $(A,B,C,D)\in \M_2(\K)^4$. Montrer que $(A\otimes B).(C\otimes D)=(AC)\otimes (BD)$
		\item Calculer $\det{(A\otimes I_2)},\det{(I_2\otimes B)}\text{ et }\det{(A\otimes B)}$ en fonction de $\det{A}$ et $\det{B}$.
		\item A quelle condition $A\otimes B$ est-elle inversible ? Quelle est alors son inverse ?
		\item On suppose que $A$ et $B$ sont diagonalisables. Montrer que $A\otimes B$ est diagonalisable.
	\end{enumerate}
	
	\section{\underline{Disques de Gershgorin} \telecom{2}}
	\label{Disques de Gershgorin}
	\textcolor{blue}{\hyperref[Disques de Gershgorin corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $A\in \M_n(\C)$. Pour $i\in \crblanc{1}{n}$ on note $R_i=\displaystyle\sum_{j\ne i}{|a_{ij}|}$.\\ Montrer que toute valeur propre de $A$ appartient à au moins un des disques fermés de centre $a_{ii}$ et de rayon $R_i$.
		\item En déduire le lemme de Hadamard :
		Toute matrice à diagonale strictement dominante (i.e $\forall i\in \crblanc{1}{n},\ |a_{ii}|>R_i$) est inversible.
	\end{enumerate}
	
	\section{Spectre de $u\circ v$ et $v\circ u$ \ccinp{1}}
	\label{Spectre de uov et vou}
	\textcolor{blue}{\hyperref[Spectre de uov et vou corrigé]{[Corrigé]}}\\
	Soit $u$ et $v$ deux endomorphismes d'un $\K$-espace vectoriel de dimension finie.
	Montrer que $u\circ v$ et $v\circ u$ ont les mêmes valeurs propres.
	
	\section{\underline{Endomorphismes qui commutent} \centraleponts{2}}
	\label{Endomorphismes qui commutent}
	\textcolor{blue}{\hyperref[Endomorphismes qui commutent corrigé]{[Corrigé]}}\\
	Soient $E$ un $\C$-espace vectoriel de dimension finie et $(u,v)\in \mathcal{L}(E)^2$.\\
	Montrer que si $u$ et $v$ commutent, alors ils ont un vecteur propre commun.
	
	\section{Vecteur propre commun \centraleponts{3}}
	\label{Vecteur propre commun}
	\textcolor{blue}{\hyperref[Vecteur propre commun corrigé]{[Corrigé]}}\\
	Soient $E$ un $\C$-espace vectoriel de dimension finie et $(u,v)\in \mathcal{L}(E)^2$.\\ Montrer que $u$ et $v$ ont un vecteur propre commun dans chacun des cas suivant:
	\begin{enumerate}[leftmargin=*]
		\item $u\circ v=0$.
		\item $\exists a\in \C,\ u\circ v=au$.
		\item $\exists b\in \C,\ u\circ v=bv$.
		\item $u\circ v=\Id_E$.
		\item $\exists (a,b)\in \C^2,\ u\circ v=au+bv$.
	\end{enumerate}
	
	\section{Eléments propres d'un endomorphisme (1) \telecom{2}}
	\label{Eléments propres d'un endomorphismes (1)}
	\textcolor{blue}{\hyperref[Eléments propres d'un endomorphismes (1) corrigé]{[Corrigé]}}\\
	Soit $(E,\proscal{\cdot}{\cdot})$ un espace euclidien. On considère des vecteurs unitaires $a$ et $b$ formant une famille libre de $E$.\\
	Réduire l'endomorphisme $\fonction{\Phi}{E}{E}{x}{\proscal{a}{x}a+\proscal{b}{x}b}$
	
	\section{\underline{Eléments propres d'un endomorphisme} (2) \centraleponts{3}}
	\label{Eléments propres d'un endomorphismes (2)}
	\textcolor{blue}{\hyperref[Eléments propres d'un endomorphisme (2) corrigé]{[Corrigé]}}\\
	Soit $E$ l'espace des fonctions continues de $[0,1]$ dans $\R$.\\ A toute application $f\in E$ on associe l'application $$\fonction{\Phi(f)}{[0,1]}{\R}{x}{\displaystyle\int_0^1{\min{(x,t)}f(t)\ dt}}$$
	\begin{enumerate}
		\item Montrer que $\Phi$ est un endomorphisme de $E$
		\item Déterminer les éléments propres de $\Phi$
	\end{enumerate}
	
	\section{Eléments propres d'un endomorphisme (3) \centraleponts{3}}
	\label{Eléments propres d'un endomorphismes (3)}
	\textcolor{blue}{\hyperref[Eléments propres d'un endomorphisme (3) corrigé]{[Corrigé]}}\\
	Soit $\B$ le sous-espace de $\C^{\Z}$ formé des suites bornées.
	On considère : \[\fonction{T}{\B}{\B}{(u_n)_{n\in \Z}}{\displaystyle\left(\frac{u_{n-1}+u_{n+1}}{2}\right)_{n\in \Z}}\]
	\begin{enumerate}
		\item Montrer que $T\in \mathcal{L}(\B)$.
		\item Déterminer les éléments propres de $T$
	\end{enumerate}
	
	\section{\underline{Eléments propres d'un endomorphisme (4)} \centraleponts{3}}
	\label{Eléments propres d'un endomorphismes (4)}
	\textcolor{blue}{\hyperref[Eléments propres d'un endomorphisme (4) corrigé]{[Corrigé]}}\\
	Soit $E=\mathcal{C}(\R^+,\R)$.\\
	Pour $f\in E$ on définit l'application $T(f):x\in \R_+^*\longmapsto \displaystyle{\frac{1}{x}\int_0^x{f(t)\ dt}}$
	\begin{enumerate}
		\item Soit $f\in E$. Montrer que $T(f)$ est prolongeable par continuité en $0$ (on notera encore $T(f)$ le prolongement).
		\item Déterminer les éléments propres de $T$.
	\end{enumerate}
	
	\section{\underline{Loi de Hooke} \ccinp{2}}
	\label{Loi de Hooke}
	\textcolor{blue}{\hyperref[Loi de Hooke corrigé]{[Corrigé]}}\\
	Soit $\fonction{\varphi}{\M_n(\R)}{\M_n(\R)}{M}{M+\Tr(M)I_n}$. Déterminer les éléments propres de $\varphi$.
	
	\section{Existence d'une valeur propre double \centraleponts{3}}
	\label{Existence d'une valeur propre double}
	\textcolor{blue}{\hyperref[Existence d'une valeur propre double corrigé]{[Corrigé]}}\\
	Soit $(A,B,C)\in \M_2(\K)^3$.\\ Montrer qu'il existe $(\alpha,\beta,\gamma)\in (\K^*)^3$ tel que $\alpha A+\beta B+\gamma C$ admette une valeur propre double.
	
	\section{Détermination de spectre \centraleponts{3}}
	\label{Détermination de spectre}
	\textcolor{blue}{\hyperref[Détermination de spectre corrigé]{[Corrigé]}}\\
	On définit une suite de matrice par $A_1=\begin{pmatrix}1&1\\1&-1\end{pmatrix}$ et $A_{n+1}=\left(\begin{array}{c|c}
		A_n&A_n\\
		\hline
		A_n&-A_n
	\end{array}\right)$.\\
	Déterminer Sp($A_n$).
	
	\section{Sommes et produits de valeurs propres}
	\label{Sommes et produits de valeurs propres}
	\textcolor{blue}{\hyperref[Sommes et produits de valeurs propres corrigé]{[Corrigé]}}\\
	Soient $A,B\in \M_n(\C)$.
	\begin{enumerate}
		\item Déterminer le spectre des applications linéaires suivantes :
		$$\fonction{\Phi}{\M_n(\C)}{\M_n(\C)}{M}{AM+MB}\quad \fonction{\Psi}{\M_n(\C)}{\M_n(\C)}{M}{AMB}$$
		\item En déduire que l'ensemble des entiers algébriques (racines dans $\C$ d'un polynôme unitaire à coefficients entiers) est un sous-anneau de $\C$.
	\end{enumerate}
	
	\section{\underline{Matrice compagnon (1)} \ccinp{2}}
	\label{Matrice compagnon (1)}
	\textcolor{blue}{\hyperref[Matrice compagnon (1) corrigé]{[Corrigé]}}\\
	Soit $P=X^n+\displaystyle\sum\limits_{k=0}^{n-1}{a_kX^k}\in \K[X]$. On pose $C_P=
	\begin{pmatrix}
		0&\dots&\dots&0&-a_0\\
		1&\ddots&\ &\vdots&-a_1\\
		0&\ddots&\ddots&\vdots&\vdots\\
		\vdots&\ddots&\ddots&0&-a_{n-2}\\
		0&\dots&0&1&-a_{n-1}
	\end{pmatrix}$.\\
	Déterminer le polynôme caractéristique de $C_P$.
	
	\section{\underline{Réduction de la transposée d'une matrice} \ccinp{1}}
	\label{Réduction de la transposée d'une matrice}
	\textcolor{blue}{\hyperref[Réduction de la transposée d'une matrice corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\K)$.
	\begin{enumerate}
		\item Montrer que $\chi_{A}=\chi_{A^\top}$.
		\item Montrer que $\forall \lambda \in \operatorname{Sp}(A),\ \dim{E_\lambda(A)}=\dim{E_\lambda(A^\top)}$.
		\item En déduire que $A^\top$ est diagonalisable si et seulement $A$ l'est.
	\end{enumerate}
	
	\section{\underline{Algorithme de Faddeev} \centraleponts{3}}
	\label{Algorithme de Faddeev}
	\textcolor{blue}{\hyperref[Algorithme de Faddeev corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\K)$. Pour $\lambda \in \K$ on pose $B(\lambda)=\operatorname{Com}(\lambda I_n-A)^\top$.
	\begin{enumerate}[leftmargin=*]
		\item Montrer qu'il existe des matrices $B_0,\dots,B_{n-1}$ de $\M_n(\K)$ telles que:$$B(\lambda)=\sum\limits_{k=0}^{n-1}{\lambda^{n-1-k}B_k}$$
		\item Montrer que $\chi_A'(\lambda)=\Tr(B(\lambda))$ Pour tout $\lambda\in \K$.
		\item On pose $\chi_A=X^n-\sum\limits_{k=1}^n{p_kX^{n-k}}$ et $B_n=0$. Montrer que $\forall k\in \crblanc{1}{n-1}$,
		$$\begin{cases}
			p_k=\displaystyle\frac{1}{k}\Tr(AB_{k-1})\\
			B_k=AB_{k-1}-p_kI_n
		\end{cases}$$
		et préciser $B_0$.
		\item Montrer que si $A$ est inversible alors $A^{-1}=\displaystyle\frac{1}{p_n}B_{n-1}$.
		\item Ecrire un programme Python calculant le polynôme caractéristique d'une matrice $A$ et un autre calculant son inverse à l'aide des questions précédentes.
	\end{enumerate}
	
	\section{\underline{Endomorphisme de transposition} \ccinp{2}}
	\label{Endomorphisme de transposition}
	\textcolor{blue}{\hyperref[Endomorphisme de transposition corrigé]{[Corrigé]}}\\
	On pose $\fonction{\Phi}{\M_n(\R)}{\M_n(\R)}{M}{M^\top}$.\\
	Calculer $\det(\Phi)$ et $\Tr(\Phi)$.
	
	\section{\underline{Exemple de matrice non diagonalisable} \ccinp{1}}
	\label{Exemple de matrice non diagonalisable}
	\textcolor{blue}{\hyperref[Exemple de matrice non diagonalisable corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Donner un exemple de matrice non diagonalisable sur $\C$.
		\item Donner un exemple de matrice diagonalisable sur $\C$ mais pas sur $\R$.
		\item La somme de deux matrices diagonalisables est-il diagonalisable ?
		\item Le produit de deux matrices diagonalisables est-il diagonalisable ?
	\end{enumerate}
	
	\section{Diagonalisabilité d'une matrice (1) \centraleponts{3}}
	\label{Diagonalisabité d'une matrice (1)}
	\textcolor{blue}{\hyperref[Diagonalisabilité d'une matrice (1) corrigé]{[Corrigé]}}\\
	Soit $(a_1,\dots,a_n)\in \K^n$.\\
	A quelle condition la matrice
	$A=\begin{pmatrix}
		0&0&\dots&0&a_1\\
		0&\ddots&\ &0&a_2\\
		\vdots&\ &\ddots&\vdots&\vdots\\
		0&0&\dots&0&a_{n-1}\\
		a_1&a_2&\dots&a_{n-1}&a_n
	\end{pmatrix}$
	est-elle diagonalisable?
	
	\section{Diagonalisabilité d'une matrice (2) \telecom{3}}
	\label{Diagonalisabité d'une matrice (2)}
	\textcolor{blue}{\hyperref[Diagonalisabilité d'une matrice (2) corrigé]{[Corrigé]}}\\
	La matrice $A=(i/j)_{1\leq i,j\leq n}$ est-elle diagonalisable ?
	
	\section{Diagonalisabilité d'une matrice (3) \centraleponts{3}}
	\label{Diagonalisabité d'une matrice (3)}
	\textcolor{blue}{\hyperref[Diagonalisabilité d'une matrice (3) corrigé]{[Corrigé]}}\\
	Soient $U\in \M_n(\C)$ et $V=\left( \begin{array}{c|c}
		0&I_n\\
		\hline
		U&0 
	\end{array}\right)$.\\
	Déterminer une condition nécessaire et suffisante sur $U$ pour que $V$ soit diagonalisable.
	

	\section{\underline {Cotrigonalisation (1)} \centraleponts{3}}
	\label{Cotrigonalisation (1)}
	\textcolor{blue}{\hyperref[Cotrigonalisation (1) corrigé]{[Corrigé]}}\\
	Soit $E$ un $\K$-espace vectoriel de dimension $n$ finie.
	\begin{enumerate}
		\item  Soient $u,v\in \mathcal L(E)$ trigonalisables et commutant. \\Montrer que $u$ et $v$ admettent une base de trigonalisation commune.
		\item Soient $u_1,\dots,u_p\in \mathcal L(E)$ trigonalisables et commutant deux à deux.\\ Montrer que $u_1,\dots,u_n$ admettent une base de trigonalisation commune.
		\item Soit $\A$ une sous-algèbre commutative de $\mathcal L(E)$ formée d'endomorphismes trigonalisables. \\Montrer qu'il existe une base de trigonalisation commune à tous les éléments de $\A$.
	\end{enumerate}
	
	\section{Cotrigonalisation (2) \centraleponts{3}}
	\label{Cotrigonalisation (2)}
	\textcolor{blue}{\hyperref[Cotrigonalisation (2) corrigé]{[Corrigé]}}\\
	Soit $E$ un $\C$-espace vectoriel de dimension $n$ finie. On note $E^*$ l'espace dual de $E$.\\
	Pour $u\in \mathcal L(E)$ on pose $\fonction{T_u}{E^*}{E^*}{\Phi}{\Phi\circ u}$.\\
	On fixe $(u,v)\in \mathcal L(E)^2$.
	\begin{enumerate}
		\item Montrer que $T_u\in \mathcal L(E^*)$.
		\item Donner une condition suffisante pour que $T_u$ et $T_v$ commutent.
		\item On suppose que $u$ et $v$ commutent. Montrer que $u$ et $v$ sont trigonalisables dans une même base.
	\end{enumerate}
	
	\section{Cotrigonalisation (3) \centraleponts{4}}
	\label{Cotrigonalisation (3)}
	\textcolor{blue}{\hyperref[Cotrigonalisation (3) corrigé]{[Corrigé]}}\\
	Soient $A,B,C$ trois matrices de $\M_n(\C)$ telles que
	$$AB-BA=C,\ AC=CA,\ BC=CB$$
	\begin{enumerate}
		\item Montrer que $A,B$ et $C$ ont un vecteur propre commun.
		\item Montrer que $A,B,C$ sont cotrigonalisables.
	\end{enumerate}
	
	\section{Cotrigonalisation (4) \centraleponts{4}}
	\label{Cotrigonalisation (4)}
	\textcolor{blue}{\hyperref[Cotrigonalisation (4) corrigé]{[Corrigé]}}\\
	Soient $A,B\in \M_n(\C)$.
	\begin{enumerate}
		\item On suppose qu'il existe $\lambda\in \C^*$ tel que $AB-BA=\lambda A$.\\ Montrer que $A$ est nilpotente. En déduire que $A$ et $B$ ont un vecteur propre commun.
		\item On suppose qu'il existe $\lambda,\mu\in \C$ tels que $AB-BA=\lambda A+\mu B$.\\ Montrer que $A$ et $B$ possèdent un vecteur propre commun. En déduire que $A$ et $B$ sont cotrigonalisables.
	\end{enumerate}
	
	\section{Cotrigonalisation (5) \centraleponts{3}}
	\label{Cotrigonalisation (5)}
	\textcolor{blue}{\hyperref[Cotrigonalisation (5) corrigé]{[Corrigé]}}\\
	Soit $(A,B)\in \M_n(\C)^2$ tel que $AB=0$. Montrer que $A$ et $B$ sont cotrigonalisables.
	
	\section{Caractérisation des matrices nilpotentes par la trace \telecom{3}}
	\label{Caractérisation des matrices nilpotente par la trace}
	\textcolor{blue}{\hyperref[Caractérisation des matrices nilpotente par la trace corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\K)$ Montrer que $A$ est nilpotente si et seulement si $\forall k\in \N^*,\ \Tr{(A^k)}=0$.
	
	\section{Facteur commun dans le polynôme caractéristique \centraleponts{3}}
	\label{Facteur commun dans le polynôme caractéristique}
	\textcolor{blue}{\hyperref[Facteur commun dans le polynôme caractéristique corrigé]{[Corrigé]}}\\
	Soit $E$ un $\K$-espace vectoriel de dimension finie. Soit $(f,g)\in \mathcal{L}(E)^2$.\\ On suppose qu'il existe $h\in \mathcal{L}(E)$ de rang $r\geq 1$ tel que $f\circ h=h\circ g$.\\ Montrer que $\chi_f$ et $\chi_g$ ont un facteur commun de degré $r$.\\
	La réciproque est-elle vraie ?
	
	\section{$\chi_{AB}=\chi_{BA}$ \centraleponts{3}}
	\label{chiAB=chiBA}
	\textcolor{blue}{\hyperref[chiAB=chiBA corrigé]{[Corrigé]}}\\
	Soient $\lambda\in \K,\ A\in \M_{n,p}(\K)$ et $B\in \M_{p,n}(\K)$. On pose $$M=\left( \begin{array}{c|c}
		\lambda I_n&-A\\
		\hline
		-B&I_p
	\end{array}\right)$$\\
	\begin{enumerate}
		\item Etablir que $\lambda^p\chi_{AB}(\lambda)=\lambda^n\chi_{BA}(\lambda)$.\\
		\item En déduire que $\chi_{AB}=\chi_{BA}$ si $n=p$.
	\end{enumerate}

	\section{Polynôme caractéristique de l'inverse \telecom{2}}
	\label{Polynôme caractérisque de l'inverse}
	\textcolor{blue}{\hyperref[Polynôme caractéristique de l'inverse corrigé]{[Corrigé]}}\\
	Soit $A\in \text{GL}_n(\K)$. Exprimer $\chi_{A^{-1}}$ en fonction de $\chi_A$.
	
	\section{Commutativité et stabilité \ccinp{2}}
	\label{Commutativité et stabilité}
	\textcolor{blue}{\hyperref[Communtativité et stabilité corrigé]{[Corrigé]}}\\
	Soient $u$ et $v$ deux endomorphismes d'un $\K$-espace vectoriel de dimension finie $E$. On suppose $u$ diagonalisable.\\
	Montrer que $u$ et $v$ commutent si et seulement si tout sous-espace propre de $u$ est stable par $v$.
	
	\section{Dimension du commutant d'une matrice diagonalisable \centraleponts{3}}
	\label{Dimension du commutant d'une matrice diagonalisable}
	\textcolor{blue}{\hyperref[Dimension du commutant d'une matrice diagonalisable corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\K)$ diagonalisable.\\
	On note $\operatorname{Sp}(A)=\{\lambda_1,\dots,\lambda_r\}$ ainsi que $n_1\dots,n_r$ leur multiplicité respective.\\ On note également $\mathcal{C}(A)=\{ M\in \M_n(\K),\ AM=MA\}$ le commutant de $A$.\\
	\begin{enumerate}
		\item Montrer que $\dim\mathcal{C}(A)=\displaystyle\sum\limits_{\lambda\in \operatorname{Sp}(A)}{\dim E_\lambda(A)^2}$
		\item En déduire que $\dim \mathcal{C}(A)=n\iff \dim \K[A]=n\iff r=n\iff \mathcal{C}(A)=\K[A]$.
	\end{enumerate}
	
	\section{\underline{Sous-espaces stables d'un endomorphisme diagonalisable} \centraleponts{3}}
	\label{Sous-espaces stables d'un endomorphisme diagonalisable}
	\textcolor{blue}{\hyperref[Sous-espaces stables d'un endomorphisme diagonalisable corrigé]{[Corrigé]}}\\
	Soit $u$ un endomorphisme diagonalisable d'un $\K$-espace vectoriel $E$ de dimension finie.
	\begin{enumerate}[leftmargin=*]
		\item Soit $F$ un sous-espace vectoriel de $E$. On pose $G=\displaystyle\bigoplus\limits_{\lambda\in \operatorname{Sp}(u)}{F\cap E_\lambda(u)}$.\\
		Montrer que $G$ est stable par $u$.
		\item Soit $F$ un sous-espace vectoriel de $E$ stable par $u$. Montrer que $F=\displaystyle\bigoplus\limits_{\lambda\in \operatorname{Sp}(u)}{F\cap E_\lambda(u)}$
		\item Montrer que les sous-espaces vectoriels de $E$ stables par $u$ sont exactement ceux de la forme $\displaystyle\bigoplus\limits_{\lambda\in \operatorname{Sp}(u)}{F_\lambda}$ où pour tout $\lambda\in \operatorname{Sp}(u),\ F_\lambda$ est un sous-espace vectoriel de $E_\lambda(u)$.\\
	\end{enumerate}
	
	\section{Caractérisation de la diagonalisabilité par les hyperplans \centraleponts{3}}
	\label{Caractérisation de la diagonalisabilité par les hyperplans}
	\textcolor{blue}{\hyperref[Caractérisation de la diagonalisabilité par les hyperplans corrigé]{[Corrigé]}}\\
	Soient $E$ un $\R$-espace vectoriel de dimension $n$ et $f\in \L(E)$.\\ Montrer que $f$ est diagonalisable si et seulement s'il existe $n$ hyperplans $H_1,\dots,H_n$ de $E$ stables par $f$ tels que $\displaystyle\bigcap\limits_{i=1}^n{H_i}=\{0_E\}$.
	
	\section{Hyperplans stables}
	\label{Hyperplans stables}
	\textcolor{blue}{\hyperref[Hyperplans stables corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $u$ un endomorphisme d'un $\C$-espace vectoriel $E$ de dimension finie.\\
		Soit $H$ un hyperplan de $E$. Montrer que $H$ est stable par $u$ si et seulement si il existe $\lambda\in \C$ tel que $\T{Im}(u-\lambda\Id_E)\subset H$.
		\item Soient $A\in \M_n(\R)$ et $u\in \L(\R^n)$ l'endomorphisme canoniquement associé.\\
		Soit $X\in \R^n\backslash\{0\}$. Montrer que l'hyperplan $H=X^\bot$ est stable par $u$ si et seulement si $X$ est valeur propre de $A^\top$.
	\end{enumerate}
	
	\section{Sous-espaces stables d'un endomorphisme à spectre simple \centraleponts{3}}
	\label{Sous-espaces stables d'un endomorphisme à spectre simple}
	\textcolor{blue}{\hyperref[Sous-espaces stables d'un endomorphisme à spectre simple corrigé]{[Corrigé]}}\\
	Soit $E$ un $\K$-espace vectoriel de dimension finie $n$ et soit $u$ un endomorphisme de $E$ à spectre simple, c'est à dire ayant exactement $n$ valeurs propres distinctes.\\
	Montrer qu'il existe un nombre fini de sous-espaces de $E$ stables par $u$ et les dénombrer.
	
	\section{Sous-espaces stables d'un endomorphisme nilpotent maximal \centraleponts{3}}
	\label{Sous-espaces stables d'un endomorphsime nilpotent maximal}
	\textcolor{blue}{\hyperref[Sous-espaces stables d'un endomorphisme nilpotent maximal corrigé]{[Corrigé]}}\\
	Soit $E$ un $\K$-espace vectoriel de dimension $n$ finie.\\ On considère $u\in \L(E)$ nilpotent d'indice $n$.\\
	Montrer que $u$ stabilise exactement $n+1$ sous-espaces vectoriels de $E$.
	
	\section{Endomorphismes de permutation \xens{4}}
	\label{Endomorphismes de permutation}
	\textcolor{blue}{\hyperref[Endomorphismes de permutation corrigé]{[Corrigé]}}\\
	Soit $\sigma\in \S_n$. On définit l'endomorphisme $u_\sigma : (x_1,\dots,x_n)\in \C^n\mapsto (x_{\sigma(1)},\dots,x_{\sigma(n)})$.
	\begin{enumerate}
		\item Déterminer le spectre de $u_\sigma$.
		\item Déterminer les sous-espaces vectoriels de $\C^n$ stables par tous les $u_\sigma,\ \sigma\in \mathcal S_n$.
	\end{enumerate}
	
	
	\section{Semi-simplicité \centraleponts{3}}
	\label{Semi-simplicité}
	\textcolor{blue}{\hyperref[Semi-simplicité corrigé]{[Corrigé]}}\\
	Soit $u$ un endomorphisme d'un $\C$-espace vectoriel $E$ de dimension finie.\\
	Montrer que $u$ est diagonalisable si et seulement si tout sous-espace vectoriel de $E$ admet un supplémentaire dans $E$ stable par $u$. Le résultat persiste t-il pour des $\R$-espaces vectoriels ?
	
	\section{Endomorphismes diagonalisables d'un $\R$-espace vectoriel \telecom{3}}
	\label{Endomorphismes diagonalisables d'un R-espace vectoriel}
	\textcolor{blue}{\hyperref[Endomorphismes diagonalisables d'un R-espace vectoriel corrigé]{[Corrigé]}}\\
	Soient $f,g$ deux endomorphismes diagonalisables d'un $\R$-espace vectoriel $E$ de dimension $n$ finie. Soit également $k$ un entier naturel impair.
	\begin{enumerate}
		\item Montrer que tout vecteur propre de $f^k$ est vecteur propre de $f$.
		\item Montrer que $f^k=g^k\implies f=g$.
	\end{enumerate}
	
	\section{\underline{Matrices à spectres disjoints} \centraleponts{3}}
	\label{Matrices à spectres disjoints}
	\textcolor{blue}{\hyperref[Matrices à spectres disjoints corrigé]{[Corrigé]}}\\
	Soient $A,B\in \M_n(\K)$.\\ Montrer l'équivalence des propositions suivantes:
	\begin{enumerate}[label=(\roman*)]
		\item $A$ et $B$ n'ont pas valeurs propres communes;
		\item $\chi_A(B)$ est inversible;
		\item $\forall X\in \M_n(\K),\ AX=XB\implies X=0$;
		\item $\forall M\in \M_n(\K),\exists X\in \M_n(\K),\ AX-XB=M$.
	\end{enumerate}
	
	

\chapter{Réduction algébrique}
	Dans toute cette section $n$ désigne un entier supérieur ou égal à $1$.\\
	
 	\section{Equation matricielle polynomiale (1) \ccinp{2}}
 	\label{Equation matricielle polynomiale (1)}
 	\textcolor{blue}{\hyperref[Equation matricielle polynomiale (1) corrigé]{[Corrigé]}}\\
	Déterminer les matrices $M\in \M_n(\C)$ vérifiant $M^5=M^2$ et $\Tr(M)=n$.
	
	\section{Equation matricielle polynomiale (2) \telecom{2}}
	\label{Equation matricielle polynomiale (2)}
	\textcolor{blue}{\hyperref[Equation matricielle polynomiale (2) corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\R)$ telle que $A^3+A^2+A=0$. Montrer que $\rg(A)$ est pair.
	
	\section{Equation matricielle avec la comatrice \xens{4}}
	\label{Equation matricielle avec la comatrice}
	\textcolor{blue}{\hyperref[Equation matricielle avec la comatrice corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Montrer que $\forall A,B\in \M_n(\C),\ \Com(AB)=\Com(A)\Com(B)$.
	\end{enumerate}
	On cherche les matrices $A\in \M_n(\C)$ telles que $A+\Com(A)^\top$ soit une matrice scalaire. Pour $\lambda\in \C$ on note $E_\lambda=\{A\in \M_n(\C),\ A+\Com(A)^\top=\lambda I_n\}$.
	\begin{enumerate}[resume]
		\item Montrer que si $A\in E_\lambda$ alors toute la classe de conjugaison de $A$ est dans $E_\lambda$.
		\item Discuter du rang de $\Com(A)$ en fonction du rang de $A$.
		\item En déduire l'ensemble des matrices recherchées.
	\end{enumerate}
	
	\section{Rang et spectre de la comatrice \xens{3}}
	\label{Rang et spectre de la comatrice}
	\textcolor{blue}{\hyperref[Rang et spectre de la comatrice corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\C)$. Déterminer le rang et le spectre de $\tilde A=\Com(A)^\top$.
	
	\section{Racine $p$-ième d'une matrice \telecom{3}}
	\label{Racine pieme d'une matrice}
	\textcolor{blue}{\hyperref[Racine pieme d'une matrice corrigé]{[Corrigé]}}\\
	Soient $A\in \text{GL}_n(\C)$ diagonalisable, $B\in \M_n(\C)$ et $p\in \N^*$ tels que $B^p=A$.\\
	Montrer que $B$ est diagonalisable. Le résultat persiste-il avec $A$ non inversible ?
	
	\section{Diagonalisibilité de $f$ dans le cas $f^2$ diagonalisable \centraleponts{3}}
	\label{Diagonalisabilité de f dans le cas f^2 diagonalisable}
	\textcolor{blue}{\hyperref[Diagonalisabilité de f dans le cas f^2 diagonalisable corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $f\in \mathcal L(\C^n)$. Montrer que $f$ est diagonalisable si et seulement si $f^2$ est diagonalisable et $\ker f=\ker f^2$.
		\item Soit $f\in \mathcal L(\R^n)$ tel que $f^2$ est diagonalisable. A quelle condition nécessaire et suffisante $f$ est-il diagonalisable ?
		\item Soit $(a_1,\dots,a_n)\in \C^n$.\\ A quelle condition nécessaire et suffisante la matrice $A=\begin{pmatrix}
			0&\dots&0&a_n\\
			\vdots&\iddots &\iddots&0\\
			0&\iddots&\iddots&\vdots\\
			a_1&0&\dots&0
		\end{pmatrix}$
		est-elle diagonalisable dans $\M_n(\C)$ ? Dans $\M_n(\R)$ ?
	\end{enumerate}
	
	\section{\underline{Endomorphismes diagonalisables non bijectifs} \ccinp{2}}
	\label{Endomorphismes diagonalisables non bijectifs}
	\textcolor{blue}{\hyperref[Endomorphismes diagonalisables non bijectifs corrigé]{[Corrigé]}}\\
	Soient $E$ un $\K$-espace vectoriel et $f\in \mathcal L(E)$.
	\begin{enumerate}
		\item On suppose qu'il existe $P\in \K[X]$ un polynôme annulateur de $f$ tel que $P(0)=0$ et $P'(0)\ne0$. Montrer que $E=\Ker f\bigoplus\text{Im}f$.
		\item On suppose $f$ diagonalisable et non bijectif. Montrer que $E=\Ker f\bigoplus\text{Im}f$.
	\end{enumerate}
	
	\section{Valuation du polynôme minimal \centraleponts{3}}
	\label{Valuation du polynôme minimal}
	\textcolor{blue}{\hyperref[Valuation du polynôme minimal corrigé]{[Corrigé]}}\\
	Soient $E$ un $\K$-espace vectoriel de dimension finie et $f\in \mathcal{L}(E)$. On note $p$ la valuation de $\pi_f$ (i.e le plus petit degré des monômes non nuls de $\pi_f$). 
	\begin{enumerate}
		\item On suppose que $p=0$. Que peut-on dire de $f$ ?
		\item Montrer que $E=\text{Ker}(f^p)\bigoplus\text{Im}(f^p)$.
		\item On suppose $p\ne 0$. Montrer que $p$ est le plus petit entier naturel non nul vérifiant l'égalité précédente.
	\end{enumerate}
	
	\section{Sous-espaces stables \centraleponts{3}}
	\label{Sous-espaces stables}
	\textcolor{blue}{\hyperref[Sous-espaces stables corrigé]{[Corrigé]}}\\
	Soient $u$ un endomorphisme d’un $\K$-espace vectoriel de dimension finie et $P\in \K[X]$ un polynôme unitaire annulateur de $u$. La décomposition de $P$ en facteurs irréductibles unitaires s’écrit $P=\displaystyle\prod\limits_{i=1}^r{P_i}$. Pour $i\in \crblanc{1}{r}$, on pose $N_i=\ker P_i(u)$.\\
	Soit $F$ un sous-espace vectoriel de $E$ stable par $u$. Montrer que $F=\displaystyle\bigoplus\limits_{i=1}^r{F\cap N_i}$.
	
	\section{Une formule sur les polynômes \centraleponts{3}}
	\label{Une formule sur les polynômes}
	\textcolor{blue}{\hyperref[Une formule sur les polynômes corrigé]{[Corrigé]}}\\
	Montrer que $\forall P\in \C_{n-1}[X],\ \displaystyle\sum\limits_{k=0}^n{(-1)^{n-k}\binom{n}{k}P(X+k)=0}$.
	
	\section{Ordre de matrice \xens{4}}
	\label{Ordre de matrice}
	\textcolor{blue}{\hyperref[Ordre de matrice corrigé]{[Corrigé]}}\\
	Soit $M\in \text{GL}_2(\Z)$. On note $\chi_M=X^2-tX+d$ le polynôme caractéristique de $M$.
	\\ En discutant les valeurs possibles de $t$ et $d$, déterminer tous les ordres possibles pour $M$, puis trouver un exemple pour chaque ordre.
	
	\section{\underline{Sous-groupes finis de GL$_2(\Z)$} \centraleponts{4}}
	\label{Sous-groupes finis de GL2(Z)}
	\textcolor{blue}{\hyperref[Sous-groupes finis de GL2(Z) corrigé]{[Corrigé]}}\\
	On note GL$_2(\Z)$ l'ensemble des matrices inversibles de $\M_2(\Z)$ dont l'inverse est aussi dans $\M_2(\Z)$.
	\begin{enumerate}
		\item Montrer que (GL$_2(\Z),\times)$ est un groupe.
		\item Soit $G$ un sous-groupe fini de GL$_2(\Z)$.\\
		Montrer que $\forall M\in G,\ M^{12}=I_2$.
	\end{enumerate}
	
	\section{\underline{Sous-groupes finis de GL$_n(\Z)$} \centraleponts{4}}
	\label{Sous-groupes finis de GLn(Z)}
	\textcolor{blue}{\hyperref[Sous-groupes finis de GLn(Z) corrigé]{[Corrigé]}}\\
	On note GL$_n(\Z)=\{ M\in \M_n(\Z),\ (M,M^{-1})\in \M_n(\Z)^2\}$.
	\begin{enumerate}
		\item Soit $M\in \M_{n}(\Z)$. Montrer que $M\in \text{GL}_n(\Z)$ si et seulement si $|\det M|=1$.\\
		Montrer que GL$_n(\Z)$ est un sous-groupe de GL$_n(\R)$.
		\item Soit $G$ un sous-groupe fini de GL$_n(\Z)$.
		\begin{enumerate}[label=\alph*.]
			\item On fixe $M\in G$ et on pose $N=\displaystyle\frac{1}{3}(M-I_n)$. Etudier la convergence de la suite $(N^k)_{k\in \N}$.
			\item On utilisant l'application $\fonction{\pi}{\M_n(\Z)}{\M_n(\Z/3\Z)}{A}{\overline{A}}$, Montrer qu'il existe un entier $K_n$ tel que $|G|\leq K_n$.
		\end{enumerate}
	\end{enumerate}
	
	\section{\underline{Isomorphisme entre GL$_n(\C)$ et GL$_m(\C)$} \centraleponts{4}}
	\label{Isomorphisme entre GLn(C) et GLm(C)}
	\textcolor{blue}{\hyperref[Isomorphisme entre GLn(C) et GLm(C) corrigé]{[Corrigé]}}\\
		\begin{enumerate}[leftmargin=*]
		\item On se donne $p$ matrices $A_1,\dots,A_p\in \M_n(\C)$ diagonalisables et commutant entre elles.
		\begin{enumerate}[label=\alph*.]
			\item Montrer que $A_1$ et $A_2$ sont codiagonalisables.
			\item Montrer que $A_1,\dots,A_p$ sont codiagonalisables.
		\end{enumerate}
		\item Soit $G$ un sous-groupe de GL$_n(\C)$ tel que $\forall A\in G,\ A^2=I_n$. Montrer que $G$ est fini. Que dire de son cardinal ?
		\item Soient $m,n\in \N^*$ distincts. Existe-il un isomorphisme de GL$_n(\C)$ sur GL$_m(\C)$ ?
	\end{enumerate}
	
	\section{Inverse et conjugaison \xens{5}}
	\label{Inverse et conjugaison}
	\textcolor{blue}{\hyperref[Inverse et conjugaison corrigé]{[Corrigé]}}\\
	Soit $A\in \M_n(\C)$. Montrer que $A\overline{A}=I_n \iff \exists S\in \text{GL}_n(\C),\ A=S{\overline S}^{-1}$.
	
	\section{\underline{Matrice compagnon (2)} \ccinp{2}}
	\label{Matrice compagnon (2)}
	\textcolor{blue}{\hyperref[Matrice compagnon (2) corrigé]{[Corrigé]}}\\
	Soit $(a_0,\dots,a_n)\in \K^n$. On pose $A=
	\begin{pmatrix}
		0&\dots&\dots&0&-a_0\\
		1&\ddots&\ &\vdots&-a_1\\
		0&\ddots&\ddots&\vdots&\vdots\\
		\vdots&\ddots&\ddots&0&-a_{n-2}\\
		0&\dots&0&1&-a_{n-1}
	\end{pmatrix}$.
	\begin{enumerate}
		\item Montrer que $\chi_A=X^n+\displaystyle\sum\limits_{k=0}^{n-1}{a_kX^k}$
		\item Montrer que $\pi_A=\chi_A$
		\item Déterminer les sous-espaces propres de $A^\top$.
	\end{enumerate}
	
	\section{\underline{Polynôme minimal ponctuel} \centraleponts{3}}
	\label{Polynôme minimal ponctuel}
	\textcolor{blue}{\hyperref[Polynôme minimal ponctuel corrigé]{[Corrigé]}}\\
	Soit $u$ un endomorphisme d'un $\K$-espace vectoriel $E$ de dimension $n$.
	\begin{enumerate}[leftmargin=*]
		\item Pour $x\in E$ on note $I_{u,x}=\{P\in \K[X],\ P(u)(x)=0_E\}$ et $E_{u,x}=\{P(u)(x),\ P\in \K[X]\}$. On fixe $x\in E$.
		\begin{enumerate}[label=\alph*.]
			\item Montrer que $I_{u,x}$ est un idéal de $\K[X]$. On note $\pi_{u,x}$ son unique générateur unitaire (appelé polynôme minimal ponctuel de $u$ en $x$).
			\item Montrer que $E_{u,x}$ est un sous-espace vectoriel de $E$ et que $(u^k(x))_{0\leq k\leq \operatorname{deg}(\pi_{u,x})-1}$ en est une base.
		\end{enumerate}
		\item Soient $x_1,\dots,x_p\in E$ tels que leur polynômes minimaux ponctuels associés soient premiers entre eux. On pose $x=\displaystyle\sum\limits_{i=1}^p{x_i}$ et $P=\displaystyle\prod\limits_{i=1}^p{\pi_{u,x_i}}$.
		\begin{enumerate}[label=\alph*.]
			\item Montrer que $\pi_{u,x}$ divise $P$.
			\item Montrer que les sous-espaces vectoriels $E_{u,x_1},\dots,E_{u,x_p}$ sont en somme directe.
			\item En déduire que $\pi_{u,x}=P$ et $E_{u,x}=\displaystyle\bigoplus\limits_{i=1}^p{E_{u,x_i}}$.
		\end{enumerate}
		\item En considérant la décomposition en facteurs irréductibles de $\pi_{u,x}$, montrer à l'aide de la question précédente qu'il existe $x\in E$ tel que $\pi_{u,x}=\pi_u$.
		\item Montrer que les conditions suivantes sont équivalentes.
		\begin{enumerate}[label=(\roman*)]
			\item $\pi_u=\chi_u$;
			\item Il existe $x\in E$ tel que $E_{u,x}=E$;
			\item Il existe une base de $E$ dans laquelle la matrice de $u$ est de la forme
			$$\begin{pmatrix}
				0&\dots&\dots&0&-a_0\\
				1&\ddots&\ &\vdots&-a_1\\
				0&\ddots&\ddots&\vdots&\vdots\\
				\vdots&\ddots&\ddots&0&-a_{n-2}\\
				0&\dots&0&1&-a_{n-1}
			\end{pmatrix}$$
			On dit dans ce cas que $u$ est un endomorphisme cyclique.
		\end{enumerate}
	\end{enumerate}
	
	\section{\underline{Endomorphismes cycliques} \centraleponts{3}}
	\label{Endomorphismes cycliques}
	\textcolor{blue}{\hyperref[Endomorphismes cycliques corrigé]{[Corrigé]}}\\
	Soit $E$ un $\K$-espace vectoriel de dimension $n$. On dit d'un endomorphisme $f$ de $E$ qu'il est cyclique lorsqu'il existe un vecteur $x_0\in E$ tel que la famille $(x_0,f(x_0),\dots,f^{n-1}(x_0))$ soit une base de $E$.\\
	Pour $Q=X^n+\displaystyle\sum\limits_{k=0}^{n-1}{a_kX^k}$ on pose
	$$C_Q=\begin{pmatrix}
		0&\dots&\dots&0&-a_0\\
		1&\ddots&\ &\vdots&-a_1\\
		0&\ddots&\ddots&\vdots&\vdots\\
		\vdots&\ddots&\ddots&0&-a_{n-2}\\
		0&\dots&0&1&-a_{n-1}
	\end{pmatrix}$$
	sa matrice compagnon.
	\begin{enumerate}
		\item Montrer que $f\in \mathcal L(E)$ est cyclique si et seulement s'il existe un polynôme $Q$ unitaire de degré $n$ et une base de $E$ dans laquelle la matrice de $f$ est $C_Q$.
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $M\in \M_n(\K)$. Montrer que $M$ est diagonalisable si et seulement si $M^\top$ l'est.
			\item Soit $Q\in \K[X]$ unitaire de degré $n$. Déterminer la dimension des sous-espaces propres de $C_Q^\top$ et en déduire une condition nécessaire et suffisante pour qu'un endomorphisme cyclique soit diagonalisable.
		\end{enumerate}
	\end{enumerate}
	
	\section{\underline{Commutant d'un endomorphisme cyclique} \centraleponts{3}}
	\label{Commutant d'un endomorphisme cyclique}
	\textcolor{blue}{\hyperref[Commutant d'un endomorphisme cyclique corrigé]{[Corrigé]}}\\
	Soit $E$ un $\K$-espace vectoriel de dimension $n$. On se donne $f\in \mathcal{L}(E)$ et $x_0\in E$ tel que $\B=(x_0,f(x_0),\dots,f^{n-1}(x_0))$ soit une base de $E$. On note $\mathcal{C}(f)=\{g\in \mathcal{L}(E),\ f\circ g=g\circ f\}$ le commutant de $f$.\\
	Soit $g\in \mathcal{C}(f)$. On note $g(x_0)=\displaystyle\sum\limits_{k=0}^{n-1}{\lambda_kf^k(x_0)}$ la décomposition de $g(x_0)$ dans la base $\B$.
	\begin{enumerate}
		\item Montrer que $g\in \K[f]$.
		\item Montrer que $\mathcal{C}(f)=\K_{n-1}[f]$.
	\end{enumerate}
	
	\section{Une démonstration de Cayley-Hamilton \centraleponts{3}}
	\label{Une démonstration de Cayley_Hamilton}
	\textcolor{blue}{\hyperref[Une démonstration de Cayley-Hamilton corrigé]{[Corrigé]}}\\
	Soient $x$ un vecteur non nul d'un $\K$-espace vectoriel $E$ de dimension $n$ et $f\in \mathcal{L}(E)$.
	\begin{enumerate}
		\item Montrer qu'il existe un entier $p\in \N^*$ pour lequel $(x,f(x),\dots,f^{p-1}(x))$ est libre et il existe $(\alpha_0\dots,\alpha_{p-1})\in \K^p$ tel que :
		$$\alpha_0x+\alpha_1f(x)+\dots+\alpha_{p-1}f^{p-1}(x)+f^p(x)=0$$
		\item Justifier que $\operatorname{Vect}(x,f(x)\dots,f^{p-1}(x))$ est stable par $f$.
		\item Montrer que $P=X^p+\displaystyle\sum\limits_{k=0}^{p-1}{\alpha_kX^k}$ divise $\chi_f$. (On pourra déterminer le polynôme caractéristique de la matrice compagnon de $P$ cf. \ref{Matrice compagnon (1)})
		\item En déduire le théorème de Cayley-Hamilton.
	\end{enumerate}
	
	\section{Indépendance de corps du polynôme minimal \centraleponts{3}}
	\label{Indépendance de corps du polynôme minimal}
	\textcolor{blue}{\hyperref[Indépendance de corps du polynôme minimal corrigé]{[Corrigé]}}\\
	Soit $M\in \M_n(\R)$. Montrer que le polynôme minimal de $M$, vu comme une matrice à coefficients réels, est le même que celui de $M$, vu comme une matrice à coefficients complexes.
	
	\section{Polynôme minimal de l'inverse \xens{3}}
	\label{Polynôme minimal de l'inverse}
	\textcolor{blue}{\hyperref[Polynôme minimal de l'inverse corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $A\in \text{GL}_n(\R)$. Exprimer le polynôme minimal de $A^{-1}$ en fonction de celui de $A$.
		\item Soit $A\in \O_n(\R)$ telle que $1$ et $-1$ ne soient pas valeurs propres de $A$. Montrer que le polynôme minimal de $A$ est de degré pair.
	\end{enumerate}
	
	\section{Polynôme minimal de la transposée \ccinp{1}}
	\label{Polynôme minimal de la transposée}
	\textcolor{blue}{\hyperref[Polynôme minmal de la transposée corrigé]{[Corrigé]}}\\
	Montrer qu'une matrice et sa transposée ont même polynôme minimal.
	
	\section{Polynôme minimal imposé \centraleponts{3}}
	\label{Polynôme minimal imposé}
	\textcolor{blue}{\hyperref[Polynôme minimal imposé corrigé]{[Corrigé]}}\\
	Le polynôme $X^4+X^3+2X^2+X+1$ peut-il être le polynôme minimal d'une matrice de $\M_5(\R)$ ?
	
	\section{\underline{Matrice de Gram} \centraleponts{3}}
	\label{Matrice de Gram}
	\textcolor{blue}{\hyperref[Matrice de Gram corrigé]{[Corrigé]}}\\
	Soit $E$ un espace euclidien.\\
	A toute famille $(x_1,\dots, x_p)$ de $p$ vecteurs de $E$ on associe la matrice $G(x_1,\dots, x_p) = (\langle x_i, x_j \rangle)_{1\leq i,j\leq p}$.
	\begin{enumerate}
		\item Soit $\B=(e_1,\dots,e_n)$ une base orthonormée de $\F=\text{Vect}(x_1,\dots,x_p)$. On note $A=\text{mat}_{\B}(x_1,\dots,x_p)$.\\
		Montrer que $G(x_1,\dots,x_p)=A^\top A$.
		\item En déduire que les valeurs propres de $G$ sont positives ou nulles.
		\item Montrer que chaque valeur propre de $G$ est majorée par $\displaystyle\sum\limits_{i=1}^p\norme{x_i}^2$.
	\end{enumerate}
	
	\section{\underline{Matrice circulante} \centraleponts{3}}
	\label{Matrice circulante}
	\textcolor{blue}{\hyperref[Matrice circulante corrigé]{[Corrigé]}}\\
	Soit $(a_0,\dots,a_{n-1})\in \C^n$.\\
	On note $A=
	\begin{pmatrix}
		a_0&a_1&a_2&\dots&a_{n-1}\\
		a_{n-1}&\ddots&\ddots&\ddots&\vdots\\
		\vdots&\ddots&\ddots&\ddots&a_2\\
		a_2&\ &\ddots&\ddots&a_1\\
		a_1&a_2&\dots&a_{n-1}&a_0
	\end{pmatrix}\in \M_n(\C)$
	\\et $J=
	\begin{pmatrix}
		0&1&0&\dots&0\\
		\vdots&\ddots&\ddots&\ddots&\vdots\\
		0&\dots&0&1&0\\
		0&\dots&\dots&0&1\\
		1&0&\dots&\dots&0
	\end{pmatrix}\in \M_n(\C)$\\\\
	\begin{enumerate}
		\item Calculer $J^k$ pour $k\in \N$.
		\item Diagonaliser $A$.
	\end{enumerate}
	
	\section{Diagonalisabilité du produit de deux matrices \centraleponts{3}}
	\label{Diagonalisabilité du produit de deux matrices}
	\textcolor{blue}{\hyperref[Diagonalisabilité du produit de deux matrices corrigé]{[Corrigé]}}\\
	Soit $(A,B)\in \M_{n,p}(\K)\times \M{p,n}{\K}$. On suppose que $AB$ est diagonalisable et inversible.\\
	Montrer que $BA$ est diagonalisable
	\begin{itemize}
		\item avec des arguments de réduction algébrique
		\item avec des arguments de réduction géométrique
	\end{itemize}
	
	\section{Diagonalisation d'une matrice par bloc \telecom{3}}
	\label{Diagonalisation d'une matrice par bloc}
	\textcolor{blue}{\hyperref[Diagonalisation d'une matrice par bloc corrigé]{[Corrigé]}}\\
	Soient $A\in \M_n(\K)$ et
	$M=\left(\begin{array}{c|c}
		A&0  \\
		\hline
		A&A
	\end{array}\right)$.
	\begin{enumerate}
		\item Comparer le spectre de $M$ à celui de $A$.
		\item Soit $P\in \K[X]$. Exprimer $P(M)$ en fonction de $P(A)$ et de $P'(A)$.
		\item Donner une condition portant sur $A$ nécessaire et suffisante à ce que $M$ soit diagonalisable.
	\end{enumerate}
	
	\section{Exponentielle matricielle \centraleponts{3}}
	\label{Exponentielle matricielle}
	\textcolor{blue}{\hyperref[Exponentielle matricielle corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Montrer que $A\in S_n(\R)\implies \exp(A)\in S_n(\R)$.
		\item Montrer que $A\in A_n(\R)\implies \exp(A)\in \text{SO}_n(\R)$.
	\end{enumerate}
	
	\section{Exponentiel d'un endomorphisme nilpotent \telecom{3}}
	\textcolor{blue}{\hyperref[Exponentiel d'un endomorphisme nilpotent corrigé]{[Corrigé]}}\\
	\label{Exponentiel d'un endomorphisme nilpotent}
	Soient $E$ un $\K$-espace vectoriel de dimension finie et $u\in \mathcal{L}(E)$ nilpotent.\\
	Montrer que Ker$(\exp(u)-\Id_E)=\text{Ker}(u)$ et Im$(\exp(u)-\Id_E)=\text{Im}(u)$.
	
	\section{Endomorphismes anticommutants \centraleponts{4}}
	\label{Endomorphismes anticommutants}
	\textcolor{blue}{\hyperref[Endomorphimes anticommutants corrigé]{[Corrigé]}}\\
	Soient $p$ un entier supérieur ou égal à $2$, $E$ un $\C$-espace vectoriel de dimension $n$ et $u_1,\dots,u_p\in \mathcal{L}(E)$ vérifiant:
	$$\forall k\in \crblanc{1}{p},\ u^2_k=-\Id_E\text{ et } \forall (i,j)\in \crblanc{1}{p}^2,(i\ne j\implies u_i\circ u_j=-u_j\circ u_i)$$
	Calculer le déterminant de chacun des $u_k$.
	
	\section{Trace entière \centraleponts{4}}
	\label{Trace entière}
	\textcolor{blue}{\hyperref[Trace entière corrigé]{[Corrigé]}}\\
	Caractériser les polynômes $P\in \C[X]$ tels que: $\forall A\in \M_n(\C),\ P(A)=0\implies \Tr{(A)}\in\Z$.
	
	\section{$P(A)$ nilpotente \centraleponts{3}}
	\textcolor{blue}{\hyperref[P(A) nilpotente corrigé]{[Corrigé]}}\\
	\label{P(A) nilpotente}
	Soit $A\in \M_n(\C)$. Déterminer les polynômes $P\in \C[X]$ tels que $P(A)$ soit nilpotente.
	
	\newpage
\chapter{Déterminant}
	Dans toute cette section $n$ désigne un entier supérieur ou égal à $1$.\\
	
	\section{Dimension de l'espace des formes multilinéaires alternées \xens{4}}
	\label{Dimension de l'espace des formes multilinéaires alternées}
	\textcolor{blue}{\hyperref[Dimension de l'espace des formes multilinéaires alternées corrigé]{[Corrigé]}}\\
	Soit $E$ un $\K$-espace vectoriel de dimension $d$. Soit $n\in \N^*$.\\
	On appelle forme $n$-linéaire alternée sur $E$ toute application $f:E^n\to\K$ telle que :
	\begin{itemize}
		\item $f$ est linéaire par rapport à chacune de ses variables;
		\item $\forall \sigma\in \mathcal S_n,\ \forall (x_1,\dots,x_n)\in E^n,\ f(x_{\sigma(1)},\dots,x_{\sigma(n)})=\varepsilon(\sigma)f(x_1,\dots,x_n)$.
	\end{itemize}
	On note $\mathcal A_n(E)$ l'ensemble des formes $n$-linéaires alternées sur $E$.
	\begin{enumerate}
		\item Donner un élément de $\mathcal A_d(E)$.
		\item Montrer que $\mathcal A_n(E)$ est un $\K$-espace vectoriel et en déterminer la dimension.
	\end{enumerate}
	
	\section{Théorème de Bézout matriciel \telecom{2}}
	\label{Théorème de Bézout mattriciel}
	\textcolor{blue}{\hyperref[Théorème de Bézout matriciel corrigé]{[Corrigé]}}\\
	Soient $A,B\in\M_n(\Z)$.
	\begin{enumerate}
		\item Montrer que $\det(A),\det(B)\in\Z$.
		\item On suppose que $\det(A)$ et $\det(B)$ sont premiers entre eux.
		\\Montrer qu'il existe deux matrices $U,V\in\M_n(\Z)$ telles que $AU+BV=I_n$.
	\end{enumerate}
	
	\section{\underline{Déterminant tridiagonal} \centraleponts{3}}
	\label{Déterminant tridiagonal}
	\textcolor{blue}{\hyperref[Déterminant tridiagonal corrigé]{[Corrigé]}}\\
	Soit $(a,b,c)\in \C^3$.\\\\
	\begin{enumerate}
		\item Calculer $\Delta_n=
		\begin{vmatrix}
			a     &c     &0     &\dots &0\\
			b     &a     &c     &\ddots&\vdots\\
			0     &b     &a     &\ddots&0\\
			\vdots&\ddots&\ddots&\ddots&c\\
			0     &\dots &0     &b     &a
		\end{vmatrix}_n$.  On pourra noter $\Delta_0=1$
		\item Expliciter le résultat pour $(a,b,c)=(2\cos\theta,1,1)$ où $\theta\in \R$.
	\end{enumerate}
	
	\section{Déterminant bitriangulaire\centraleponts{3}}
	\label{Déterminant bitriangulaire}
	\textcolor{blue}{\hyperref[Déterminant bitriangulaire corrigé]{[Corrigé]}}\\
	Soit $(a,b) \in \C^2$ avec $a\ne b$.\\
	Calculer le déterminant de $M=
	\begin{pmatrix}
		0& &(a)\\
		&\ddots&\\
		(b)& &0\\
	\end{pmatrix}\in \M_n(\C)$.
	
	
	\section{\underline{Déterminant de Vandermonde}\ccinp{1}}
	\label{Déterminant de Vandermonde}
	\textcolor{blue}{\hyperref[Déterminant de Vandermonde corrigé]{[Corrigé]}}\\
	Pour $(x_1,\dots,x_n)\in \K^n$ on définit $V(x_1,\dots,x_n)=
	\begin{vmatrix}
		1     &x_1   &x_1^2 &\dots &x_1^{n-1}\\
		1     &x_2   &x_2^2 &\dots &x_2^{n-1}\\
		\vdots&\vdots&\vdots&\ddots&\vdots\\
		1     &x_n   &x_n^2 &\dots &x_n^{n-1}
	\end{vmatrix}$\\
	Calculer $V(x_1,\dots,x_n)$.\\
	On pourra étudier pour $(x_1,\dots,x_{n-1})\in \K^{n-1}$ fixée $f: x\longmapsto V(x_1,\dots,x_{n-1},x)$
	\subsection{Utilisation du déterminant de Vandermonde}
	Calculer le déterminant : $$\begin{vmatrix}
		a_1+a_2 & a_2+a_3&\dots &a_n+a_1\\
		a_1^2+a_2^2 & a_2^2+a_3^2&\dots &a_n^2+a_1^2\\
		\vdots & \vdots & &\vdots\\
		a_1^n+a_2^n & a_2^n+a_3^n&\dots &a_n^n+a_1^n\\
	\end{vmatrix}$$
	
	\section{Déterminant de Hilbert\centraleponts{3}}
	\label{Déterminant de Hilbert}
	\textcolor{blue}{\hyperref[Déterminant de Hilbert corrigé]{[Corrigé]}}\\
	Notons $H_n=\displaystyle\left(\frac{1}{i+j+1}\right)_{0\leq i,j\leq n-1}$\\
	\begin{enumerate}[leftmargin=*]
		\item On pose pour tout x réel, $\Delta_n(x)=
		\begin{vmatrix}
			1            &\frac{1}{2}  &\frac{1}{3}  &\dots &\frac{1}{n}\\
			\frac{1}{2}  &\frac{1}{3}  &\frac{1}{4}  &\dots &\frac{1}{n+1}\\
			\vdots       &\vdots       &\vdots       &\ddots&\vdots\\
			\frac{1}{n-1}&\frac{1}{n}  &\frac{1}{n+1}&\dots &\frac{1}{2n-2}\\
			\frac{1}{x}  &\frac{1}{x+1}&\frac{1}{x+1}&\dots &\frac{1}{x+n-1}
		\end{vmatrix}$\\\\
		\begin{enumerate}[label=\alph*.]
			\item Montrer qu'il existe $Q_n\in \R_{n-1}[X]\text{ tel que } \Delta_n(X)=\displaystyle\frac{Q_n(X)}{X(X+1)(X+2)\dots(X+n-1)}$
			\item Montrer qu'il existe $\lambda_n\in \R\text{ tel que } Q_n(X)=\lambda_n(X-1)(X-2)\dots(X-n+1)$
		\end{enumerate}
		\item Vérifier que $\det{H_n}=\displaystyle\frac{2^nn!}{(2n)!}\prod\limits_{k=1}^{n-1}{\frac{(k!)^4}{((2k)!)^2}}$
	\end{enumerate}
	
	\section{\underline{Déterminant de Gram}\telecom{3}}
	\label{Déterminant de Gram}
	\textcolor{blue}{\hyperref[Déterminant de Gram corrigé]{[Corrigé]}}\\
	Soit $E$ un espace euclidien. A toute famille $(x_1,\dots,x_p)$ de $p$ vecteurs de $E$ on associe la matrice\\
	$G(x_1,\dots,x_p)=(\langle x_i,x_j\rangle)_{1\leq i,j\leq p}$.
	\begin{enumerate}[leftmargin=*]
		\item Soit $\B=(e_1,\dots,e_n)$ une base orthonormée de $\F=\text{Vect}(x_1,\dots,x_p)$. On note $A=\text{mat}_{\B}(x_1,\dots,x_p)$.\\
		Montrer que $G(x_1,\dots,x_p)=A^\top A$.
		\item En déduire que $(x_1,\dots,x_p)$ est liée si et seulement si $\det{G(x_1,\dots,x_p)}=0$ puis que :\\
		$\forall x\in E,\ \det{G(x_1,\dots,x_p,x)}=d(x,\F)^2\det{G(x_1,\dots,x_p)}$.
	\end{enumerate}
	
	\section{\underline{Déterminant de Cauchy}\centraleponts{3}}
	\label{Déterminant de Cauchy}
	\textcolor{blue}{\hyperref[Déterminant de Cauchy corrigé]{[Corrigé]}}\\
	Pour toutes familles $(a_k)_{1\leq k\leq n} \text{ et } (b_k)_{1\leq k\leq n} \text{ de complexes telles que } a_k+b_k\ne 0$ pour tout $k \in \crblanc{1}{n}$, on définit $C_n=C(a_1,\dots,a_n,b_1,\dots,b_n)=\displaystyle\det{\left(\frac{1}{a_i+b_j}\right)_{1\leq i,j\leq n}}$.\\\\
	\begin{enumerate}[leftmargin=*]
		\item On pose $F(X)=C(a_1,\dots,a_{n-1},X,b_1,\dots,b_n)$.\\
		Montrer qu'il existe $P\in \C_{n-1}[X] \text{ tel que } F(X)=\displaystyle\frac{P(X)}{\displaystyle\prod\limits_{i=1}^{n}{(X+b_i)}}$.\\
		\item \begin{enumerate}[label=\alph*.]
			\item Déterminer les racines de $P$.
			\item Exprimer le coefficient dominant de $P$ en fonction de $C_{n-1}$. 
			\\(On pourra s'intéresser à $(X+b_n)F(X)$)
		\end{enumerate} 
		\item Calculer $C_n$.
	\end{enumerate}
	
	\section{Déterminant de Smith\centraleponts{3}}
	\label{Déterminant de Smith}
	\textcolor{blue}{\hyperref[Déterminant de Smith corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On pose $A=(a_{ij})_{1\leq i,j\leq n}$ avec
		$
		a_{ij}=
		\begin{cases}
			1 & \mbox{si } j|i\\
			0 & \mbox{sinon}
		\end{cases}
		$\\
		Calculer le déterminant de $A$.
		\item On pose $D=(d_{ij})_{1\leq i,j\leq n}$ avec $d_{ij}$ le nombre de diviseurs communs à $i$ et $j$.\\
		Calculer le déterminant de $D$.
		\item On pose $S=(i\wedge j)_{1\leq i,j\leq n}$\\
		Montrer que $\det S=\displaystyle\prod_{k=1}^{n}\varphi(k)$ où $\varphi$ désigne l'indicatrice d'Euler.\\
		On pourra admettre la formule classique $n=\displaystyle\sum_{d|n}\varphi(d)$.
	\end{enumerate}
	
	\section{Déterminant de Cayley-Menger \xens{4}}
	\label{Déterminant de Cayley-Menger}
	\textcolor{blue}{\hyperref[Déterminant de Cayley-Menger corrigé]{[Corrigé]}}\\
	Soient $x_0,\dots,x_n$ des points de $\R^n$ et $d_{i,j}=\norme{x_j-x_i}$ les distances associées.
	\\On pose $\Gamma(x_0,\dots,x_n)=\begin{vmatrix}
		0&1&\dots&1\\
		1&\ddots&d_{i,j}^2&\\
		\vdots & d_{i,j}^2&\ddots &\\
		1&&&0
	\end{vmatrix}$
	Démontrer que : $$\det(x_1-x_0,\dots,x_n-x_0)^2=\frac{(-1)^{n+1}}{2^n}\Gamma(x_0,\dots,x_n)$$
	
	\underline{Remarque :} Ce résultat permet de calculer le volume du simplexe (généralisation du triangle à une dimension quelconque).
	\\Cette formule se simplifie en dimension 2. On reconnaît la formule de Héron d'Alexandrie : la surface $S$ d'un triangle de côté $a,b,c$ vérifie la relation $$S^2=p(p-a)(p-b)(p-c)$$ où $p$ est le demi périmètre.
	\newpage
\chapter{Groupes}
	\section{Existence d'un idempotent \xens{3}}
	\label{Existence d'un idempotent}
	\textcolor{blue}{\hyperref[Existence d'un idempotent corrigé]{[Corrigé]}}\\
	Soit $(E,\cdot)$ un semi-groupe (c'est à dire que la loi $\cdot$ est une loi de composition interne associative sur $E$) non vide et fini.\\
	Montrer qu'il existe $s\in E$ tel que $s\cdot s=s$.
	
	\section{Sous-semi-groupes finis de $\M_n(\K)$}
	\label{Sous-semi-groupes finis de Mn(K)}
	\textcolor{blue}{\hyperref[Sous-semi-groupes finis de Mn(K) corrigé]{[Corrigé]}}\\
	Soit $(U,\cdot)$ un sous-semi-groupe de $\M_n(\K)$ (i.e $U$ est stable par produit) non vide et fini.\\
	Montrer que $\exists A\in U,\ \Tr(A)\in \crblanc{0}{n}$.
	
	\section{\underline{Groupe composé d'involutions} \ccinp{1}}
	\label{Groupe composé d'involutions}
	\textcolor{blue}{\hyperref[Groupe composé d'involutions corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe de neutre $e$ pour lequel, $\forall x\in G,\ x^2=e$. \\Montrer que $G$ est commutatif.
	\newpage
	\section{\underline{Centre et Commutant} \ccinp{1}}
	\label{Centre et Commutant}
	\label{Centre}
	\textcolor{blue}{\hyperref[Centre et Commutant corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe.\\
	\begin{enumerate}[leftmargin=*]
		\item Montrer que le centre de $G$ définit par:\\
		$$Z(G)= \ens{a\in G,\ \forall x\in G,ax=xa}$$
		est un sous-groupe de $G$.
		\item Montrer que pour tout $x \in G$, le commutant de $x$ définit par:
		$$\mathcal{C}(x)=\ens{a\in G,\ ax=xa}$$
		est un sous groupe de $G$.
	\end{enumerate}
	
	\section{\underline{Théorème de Dixon} \xens{4}}
	\label{Probabilité que deux élémebts d'un groupe commutent}
	\textcolor{blue}{\hyperref[Probabilité que deux éléments d'un groupe commutent corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe fini non commutatif. Montrer que la probabilité que deux éléments choisis au hasard dans $G$ commutent est inférieure à $\displaystyle\frac{5}{8}$.
	
	\section{\underline{Opérations sur les sous-groupes} \ccinp{2}}
	\label{Opérations sur les sous-groupes}
	\textcolor{blue}{\hyperref[Opérations sur les sous-groupes corrigé]{[Corrigé]}}\\
	Soient $G$ un groupe et $H,K$ deux sous-groupes de $G$.
	\begin{enumerate}[leftmargin=*]
		\item Montrer que $H\cap K$ est un sous-groupe de $G$.
		\item Montrer que $H\cup K$ est un sous-groupe de $G$ si et seulement si $H\subset K$ ou $K\subset H$.
	\end{enumerate}
	
	\section{\underline{Sous-groupes finis de $\U$} \telecom{2}}
	\label{Sous-groupes finis de U}
	\textcolor{blue}{\hyperref[Sous-groupes finis de U corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Déterminer les sous-groupes finis de $\U$.
	\end{enumerate}
	On fixe $m$ et $n$ deux entiers naturels.
	\begin{enumerate}[resume,leftmargin=*]
		\item Montrer que $\U_m\subset \U_n\iff m|n$.
		\item Montrer que $\U_m\cap\U_n=\U_{m\wedge n}$.
		\item Déterminer le sous-groupe engendré par $\U_m\cup\U_n$.
	\end{enumerate}
	
	\section{Groupes quasi-cycliques de Prüfer \centraleponts{3}}
	\label{Groupes quasi-cycliques de Prüfer}
	\textcolor{blue}{\hyperref[Groupes quasi-cycliques de Prüfer corrigé]{[Corrigé]}}\\
	Soit $p$ un nombre premier. On pose $G_p=\{z\in \C,\ \exists k\in \N,\ z^{p^k}=1\}$.
	\begin{enumerate}
		\item Montrer que $G_p$ est un sous-groupe de $(\U,\times)$.
		\item Décrire $G_p$ à l'aide des groupes cycliques $\U_{p^k}$ des racines $p^k$-ièmes de l'unité.
		\item Montrer que les sous-groupes stricts de $G_p$ sont cycliques et qu'aucun d'entre eux n'est maximal pour l'inclusion.
	\end{enumerate}
	
	\section{Sous-groupes de GL$_n(\C)$ cyclique \centraleponts{3}}
	\label{Sous-groupes de GLn(C) qui intersectent trivialement le groupe spécial linéaire}
	\textcolor{blue}{\hyperref[Sous-groupes de GLn(C) qui intersectent trivialement le groupe spécial linéaire corrigé]{[Corrigé]}}\\
	On désigne par SL$_n(\C)$ l'ensemble des matrices à coefficients complexes dont le déterminant vaut $1$.\\
	Soit $G$ un sous-groupe fini de GL$_n(\C)$ tel que $G\cap \text{SL}_n(\C)=\{I_n\}$.\\
	Montrer que $G$ est cyclique.
	
	\section{\underline{Matrices inversibles à coefficients entiers} \telecom{2}}
	\label{Matrices inversibles à coefficients entiers}
	\textcolor{blue}{\hyperref[Matrices inversibles à coefficients entiers corrigé]{[Corrigé]}}\\
	On note GL$_n(\Z)$ l'ensemble des matrices inversibles à coefficients entiers de $\M_n(\R)$ dont l'inverse est aussi à coefficients entiers.
	\begin{enumerate}
		\item Montrer que si $M$ est à coefficients dans $\Z$, alors $M\in \text{GL}_n(\Z)$ si et seulement si $\det(M)=\pm1$.
		\item Montrer que GL$_n(\Z)$ est un sous groupe de GL$_n(\R)$.
	\end{enumerate}
	
	\section{\underline{Sous-groupes de $(\Z,+)$} \ccinp{2}}
	\label{Sous-groupes de (Z,+)}
	\textcolor{blue}{\hyperref[Sous-groupes de Z corrigé]{[Corrigé]}}\\
	Soit $G$ un sous-groupe de $\Z$ différent du sous-groupe trivial $\{0\}$.
	\begin{enumerate}
		\item Justifier l'existence de $a=\min G\cap \N^*$.
		\item Montrer que $a\Z\subset G$.
		\item Montrer que $G=a\Z$.
	\end{enumerate}
	\newpage
	\section{\underline{Sous-groupes de $(\R,+)$} \centraleponts{3}}
	\label{Sous-groupes de (R,+)}
	\textcolor{blue}{\hyperref[Sous-groupes de R corrigé]{[Corrigé]}}\\
	Soit $G$ un sous-groupe non trivial de $\R$ i.e $G\ne \{0\}$.
	\begin{enumerate}
		\item Justifier que $G\cap \R^*_+$ possède une borne inférieure que l'on notera $a$.
		\item On suppose $a>0$.
		\begin{enumerate}[label=\alph*.]
			\item On suppose que $a\notin G$. Justifier l'existence de deux éléments distincts $x$ et $y$ de $G$ appartenant à l'intervalle $]a,2a[$.
			\item Aboutir à une contradiction pour en déduire que $a$ est dans $G$.
			\item Montrer que $G=a\Z$
		\end{enumerate}
		\item On suppose que $a=0$.\\
		Montrer que $G$ est dense dans $\R$.
	\end{enumerate}
	
	\subsection{Fonction (multi)périodique \centraleponts{3}}
	Soit $f:\R \longrightarrow \R$ continue, $1$-périodique et $\pi$-périodique.\\
	Montrer que $f$ est constante.
	
	\subsection{$\cos(\N)$ \centraleponts{3}}
	Montrer que $\{\cos(n),n\in \N\}$ est dense dans $[-1,1]$.
	
	\subsection{Valeur d'adhérence du cercle unité \centraleponts{3}}
	Soit $z\in \U$. Montrer que $1$ est une valeur d'adhérence de la suite $(z^n)_{n\in \N}$.
	
	\section{Sous-groupes distingué}
	\label{Sous-groupes distingués}
	\textcolor{blue}{\hyperref[Sous-groupes distingué corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe\\
	On dit que $H$ est un sous-groupe distingué dans $G$ quand $H$ est un sous groupe de $G$ et \[\forall g\in G,\forall h\in H, ghg^{-1}\in H\]
	\begin{enumerate}
		\item Montrer que le centre de $G$ (défini à \ref{Centre}) est distingué dans $G$.
		\item Soit $f$ un morphisme de groupe de $G$ dans $G'$.\\ Montrer que $\Ker(f)$ est distingué dans $G$.
	\end{enumerate}
	
	\section{Nature d'une suite}
	\label{Nature d'une suite}
	\textcolor{blue}{\hyperref[Nature d'une suite corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $x\in \R\backslash\Q$. Montrer qu'il existe une infinité de couples $(p,q)\in \Z\times \N^*$ tels que,
		$$\left|x-\frac{p}{q}\right|<\frac{1}{q^2}$$
		\item Montrer que la suite de terme général $u_n=\ddots\frac{1}{n\sin n}$ diverge.
	\end{enumerate}
	\textit{Un jour peut-être Marconot se rappellera pourquoi il a mit cet exo là et quelle est la mystérieuse question 2...}
	
	\section{Une relation utile sur les morphismes de groupes}
	\label{Une relation utile sur les morphismes de groupes}
	\textcolor{blue}{\hyperref[Une relation utile sur les morphismes de groupes corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe fini et $G'$ un groupe. Soit $f:G\to G'$ un morphisme de groupes.\\
	Montrer que $|G|=|\Ker(f)|\times|\text{Im}(f)|$.
	
	\section{Automorphisme d'inversion \ccinp{1}}
	\label{Automorphisme d'inversion}
	\textcolor{blue}{\hyperref[Automorphisme d'inversion corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe. Montrer que $\fonction{f}{G}{G}{x}{x^{-1}}$ est un automorphisme si et seulement si $G$ est commutatif.
	
	\section{Automorphismes intérieurs \ccinp{1}}
	\label{Automorphismes intérieurs}
	\textcolor{blue}{\hyperref[Automorphismes intérieurs corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe. On définit pour un élément $a\in G$ l'application:
	$$\fonction{\varphi_a}{G}{G}{x}{axa^{-1}}$$
	\begin{enumerate}[leftmargin=*]
		\item Soit $a\in G$. Montrer que $\varphi_a$ est un automorphisme de $G$.
		\item On pose $\mathfrak{J}(G)=\{\varphi_a,a\in G\}$. Montrer que $\mathfrak{J}(G)$ est un sous-groupe de $(\text{Aut}(G),\circ)$.\\
	\end{enumerate}
	
	\section{Endomorphismes continus de $\R$ \telecom{2}}
	\label{Endomorphismes continus de R}
	\textcolor{blue}{\hyperref[Endomorphismes continus de R corrigé]{[Corrigé]}}\\
	Montrer que les endomorphismes de groupe de $(\R,+)$ continus sont les homothéties i.e les applications $x\mapsto \lambda x$ avec $\lambda \in \R$.
	
	\section{Morphismes de $\Q$ dans $\Z$ \ccinp{1}}
	\label{Morphismes de Q dans Z}
	
	\textcolor{blue}{\hyperref[Morphismes de Q dans Z corrigé]{[Corrigé]}}\\
	Déterminer les morphismes de groupes de $(\Q,+)$ dans $(\Z,+)$.
	
	\section{Morphismes de $\Z/n\Z$ dans $\Z/m\Z$ \telecom{2}}
	\label{Morphismes de Z/nZ dans Z/mZ}
	\textcolor{blue}{\hyperref[Morphismes de Z/nZ dans Z/mZ corrigé]{[Corrigé]}}\\
	Soient $m,n\in \N^*$. Déterminer les morphismes de groupes de $(\Z/n\Z,+)$ dans $(\Z/m\Z,+)$.
	
	\section{Morphismes de GL$_n(\R)$ dans $\Z /m\Z$ \xens{5}}
	\label{Morphismes de GLn(R) dans Z/mZ}
	\textcolor{blue}{\hyperref[Morphismes de GLn(R) dans Z/mZ corrigé]{[Corrigé]}}\\
	Soient $m,n \in \N^*$. Déterminer les morphismes de $(\text{GL}_n(\R),\times)$ dans $(\Z /m\Z,+)$.
	
	\section{Quasi-morphisme \centraleponts{3}}
	\label{Quasi-morphisme}
	\textcolor{blue}{\hyperref[Quasi-morphisme corrigé]{[Corrigé]}}\\
	Soient $G$ un groupe, $\delta>0$ et $f:G\to \C$ non bornée telle que
	$$\forall (x,y)\in G^2,\ |f(xy)-f(x)f(y)|\leq \delta$$
	\begin{enumerate}
		\item Montrer que $f$ ne s'annule pas.
		\item Soit $(z_n)\in G^\N$ telle que $(|f(z_n)|)_{n\in \N}$ diverge vers $+\infty$. Pour $x$ fixé, exprimer $f(x)$ à l'aide de $(z^n)_{n\in \N}$.
		\item En déduire que $f$ est un morphisme.
	\end{enumerate}
	
	\section{Morphisme de $\Z^\N$ dans $\Z$ presque nul \centraleponts{3}}
	\label{Morphisme de Z^N dans Z presque nul}
	\textcolor{blue}{\hyperref[Morphisme de Z^N dans Z presque nul corrigé]{[Corrigé]}}\\
	Soit $\Phi:\Z^\N\to\Z$ un morphisme qui s'annule sur les suites presque nulles (i.e qui n'ont qu'un nombre fini de termes non nuls).
	\begin{enumerate}
		\item Soit $(x_n)\in \Z^\N$. Montrer qu'il existe deux suites d'entiers $(y_n)$ et $(z_n)$ telles que $\forall n\in \N,\ x_n=2^ny_n+3^nz_n$.
		\item En déduire que $\Phi$ est nul.
	\end{enumerate}

	\section{Groupes de matrices \xens{3}}
	\label{Sous-groupes de GLn(R)}
	\textcolor{blue}{\hyperref[Sous-groupes de GLn(R) corrigé]{[Corrigé]}}\\
	Soit $G$ une partie de $\M_n(\K)$ tel que $(G,\times)$ est un groupe non trivial.\\
	Montrer qu'il existe $r\in \crblanc{1}{n}$ tel que $G$ soit isomorphe à un sous-groupe de GL$_r(\K)$.
	
	\section{Caractérisation de la finitude d'un groupe par ses sous-groupes \xens{3}}
	\label{Caractérisation de la finitude d'un groupe par ses sous-groupes}
	\textcolor{blue}{\hyperref[Caractérisation de la finitude d'un groupe par ses sous-groupes corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe. Montrer que $G$ est fini si et seulement si l'ensemble de ses sous-groupes est fini.
	
	\section{Sous-groupe des éléments d'ordre fini \xens{4}}
	\label{Sous-groupe des éléments d'ordre fini}
	\textcolor{blue}{\hyperref[Sous-groupes des éléments d'ordre fini corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe et $E$ l'ensemble des éléments d'ordre fini de $G$.\\
	Démontrer que si $E$ est fini alors $E$ est un sous-groupe de $G$.\\
	\textit{On pourra considérer le sous-groupe $H$ engendré par $E$}.
	
	\section{Relations d'équivalence naturelles sur les groupes \etoile{1}}
	\label{Relations d'équivalence naturelles sur les groupes}
	\textcolor{blue}{\hyperref[Relations d'équivalence naturelles sur les groupes corrigé]{[Corrigé]}}\\
	Soit $H$ un sous-groupe d'un groupe $G$. On définit trois relations binaires $\sim,\ \sim_g,\ \sim_d$ sur $G$ de la manière suivante :
	\begin{itemize}
		\item $\forall (x,y)\in G^2,\ x\sim y\iff \exists h\in H,\ y=h^{-1}xh$
		\item $\forall (x,y)\in G^2,\ x\sim_g y\iff \exists h\in H,\ y=hx$
		\item $\forall (x,y)\in G^2,\ x\sim_d y\iff \exists h\in H,\ y=xh$
	\end{itemize}
	Montrer que $\sim,\ \sim_g,\ \sim_d$ sont des relations d'équivalence sur $G$.
	
	\section{\underline{Théorème de Lagrange} \centraleponts{3}}
	\label{Théorème de Lagrange}
	\textcolor{blue}{\hyperref[Théorème de Lagrange corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe commutatif fini. Montrer que l'ordre d'un sous-groupe de $G$ divise l'ordre de $G$.
	
	\section{Un cas particulier du lemme de Cauchy \xens{4} (HP)}
	\label{Un cas particulier du lemme de Cauchy}
	\textcolor{blue}{\hyperref[Un cas particulier du lemme de Cauchy corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $(G,*)$ un groupe d'élément neutre $e$ tel que $\forall x \in G$, $x*x=e$.
		\begin{enumerate}[label=\alph*.]
			\item Montrer que $G$ est commutatif.
			\item On suppose $G$ d'ordre fini. Montrer que $|G|$ est une puissance de $2$.\\    
			\textit{Indication: Montrer que l'on peut munir $G$ d'une structure de $\Z/2\Z$-espace vectoriel.}
		\end{enumerate}
		\item Soit $G$ un groupe de cardinal $2p$ avec $p$ un nombre premier différent de $2$. Montrer que $G$ possède un élément d'ordre $p$.
	\end{enumerate}
	
	\section{Groupe d'ordre premier \ccinp{1}}
	\label{Groupe d'ordre premier}
	\textcolor{blue}{\hyperref[Groupe d'ordre premier corrigé]{[Corrigé]}}\\
	Montrer que tout groupe d'ordre premier est cyclique.
	
	\subsection{Plus petit groupe non commutatif \centraleponts{3}}
	\label{Plus petit groupe non commutatif}
	\textcolor{blue}{\hyperref[Plus petit groupe non commutatif corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Montrer qu'un groupe cyclique est commutatif.
		\item Quel est le plus petit entier $n$ tel qu'il existe un groupe d'ordre $n$ non commutatif ?
	\end{enumerate}
	
	\section{Sous-groupe d'un groupe cyclique \centraleponts{3}}
	\label{Sous-groupe d'un groupe cyclique}
	\textcolor{blue}{\hyperref[Sous-groupe d'un groupe cyclique corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe cyclique d'ordre $n\in \N^*$. Montrer que pour tout diviseur positif $d$ de $n$, il existe un unique sous-groupe de $G$ d'ordre $d$ et en déduire que les sous-groupes d'un groupe cyclique sont cycliques.
	
	\section{Exposant d'un groupe abélien fini \xens{4} (HP)}
	\label{Exposant d'un groupe abélien fini}
	\textcolor{blue}{\hyperref[Exposant d'un groupe abélien fini corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe abélien fini. Pour tout $x\in G$, on note $O(x)$ l'ordre de $x$.
	\begin{enumerate}[leftmargin=*]
		\item Soit $(x,y)\in G^2$ \text{ tel que }$O(x)\wedge O(y)=1$. Montrer que $O(xy)=O(x)O(y)$. Dans le cas général, a-t-on $O(xy)=\ppcm(O(x),O(y))$ ?
		\item Soit $(m,n)\in (\N^*)^2$. Montrer l'existence de $(m',n')\in (\N^*)^2$ tel que $m'|m,\ n'|n,\ \pgcd(m',n')=1 \text{ et } \ppcm(m,n)=m'n'$.
		\item Montrer qu'il existe $z\in G$ tel que $O(z)$ soit le ppcm des ordres des éléments de $G$ (ce ppcm est appelé l'exposant du groupe $G$)
		\item Soient $\K$ un corps commutatif et $G$ un sous-groupe fini du groupe multiplicatif $\K^*$. Démontrer que $G$ est cyclique.\\
		$\textit{On admettra qu'un polynôme à coefficients dans $\K$ admet toujours moins de racines que son degré.}$
	\end{enumerate}
	
	\section{Un groupe d'inversibles non cyclique \centraleponts{3}}
	\label{Un groupe d'inversibles non cyclique}
	\textcolor{blue}{\hyperref[Un groupe d'inversibles non cyclique corrigé]{[Corrigé]}}\\
	Soit $n\geq 3$ un entier.
	\begin{enumerate}
		\item Soit $a$ un entier impair. Montrer que $a^{2^{n-2}}\equiv1[2^n]$.
		\item En déduire que le groupe des inversibles de $\Z/2^n\Z$, $\left(\Z/2^n\Z\right)^\times$ n'est pas cyclique.
	\end{enumerate}
	
	\section{Ordre dans un groupe de cardinal pair \telecom{2}}
	\label{Ordre dans un groupe de cardinal pair}
	\textcolor{blue}{\hyperref[Ordre dans un groupe de cardinal pair corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe de cardinal $2n$.
	\begin{enumerate}
		\item Démontrer que la relation binaire $\mathcal R$ définie sur $G$ par :
		$$x\mathcal Ry\iff (x=y\text{ ou }x=y^{-1})$$
		est une relation d'équivalence sur $G$.
		\item En déduire que $G$ admet un élément d'ordre deux.
	\end{enumerate}
	
	\section{\underline{Ordre dans $\Z/n\Z$} \telecom{2}}
	\label{Ordre dans Z/nZ}
	\textcolor{blue}{\hyperref[Ordre dans Z/nZ corrigé]{[Corrigé]}}\\
	Soit $(k,n)\in \Z\times \N^*$. On note $d$ l'ordre de $\overline{k}$ dans $\Z/n\Z$.\\
	Montrer que $d=\displaystyle\frac{n}{n\land k}$.
	
	\section{Passage par les groupes \centraleponts{2}}
	\label{Passage par les groupes}
	\textcolor{blue}{\hyperref[Passage par les groupes corrigé]{[Corrigé]}}\\
	Le but de cet exercice est de montrer qu'il n'existe pas d'entier $n\geq 2$ tel que $n|2^n-1$. On raisonne par l'absurde et on note $p$ le plus petit diviseur premier d'un tel entier $n$. On note aussi $m$ la classe de $2$ dans $(\Z/p\Z)^\times$.
	\begin{enumerate}
		\item Montrer que $m|p-1$.
		\item Montrer que $m|n$.
		\item Conclure.
	\end{enumerate}
	
	\section{Groupe infini non monogène \ccinp{1}}
	\label{Groupe infini non monogène}
	\textcolor{blue}{\hyperref[Groupe infini non monogène corrigé]{[Corrigé]}}\\
	Donner un exemple de groupe infini non monogène.
	
	\section{Groupe non cyclique \telecom{2}}
	\label{Groupe non cyclique}
	\textcolor{blue}{\hyperref[Groupe non cyclique corrigé]{[Corrigé]}}\\
	Soit $(n,m)\in {(\N^*)}^2$. Montrer que $(\Z/n\Z\times\Z/m\Z,+)$ est cyclique si et seulement si $m\land n=1$.
	
	\section{Ordre dans le groupe symétrique \telecom{2}}
	\label{Ordre dans le groupe symétrique}
	\textcolor{blue}{\hyperref[Ordre dans le groupe symétrique corrigé]{[Corrigé]}}\\
	Soit $n\in \N^*$. Déterminer l'ordre de $\sigma\in \mathcal S_n$ en fonction de sa décomposition en produit de cycles à supports disjoints.
	
	\section{Sous-groupe engendré par les nombres premiers \ccinp{1}}
	\label{Sous-groupe engendré par les nombres premiers}
	\textcolor{blue}{\hyperref[Sous-groupe engendré par les nombres premiers corrigé]{[Corrigé]}}\\
	Déterminer le sous-groupe engendré par l'ensemble $\mathcal P$ des nombres premiers dans $(\C^*,\times)$.
	
	\section{Sous-groupe engendré par le complémentaire d'un sous-groupe \telecom{2}}
	\label{Sous-groupe engendré par le complémentaire d'un sous-groupe}
	\textcolor{blue}{\hyperref[Sous-groupe engendré par le complémentaire d'un sous-groupe corrigé]{[Corrigé]}}\\
	Soit $H$ un sous-groupe strict d'un groupe $G$.\\
	Déterminer le sous-groupe engendré par le complémentaire de $H$ dans $G$.
	
	\section{Partie génératrice \etoile{3} (HP)}
	\label{Partie génératrice}
	\textcolor{blue}{\hyperref[Partie génératrice corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soient $G$ un groupe fini et $H$ un sous-groupe strict de $G$. Montrer que $2|H|\leq |G|$.
		\item Montrer que tout groupe fini $G$ de cardinal $n\geq 2$ possède une partie génératrice constituée d’au plus $\log_2(n)$ éléments.
		\item A l'aide de l'exercice \ref{Un cas particulier du lemme de Cauchy} donner un groupe pour lequel cette borne est optimale.
	\end{enumerate}
	
	\section{Groupe alterné \centraleponts{3}}
	\label{Groupe alterné}
	\textcolor{blue}{\hyperref[Groupe alterné corrigé]{[Corrigé]}}\\
	Soit $n\geq 3$.\\
	On note $\A_n$ l'ensemble des permutations de $\mathcal S_n$ de signature 1.
	\begin{enumerate}
		\item Montrer que $\A_n$ est un sous-groupe de $\mathcal S_n$.
		\item Montrer que $\A_n$ est engendré par les $3$-cycles.
	\end{enumerate}
	
	\section{Cardinal minimal d'une famille de transpositions engendrant $\mathcal S_n$ \centraleponts{3}}
	\label{Cardinal minimal d'une famille de transpostions engendrant Sn}
	\textcolor{blue}{\hyperref[Cardinal minimal d'une famille de transpositions engendrant Sn corrigé]{[Corrigé]}}\\
	Soit $n\in \N^*$. On note $(e_1,\dots,e_n)$ la base canonique de $\R^n$ et pour $i\in \crblanc{2}{n}$ on note $t_i=(1,i)$. Enfin pour tout $s\in \mathcal S_n$ on définit $u_s\in \mathcal L(\R^n)$ par $\forall i\in \crblanc{1}{n},\ u(e_i)=e_{s(i)}$.
	\begin{enumerate}
		\item Montrer que $(t_2,\dots,t_n)$ engendre $\mathcal S_n$.
		\item Interpréter géométriquement $u_s$ lorsque $s$ est une transposition.
		\item Soit $s=(1,\dots,n)$. On suppose que $s$ est la composée de $p$ transpositions. Montrer que $p\geq n-1$.
		\item Quel est le cardinal minimal d'une famille de transposition génératrice de $\mathcal S_n$ ?
	\end{enumerate}
	
	\section{Partie génératrice de $\O(E)$ \xens{4}}
	\label{Partie génératrice de O(E)}
	\textcolor{blue}{\hyperref[Partie génératrice de O(E) corrigé]{[Corrigé]}}\\
	Soit $E$ un espace euclidien. Montrer que $\O(E)$ est engendré par les réflexions.
	
	\section{Groupe dans un plan euclidien \centraleponts{3}}
	\label{Groupe dans un plan euclidien}
	\textcolor{blue}{\hyperref[Groupe dans un plan euclidien corrigé]{[Corrigé]}}\\
	Soit $E$ un plan euclidien.
	\begin{enumerate}[leftmargin=*]
		\item Déterminer les sous-groupes finis de $\mathcal{SO}(E)$.
		\item Caractériser les sous-groupes finis de $\O(E)$.
		\item Soit $G$ un sous-groupe fini de GL$(E)$. Déterminer un produit scalaire pour lequel tous les éléments de $G$ sont des automorphismes orthogonaux.
	\end{enumerate}
	
	\section{Groupe diédral \centraleponts{3}}
	\label{Groupe diédral}
	\textcolor{blue}{\hyperref[Groupe diédral corrigé]{[Corrigé]}}\\
	Soit $n\in \N^*$. On appelle groupe diédral d'ordre $2n$ et on note $D_n$ le sous-groupe de GL$_2(\R)$ engendré par les matrices $R=\begin{pmatrix}
		\cos\left(\frac{2\pi}{n}\right)&-\sin\left(\frac{2\pi}{n}\right)\\
		\sin\left(\frac{2\pi}{n}\right)&\cos\left(\frac{2\pi}{n}\right)
	\end{pmatrix}$ et $S=\begin{pmatrix}
		1&0\\
		0&-1
	\end{pmatrix}$.
	\begin{enumerate}
		\item Montrer que $SR=R^{-1}S$.
		\item En déduire que $D_n=\{I_2,R,\dots,R^{n-1},S,SR,\dots,SR^{n-1}\}$.
		\item Déterminer les groupes diédraux abéliens.
	\end{enumerate}
	On note $G_n$ le polygone du plan régulier à $n$ côtés centrés en l'origine et dont l'un des sommets est $(1,0)$.
	\begin{enumerate}[resume]
		\item Déterminer les coordonnées des sommets de $G_n$
		\item Soit $\Delta_n$ l'ensemble des isométries vectorielles de $\R^2$ préservant $G_n$, c'est-à-dire effectuant une permutation de ses sommets. Après avoir brièvement justifié que $\Delta_n$ est un groupe, montrer qu'il est isomorphe à $D_n$.
	\end{enumerate}
	
	\section{\underline{Matrices de permutation} \telecom{1}}
	\label{Matrices de permutation}
	\textcolor{blue}{\hyperref[Matrices de permutation corrigé]{[Corrigé]}}\\
	Pour $\sigma\in \mathcal{S}_n$ on pose $P_\sigma=(\delta_{i,\sigma(j)})_{1\leq i,j\leq n}$ et on définit $\mathcal{P}_n=\{ P_\sigma,\ \sigma\in S_n\}$ l'ensemble des matrices de permutation d'ordre $n$. $\delta_{i,j}$ représente le symbole de Kronecker, $\delta_{i,j}=\begin{cases}1&\mbox{ si }i=j\\0&\mbox{ si }i\ne j\end{cases}$.
	\begin{enumerate}
		\item Montrer que $\fonction{f}{\mathcal{S}_n}{\mathcal{P}_n}{\sigma}{P_\sigma}$ est un isomorphisme de groupes.
		\item Soit $\sigma\in \mathcal{S}_n$. Que vaut $P_\sigma^\top$ ?
		\item Montrer que $\forall \sigma\in \mathcal{S}_n,\ \det P_\sigma=\varepsilon(\sigma)$.
	\end{enumerate}
	
	\section{Groupe dérivé \xens{4}}
	\label{Groupe dérivé}
	\textcolor{blue}{\hyperref[Groupe dérivé corrigé]{[Corrigé]}}\\
	Soit $G$ un groupe. On note $D$ le sous-groupe (dit dérivé) de $G$ engendré par les éléments de la forme $xyx^{-1}y^{-1}$ (avec $x,y\in G$) et $C$ celui engendré par les éléments de la forme $x^2$ ($x\in G$).
	\begin{enumerate}
		\item Montrer que $D\subset C$.
		\item On suppose que $G$ est engendré par les éléments de $G$ qui vérifient $x=x^{-1}$. Montrer que $D=C$.
		\item 
		\begin{enumerate}[label=\alph*.]
			\item Montrer que toute matrice de SO$_2(\R)$ peut s'écrire comme un produit de deux symétries. Y a-t-il unicité ?
			\item Montrer que $\O_2(\Q)=\M_2(\Q)\cap\O_2(\R)$ est un groupe puis que SO$_2(\Q)=\{M\in \O_2(\Q),\ \det(M)=1\}$ est un sous-groupe commutatif de $\O_2(\Q)$. Démontrer que le groupe dérivé de $\O_2(\Q)$ est un sous-groupe strict de SO$_2(\Q)$.
		\end{enumerate}
	\end{enumerate}
	
	\section{Sous-groupe discret de $\C$ et de SL$_2(\R)$ \xens{5}}
	\label{Sous-groupe discret de C et de SL2(R)}
	\textcolor{blue}{\hyperref[Sous-groupe discret de C et de SL2(R) corrigé]{[Corrigé]}}\\
	On dit qu'une partie $A$ d'un espace vectoriel normée est discrète si en tout point $x$ de $A$ on peut trouver un voisinage qui ne contient pas d'élément de $A$, hormis $x$. On s'intéresse dans cet exercice aux sous-groupes discrets de $(\C,+)$ et de $(\text{SL}_2(\R),\times)$.
	\begin{enumerate}
		\item Donner des exemples de sous-groupes discrets de $\C$ et de SL$_2(\R)$.
		\item Soit $\Gamma$ un sous-groupe discret non trivial de $(\C,+)$ et $\lambda\in \C^*$ vérifiant $\lambda\Gamma=\Gamma$.\\
		Montrer que $\lambda^4=1$ ou $\lambda^6=1$.
		\item Soient $\lambda\in \R^*$ et $\Gamma$ le sous-groupe engendré par $\begin{pmatrix}1&1\\0&1\end{pmatrix}$ et $\begin{pmatrix}\lambda&0\\0&\lambda^{-1}\end{pmatrix}$. A quelle condition sur $\lambda$ le sous-groupe $\Gamma$ est-il discret ?
	\end{enumerate}
	
	
	\newpage
\chapter{Anneaux et corps}
	Pour $(A,+,\times)$ un anneau on désignera par $0_A$ ou simplement $0$ son neutre additif et par $1_A$ ou simplement $1$ son neutre multiplicatif.\\
	
	\section{Centre d'un anneau \etoile{1} (HP)}
	\label{Centre d'un anneau}
	\textcolor{blue}{\hyperref[Centre d'un anneau corrigé]{[Corrigé]}}\\
	Soit $(A,+,\times)$ un anneau. On pose $Z(A)=\{x\in A,\ \forall a\in A,\ ax=xa\}$.
	\begin{enumerate}
		\item Montrer que $Z(A)$ est un sous-anneau commutatif de $A$.
		\item On suppose que $Z(A)$ est un corps. Montrer que $(A,+,\times,\cdot)$ où $\cdot$ est la loi $\times$ est une $Z(A)$-algèbre unitaire, associative et commutative.
	\end{enumerate}
	
	\section{Calcul d'un inverse \centraleponts{3}}
	\label{Calcul d'un inverse}
	\textcolor{blue}{\hyperref[Calcul d'un inverse corrigé]{[Corrigé]}}\\
	Soient $A$ un anneau et $(a,b)\in A^2$ tel que $1-ab$ est inversible.
	\begin{enumerate}
		\item On suppose que $ab$ est nilpotent.\\
		Montrer que $ba$ est nilpotent puis que $1-ba$ est inversible.
		\item Montrer que $1-ba$ est inversible.
	\end{enumerate}
	
	\section{Anneau de Boole \telecom{2}}
	\label{Anneau de Boole}
	\textcolor{blue}{\hyperref[Anneau de Boole corrigé]{[Corrigé]}}\\
	Soit $E$ un ensemble non vide. Pour $A,B\in \mathcal P(E)$ on définit la différence symétrique de $A$ par $B$ par $A\Delta B=(A\backslash B)\cup (B\backslash A)$.
	\begin{enumerate}
		\item Montrer que $(\mathcal P(E),\Delta,\cap)$ est un anneau commutatif.
		\item Quels sont les éléments de $\mathcal P(E)$ inversibles pour la loi $\cap$ ?
		\item l'anneau $(\mathcal P(E),\Delta,\cap)$ est-t-il intègre ?
	\end{enumerate}
	
	\subsection{L'anneau de Boole est principal \centraleponts{3}}
	On suppose que $E$ est fini. Montrer que les idéaux de $(\mathcal P(E),\Delta,\cap)$ sont de la forme $\mathcal P(F)$ avec $F\subset E$.
	
	\section{Condition suffisante pour qu'un anneau soit commutatif \centraleponts{3}}
	\label{Condition suffisante pour qu'un anneau soit commutatif}
	\textcolor{blue}{\hyperref[Condition suffisante pour qu'un anneau soit commutatif corrigé]{[Corrigé]}}\\
	Soit $(A,+,\times)$ un anneau.
	\begin{enumerate}
		\item On suppose que $\forall x\in A,\ x^2=x$\\
		Montrer que $A$ est commutatif.
		\item On suppose que $\forall x\in A, x^3=x$
		\begin{enumerate}[label=\alph*.]
			\item Déterminer les éléments nilpotents de $A$.
			\item Soient $e\in A$ tel que $e^2=e$, $a\in A$ et $b=ea(1-e)$.\\
			Calculer $b^2$ et en déduire que $ea=ae$.\\
			En déduire que pour tout $x\in A$, $x^2\in Z(A)$ où $Z(A)$ désigne le centre de $(A,+,\times)$ (cf. \ref{Centre d'un anneau}).
			\item Montrer que pour tout $x\in A$, $(2x,3x)\in Z(A)^2$ et en déduire que $A$ est commutatif.
		\end{enumerate}
	\end{enumerate}
	\textit{De manière plus générale, un théorème dû à Jacobson énonce que si pour tout $x\in A$ il existe $n\in \N$ tel que $x^n=x$ alors $A$ est commutatif.}
	
	\section{Anneaux commutatifs ou anti-commutatifs \centraleponts{4}}
	\label{Anneaux commutatifs ou anti-commutatifs}
	\textcolor{blue}{\hyperref[Anneaux commutatifs ou anti-commutatifs corrigé]{[Corrigé]}}\\
	Un pseudo-anneau est un triplet $(A,+,\times)$ qui vérifie tous les axiomes de la structure d'anneau sauf l'existence de l'élément neutre unité.\\
	On se donne $A$ un pseudo-anneau tel que $\forall x,y\in A,xy\in \{yx,-yx\}$. Montrer que $A$ est commutatif ou anti-commutatif.\\
	Que dire si $A$ est un anneau ?
	
	\section{Anneau régulier \centraleponts{4}}
	\label{Anneau régulier}
	\textcolor{blue}{\hyperref[Anneau régulier corrigé]{[Corrigé]}}\\
	On dit qu'un anneau $A$ est \textit{régulier} lorsque $\forall a\in A,\exists u\in A,\ aua=a$.
	\begin{enumerate}
		\item $(\Z,+,\times)$ est-t-il régulier ?
		\item Un corps est-t-il régulier ?
		\item \begin{enumerate}[label=\alph*.]
			\item Montrer que si $A$ et $B$ sont deux anneaux réguliers alors l'anneau produit $A\times B$ est régulier.
			\item Soit $n\in \N$. Déterminer une condition nécessaire et suffisante pour que $(\Z/n\Z,+,\times)$ soit régulier.
		\end{enumerate}
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $E$ un espace vectoriel. Montrer que $(\mathcal L(E),+,\circ)$ est régulier.
			\item Soit $n\in \N^*$. On note $A$ la matrice de $\M_n(\K)$ n'ayant que des $0$, sauf sur sa sur-diagonale où il n'y a que des $1$.\\
			Exhiber une matrice $U\in \M_n(\K)$ telle que $AUA=A$.
		\end{enumerate}
		\item Montrer que le centre d'un anneau régulier est régulier (cf. \ref{Centre d'un anneau}).
	\end{enumerate}
	
	\section{Anneau intègre fini \telecom{1}}
	\label{Anneau intègre fini}
	\textcolor{blue}{\hyperref[Anneau intègre fini corrigé]{[Corrigé]}}\\
	Montrer qu'un anneau intègre fini est un corps.
	
	\section{Anneau principal (1) \telecom{3}}
	\label{Anneau principal (1)}
	\textcolor{blue}{\hyperref[Anneau principal (1) corrigé]{[Corrigé]}}\\
	Soit $(A,+,\times)$ un anneau commutatif. On dit qu'un idéal $I$ de $A$ est \textit{principal} lorsqu'il existe $x\in A$ tel que $I=xA$. Lorsque tous les idéaux de $A$ sont principaux, on dit que $A$ est un anneau principal.
	\begin{enumerate}
		\item Montrer que $(\Q,+,\times)$ est un anneau principal.
		\item Montrer que $(\Z,+,\times)$ est un anneau principal.
		\item Montrer que $(\D,+,\times)$ est un anneau principal.
		\item Soit $n\in \N^*$. Montrer que $(\Z/n\Z,+)$ est principal.
	\end{enumerate}
	
	\section{Anneau principal (2) \etoile{3} (HP)}
	\label{Anneau principal (2)}
	\textcolor{blue}{\hyperref[Anneau principal (2) corrigé]{[Corrigé]}}\\
	Soit $(A,+,\times)$ un anneau commutatif. On dit qu'un idéal $I$ de $A$ est \textit{principal} lorsqu'il existe $x\in A$ tel que $I=xA$. Lorsque tous les idéaux de $A$ sont principaux, on dit que $A$ est un anneau principal.
	\begin{enumerate}
		\item $(\Z,+,\times)$ est-il principal ?
		\item $(\Z[X],+,\times)$ est-il principal ?
		\item Soit $A$ un anneau commutatif. A quelle condition $A[X]$ est-il principal ?\\
		\textit{On admettra que l'on peut définir une notion de degré dans $A[X]$ analogue à celle de $\C[X]$ ou de $\R[X]$.}
	\end{enumerate}
	
	\section{Anneau euclidien \centraleponts{2}}
	\label{Anneau euclidien}
	\textcolor{blue}{\hyperref[Anneau euclidien corrigé]{[Corrigé]}}\\
	Soit $(A,+,\times)$ un anneau intègre.\\
	On dit que $A$ est un anneau euclidien lorsqu'il existe une application $\varphi:A\backslash\{0_A\}\longrightarrow \N$ telle que pour $a\in A$ et $b\in A\backslash\{0_A\}$, il existe $(q,r)\in A^2$ tel que:
	$$a=bq+r\text{ et }(r=0_A\text{ ou }\varphi(r)<\varphi(b))$$
	\begin{enumerate}
		\item Donner des exemples d'anneaux euclidiens.
		\item Montrer qu'un anneau euclidien est principal (cf. \ref{Anneau principal (1)}).
	\end{enumerate}
	
	\section{Entiers de Gauss}
	\label{Entiers de Gauss}
	\textcolor{blue}{\hyperref[Entiers de Gauss corrigé]{[Corrigé]}}\\
	On note $\Z[i]=\{a+ib,\ (a,b)\in \Z^2\}$.
	
	\subsection{Structure et inversibles de $\Z[i]$ \ccinp{2}}
	\begin{enumerate}[leftmargin=*]
		\item Montrer que $(\Z[i],+,\times)$ est un anneau commutatif intègre.
		\item Déterminer $\Z[i]^\times$, l'ensemble des inversibles de $\Z[i]$.
	\end{enumerate}
	On admet ses résultats comme connus dans les exercices qui suivent et on pose pour tous $x\in \Z[i],\ N(x)=x\overline x$.
	
	\subsection{$\Z[i]$ est euclidien \centraleponts{3}}
	Démontrer que :
	$$\forall (x,y)\in \Z[i]\times \Z[i]\backslash\{0\},\ \exists (q,r)\in \Z[i],\ x=qy+r\land N(r)<N(a)$$
	En déduire que tous les idéaux de $\Z[i]$ sont de la forme $x\Z[i]$ avec $x\in \Z[i]$.
	
	\subsection{Une somme \xens{4}}
	Soit $(n,k)\in \N \times \N^*$.\\
	Montrer que $\displaystyle\frac{1}{4}\sum\limits_{\substack{x\in \Z[i]\\N(x)=n}}{x^k}\in \Z$.
	
	\subsection{Irréductibles de $\Z[i]$ \centraleponts{4}}
	\begin{enumerate}[leftmargin=*]
		\item Soit $a\in \Z[i]$. Montrer que si $N(a)$ est premier alors $a$ est irréductible dans $\Z[i]$, c'est à dire:\\
		$a\notin \Z[i]^\times\land\forall (b,c)\in \Z[i]^2,\ a=bc\implies (b\in \Z[i]^\times\vee c\in\Z[i]^\times)$.
		\item Soit $p$ un nombre premier. Montrer l'équivalence des propriétés suivantes :
		\begin{enumerate}[label=(\roman*)]
			\item $p$ est irréductible dans $\Z[i]$;
			\item $p\equiv 3[4]$;
			\item il n'existe pas $a\in \Z[i]$ tel que $p=N(a)$.
		\end{enumerate}
		On admettra que $-1$ est un carré dans $\Z/p\Z$ si et seulement si $p$ n'est pas congrus à $3$ modulo $4$.
		\item En déduire tous les irréductibles de $\Z[i]$.
	\end{enumerate}
	
	\section{Anneau Noethérien \etoile{4} (HP)}
	\label{Anneau Noethérien}
	\textcolor{blue}{\hyperref[Anneau Noethérien corrigé]{[Corrigé]}}\\
	Un anneau commutatif $(A,+,\times)$ est dit \textit{noethérien} lorsque tous ses idéaux sont engendrés par un nombre fini d'élément (on dit que ses idéaux sont de types finis).\\
	Soit $A$ un anneau commutatif. Montrer que les trois propositions suivantes sont équivalentes :
	\begin{enumerate}[label=(\roman*)]
		\item $A$ est noethérien;
		\item toute suite croissante (pour l'inclusion) d'idéaux de $A$ stationne;
		\item Tout ensemble non vide d'idéaux de $A$ admet un élément maximal pour l'inclusion.
	\end{enumerate}
	\textit{\underline{Remarque} : l'appellation d'anneau noethérien provient de la mathématicienne Emmy Noether qui s'est intéressée aux propriétés de tels anneaux. Son influence sur les sciences s'étend aussi à la physique notamment par le théorème de Noether qui explique le lien fondamental entre la symétrie et les lois de conservation.}
	
	\subsection{$\Z$ et $\K[X]$ sont noethériens \telecom{2}}
	\begin{enumerate}[leftmargin=*]
		\item Montrer que toute suite croissante d'idéaux de $\Z$ stationne.
		\item Montrer que toute suite croissante d'idéaux de $\K[X]$ stationne.
	\end{enumerate}
	
	\section{Morphismes d'anneaux de fonctions réelles}
	\label{Morphismes d'anneaux de fonctions réelles}
	\textcolor{blue}{\hyperref[Morphismes d'anneaux de fonctions réelles corrigé]{[Corrigé]}}\\
	On note $\mathcal C(\R,\R)$ l'ensemble des fonctions continues de $\R$ dans $\R$ et $\mathcal D(\R,\R)$ celui des fonctions dérivables de $\R$ dans $\R$.\\
	Déterminer les morphismes d'anneaux de $\mathcal C(\R,\R)$ dans $\mathcal D(\R,\R)$.\\
	\textit{\underline{Indication :} on pourra montrer qu'un tel morphisme est nécessairement à valeurs dans l'ensemble des fonctions constantes}
	
	\section{Caractérisation d'un corps par ses idéaux \telecom{2}}
	\label{Caractérisation d'un corps par ses idéaux}
	\textcolor{blue}{\hyperref[Caractérisation d'un corps par ses idéaux corrigé]{[Corrigé]}}\\
	Soit $A$ un anneau non nul.\\
	Montrer que $A$ est un corps si et seulement si ses seuls idéaux sont $\{0_A\}$ et $A$.
	
	\section{Opérations sur les idéaux, idéaux principaux \centraleponts{3}}
	\label{Opérations sur les idéaux, idéaux principaux}
	\textcolor{blue}{\hyperref[Opérations sur les idéaux, idéaux principaux corrigé]{[Corrigé]}}\\
	Soit $A$ un anneau commutatif. On dit qu'un idéal $I$ de $A$ est \textit{principal} s'il existe $x\in A$ tel que $I=xA$.
	\begin{enumerate}
		\item Montrer que si $I$ et $J$ sont deux idéaux de $A$ alors $I+J$ et $I\cap J$ sont aussi des idéaux de $A$.
		\item
		\begin{enumerate}[label=\alph*.]
			\item Soit $(a,b)\in \Z^2$. Donner deux entiers $c$ et $d$ tels que $a\Z\cap b\Z=c\Z$ et $a\Z+b\Z=d\Z$.
			\item Montrer que si $I,J$ et $I+J$ sont des idéaux principaux de $A$ alors $I\cap J$ en est aussi un.
		\end{enumerate}
	\end{enumerate}
	
	\section{Idéal premier \telecom{2}}
	\label{Idéal premier}
	\textcolor{blue}{\hyperref[Idéal premier corrigé]{[Corrigé]}}\\
	Soit $A$ un anneau commutatif. On dit qu'un idéal $I$ de $A$ est \textit{premier} lorsque $A\backslash I$ est stable pour le produit i.e:
	$$\forall (x,y)\in A^2,\ xy\in I \implies (x\in I \text{ ou } y\in I)$$\\
	Soit $A$ un anneau commutatif dont tous les idéaux sont premiers. Montrer que $A$ est un corps.
	
	\section{Idéal maximal \centraleponts{4}}
	\label{Idéal maximal}
	\textcolor{blue}{\hyperref[Idéal maximal corrigé]{[Corrigé]}}\\
	Soient $A$ un anneau commutatif et $I$ un idéal de $A$.\\
	On dit que $I$ est un idéal \textit{maximal} de $A$ lorsqu'il est contenu dans exactement deux idéaux de $A$, $A$ et lui-même. (Ainsi $A$ n'est pas maximal)
	\begin{enumerate}
		\item Montrer que $I$ est maximal si et seulement si $\forall a\in A\backslash I,\ I+aA=A$.
		\item Montrer que tout idéal maximal est premier (cf. \ref{Idéal premier}).
		\item Donner un exemple d'idéal premier.
		\item On note $A=(\mathcal C^{\infty}(\R,\R),+,\times)$. Pour chacun des cas présentés, Vérifier que l'ensemble est un idéal de $A$ et préciser s'il est principal, premier, maximal.
		\begin{enumerate}[label=\alph*.]
			\item l'ensemble $I$ des fonctions de $A$ qui s'annule en $0$.\\
			\textit{On pourra considérer la fonction $g$ définie par $g(x)=\displaystyle\frac{f(x)}{x}$ si $x\ne 0$ et $g(0)=f'(0)$.}
			\item l'ensemble $J$ des fonctions $f$ de $A$ telle que $\forall k\in \N,\ f^{(k)}(0)=0$.\\
			\textit{On pourra déterminer une fonction de $J$ et montrer qu'elle ne s'annule qu'en $0$.}
		\end{enumerate}
	\end{enumerate}
	
	\section{Idéaux d'un espace de fonction}
	\label{Idéaux d'un espace de fonction}
	\textcolor{blue}{\hyperref[Idéaux d'un espace de fonction corrigé]{[Corrigé]}}\\
	Soit $K$ une partie compacte d'un espace vectoriel normé. On note $A=\mathcal C^0(K,\R)$. Pour tout $x\in K$, on note $m_x=\{f\in A,\ f(x)=0\}$.
	\begin{enumerate}
		\item Montrer que tout idéal propre de $A$ (i.e différent de $E$.) est contenu dans un $m_x$.
		\item Donner un contre exemple quand $K$ n'est pas compact.
	\end{enumerate}
	
	\section{\underline{Radical d'un idéal} \ccinp{1}}
	\label{Radical d'un idéal}
	\textcolor{blue}{\hyperref[Radical d'un idéal corrigé]{[Corrigé]}}\\
	Soit $A$ un anneau commutatif. Pour tout idéal $I$ de $A$ On note
	$$R(I)=\{x\in A,\ \exists n\in \N,\ x^n\in I\}$$
	L'ensemble $R(I)$ est appelé \textit{radical} de $I$.
	\begin{enumerate}
		\item Soit $I$ un idéal de $A$. Montrer que $R(I)$ est un idéal de $A$ qui contient $I$.
		\item Soit $I$ un idéal de $A$. Montrer que $R(R(I))=R(I)$.
		\item Soient $I$ et $J$ deux idéaux de $A$. Montrer que $R(I\cap J)=R(I)\cap R(J)$.
	\end{enumerate}
	
	\section{Nilradical \telecom{2}}
	\label{Nilradical}
	\textcolor{blue}{\hyperref[Nilradical corrigé]{[Corrigé]}}\\
	Soit $A$ un anneau commutatif. On appelle \textit{nilradical} de $A$ l'ensemble $\mathcal N(A)$ des éléments nilpotents de $A$.
	\begin{enumerate}
		\item Montrer que le nilradical est un idéal.
		\item Soit $n\in \N^*$. Déterminer le nilradical de $\Z/n\Z$.
	\end{enumerate}
	
	\section{Radical de Jacobson \centraleponts{2}}
	\label{Radical de Jacobson}
	\textcolor{blue}{\hyperref[Radical de Jacobson corrigé]{[Corrigé]}}\\
	Soit $A$ un anneau commutatif. Un idéal de $A$ est dit \textit{maximal} lorsque qu'il est contenu dans exactement deux idéaux de $A$, $A$ et lui-même (ainsi $A$ n'est pas maximal).\\
	On note $A^\times$ l'ensemble des inversibles de $A$ et $\mathcal I(A)$ l'ensemble des idéaux maximaux de $A$. Enfin on pose le radical de Jacobson : $$J=\bigcap\limits_{I\in \mathcal I(A)}I$$
	\begin{enumerate}
		\item Soit $I$ un idéal de $A$. Montrer que $I$ est maximal si et seulement si $\forall a\in A\backslash I,\ I+aA=A$.
		\item Montrer que $x\in J\iff \forall a\in A,\ 1_A-ax\in A^\times$.\\
		\textit{On admettra le théorème de Krull : Dans un anneau commutatif unitaire, tout idéal autre que l'anneau lui-même est contenu dans un idéal maximal.}
	\end{enumerate}
	
	\section{Idéaux de $\M_n(\K)$}
	\label{Idéaux de Mn(K)}
	\textcolor{blue}{\hyperref[Idéaux de Mn(K) corrigé]{[Corrigé]}}\\
	Un sous-groupe $J$ de $(\M_n(\K),+)$ est appelé \textit{idéal à droite} de $\M_n(\K)$ lorsque :
	$$\forall A\in \M_n(\K),\ \forall M\in J,\ MA\in J$$
	Un sous-groupe $J$ de $(\M_n(\K),+)$ est appelé \textit{idéal à gauche} de $\M_n(\K)$ lorsque :
	$$\forall A\in \M_n(\K),\ \forall M\in J,\ AM\in J$$
	Lorsque $J$ est à la fois un idéal à gauche et à droite, on dit que $J$ est un \textit{idéal bilatère}.
	
	\subsection{Idéaux bilatère \centraleponts{3}}
	Soit $J$ un idéal bilatère de $\M_n(\K)$.
	\begin{enumerate}
		\item Montrer que si $I_n\in J$ alors $J=\M_n(\K)$.
		\item Montrer que si $J$ contient une matrice inversible alors $J=\M_n(\K)$.
		\item On suppose que $J$ n'est pas réduit à $\{0\}$ et on fixe une matrice $A\in J$ de rang $r$ non nul.
		\begin{enumerate}[label=\alph*.]
			\item Montrer que $\begin{pmatrix}
				I_r & 0\\
				0   & 0
			\end{pmatrix}\in J$
			\item Justifier l'existence de $n-r+1$ matrices $A_1,\dots,A_{n-r+1}$, toutes équivalentes à $A$ et telles que la somme $A_1+\dots+A_{n-r+1}$ est inversible. 
		\end{enumerate}
		\item Que peut-on dire des idéaux bilatères de $\M_n(\K)$ ?
	\end{enumerate}
	
	\subsection{Idéaux à droite \ccinp{2}}
	Soit $E$ un sous-espace vectoriel de $\M_{n,1}(\K)$. On pose $J_E=\{M\in \M_n(\K),\ \text{Im}(M)\subset E\}$.
	\begin{enumerate}
		\item Montrer que $J_E$ est un idéal à droite de $\M_n(\K)$.
		\item Soient $p,q\in \crblanc{1}{n},\ u\in \mathcal L(\R^p,\R^n)$ et $v\in \mathcal L(\R^q,\R^n)$ tels que l'image de $v$ est contenue dans celle de $u$. On fixe $S$ un supplémentaire de $\Ker(u)$ dans $\R^p$ et on note $(e_1,\dots,e_q)$ la base canonique de $\R^q$.
		\begin{enumerate}[label=\alph*.]
			\item Justifier l'existence, pour tout $i\in \crblanc{1}{q}$, d'un unique élément $\varepsilon_i\in S$ tel que $u(\varepsilon_i)=v(e_i)$.
			\item En déduire l'existence de $w\in \mathcal L(\R^q,\R^p)$ telle que $v=u\circ w$.
			\item Soit $(A,B)\in \M_{n,p}(\K)\times\M_{n,q}{\K}$ tel que $\text{Im}(B)\subset \text{Im}(A)$.\\
			Déduire de ce qui précède l'existence d'une matrice $C\in \M_{p,q}(\K)$ vérifiant que $B=AC$.
		\end{enumerate}
		\item Soient $A,B$ et $C$ trois matrices carrées d'ordre $n$ à coefficients dans $\K$ telles que $\text{Im}(C)\subset\text{Im}(A)+\text{Im}(B)$.
		\begin{enumerate}[label=\alph*.]
			\item On désigne par $D=(A|B)$ la matrice de $\M_{n,2n}{\K}$ obtenue en juxtaposant les matrices $A$ et $B$, c'est à dire que les $n$ première colonnes de $D$ sont celles de $A$ et les $n$ dernières sont celles de $B$.\\
			Montrer que $\text{Im}(D)=\text{Im}(A)+\text{Im}(B)$
			\item En déduire l'existence de deux matrices $U,V\in \M_n(\K)$ vérifiant $C=AU+BV$.
		\end{enumerate}
		\item Soit $J$ un idéal à droite de $\M_n(\K)$.
		\begin{enumerate}[label=\alph*.]
			\item Montrer que $\exists M_0\in J,\ \forall M\in J,\ \rg(M)\leq\rg(M_0)$. On note $r=\rg(M_0)$ pour la suite.
			\item Soit $M\in J$ telle que $\text{Im}(M)\not\subset \text{Im}(M_0)$.\\
			En utilisant le sous-espace vectoriel $\text{Im}(M)+\text{Im}(M_0)$, montrer l'existence d'un élément de $J$ de rang strictement supérieur à $r$.
			\item Montrer que $J=J_{M_0}$.
		\end{enumerate}
		\item Quels sont les idéaux à droite de $\M_n(\K)$ ?
	\end{enumerate}
	
	\subsection{Idéaux à gauche \ccinp{2}}
	Soit $E$ un sous-espace vectoriel de $\M_{n,1}(\K)$. On pose $J^E=\{M\in \M_n(\K),\ E\subset \Ker(M)\}$.
	\begin{enumerate}
		\item Montrer que $J^E$ est un idéal à gauche de $\M_n(\K)$.
		\item Soient $p,q\in \crblanc{1}{n},\ u\in \mathcal L(\R^n,\R^p)$ et $v\in \mathcal L(\R^q,\R^p)$ tels que $\Ker(u)\subset \Ker(v)$. On note $(e_1,\dots,e_n)$ une base de $\R^n$ telle que $(e_{r+1},\dots,e_n)$ soit une base de $\Ker(u)$.
		\begin{enumerate}[label=\alph*.]
			\item Montrer que $(u(e_1),\dots,u(e_r))$ est une famille libre de $\R^p$.
			\item Montrer qu'il existe $w\in \mathcal L(\R^p,\R^q)$ telle $v=w\circ u$.
			\item Soit $(A,B)\in \M_{p,n}{\K}\times \M_{q,n}{\K}$ tel que $\Ker(A)\subset \Ker(B)$.\\
			Déduire de ce qui précède l'existence d'une matrice $C\in \M_{q,p}{\K}$ vérifiant $B=CA$.
		\end{enumerate}
		\item Soient $A,B$ et $C$ trois matrices carrées d'ordre $n$ à coefficients dans $\K$ telles que $\Ker(A)\cap\Ker(B)\subset \Ker(C)$.\\
		Montrer qu'il existe deux matrices $U,V\in \M_n(\K)$ telles que $C=UA+VB$.
		\item Quels sont les idéaux à gauche de $\M_n(\K)$ ?
	\end{enumerate}
	
	\section{Caractéristique d'un anneau \etoile{2} (HP)}
	\label{Caractéristique d'un anneau}
	\textcolor{blue}{\hyperref[Caractéristique d'un anneau corrigé]{[Corrigé]}}\\
	Soit $A$ un anneau.
	\begin{enumerate}[leftmargin=*]
		\item Montrer que l'application $\varphi :\Z \longmapsto A$ définie par $\varphi(k)=k.1_A$ est un morphisme d'anneaux.
		\item Justifier qu'il existe $n\in \N$ tel que $\ker(\varphi)=n\Z$.\\
		\textit{Cet entier est appelé caractéristique de l'anneau $A$.}
		\item Montrer que la caractéristique d'un anneau intègre est nulle ou égale à un nombre premier.
		\item Soit $K$ un corps fini de caractéristique $p$. Montrer que $p$ est un nombre premier puis qu'on peut munir $K$ d'une structure de $\Z/p\Z$-espace vectoriel. En déduire qu'il existe $n\in \N$ tel que $\Card(K)=p^n$.
	\end{enumerate}
	
	\section{Anneau intègre \etoile{2} (HP)}
	\label{Anneau intègre}
	\textcolor{blue}{\hyperref[Anneau intègre corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Montrer qu'un corps est intègre.
		\item Donner un exemple d'anneau intègre qui n'est pas un corps.
		\item Montrer que pour $A$ un anneau commutatif, $A[X]$ est intègre si et seulement si $A$ l'est.
	\end{enumerate}
	
	\section{Sous-corps minimal de $\C$ \etoile{1}}
	\label{Sous-corps minimal de C}
	\textcolor{blue}{\hyperref[Sous-corps minimal de C corrigé]{[Corrigé]}}\\
	Démontrer l'existence et déterminer le sous-corps de $\C$ minimal au sens de l'inclusion.
	
	\section{Corps d'Attila}
	\label{Corps d'Attila}
	\textcolor{blue}{\hyperref[Corps d'Attila corrigé]{[Corrigé]}}\\
	On note $A$ la matrice de $\M_n(\K)$ qui n'est composée que de $1$.
	\begin{enumerate}
		\item Montrer que $(\Vect(A),+,\times)$ est un corps.
		\item Justifier que $(\Vect(A)\backslash\{0\},\times)$ est un groupe et expliciter son élément neutre.
	\end{enumerate}
	
	\section{\underline{Endomorphisme de corps de $\R$} \telecom{2}}
	\label{Endomorphisme de corps de R}
	\textcolor{blue}{\hyperref[Endomorphisme de corps de R corrigé]{[Corrigé]}}\\
	Soit $f$ un endomorphisme de corps de $\R$.
	\begin{enumerate}
		\item Montrer que $f_{|\Q}=\Id_\Q$.
		\item Montrer que $f$ est croissant.
		\item Montrer que $f=\Id_\R$.
	\end{enumerate}
	
	\section{Automorphismes de $\Q[\sqrt{2}]$ \centraleponts{3}}
	\label{Automorphismes de Qsqrt2}
	\textcolor{blue}{\hyperref[Automorphismes de Qsqrt2 corrigé]{[Corrigé]}}\\
	On note $\Q[\sqrt 2]=\{a+b\sqrt 2,\ (a,b)\in \Q^2\}$.\\
	Montrer que $\Q[\sqrt 2]$ est un sous-corps de $\C$ et en déterminer tous les automorphismes.
	
	\section{Algèbre des quaternions}
	\label{Algèbre des quaternions}
	\textcolor{blue}{\hyperref[Algèbre des quaternions corrigé]{[Corrigé]}}\\
	On se place dans $\M_2(\C)$ et on pose les matrices
	$$I=\begin{pmatrix}i&0\\0&-i\end{pmatrix}\quad J=\begin{pmatrix}0&-1\\1&0\end{pmatrix}\quad K=\begin{pmatrix}0&-i\\-i&0\end{pmatrix}$$
	On note $\mathbb H=\Vect(I_2,I,J,K)$.
	\begin{enumerate}
		\item Montrer que $I^2=J^2=K^2=IJK=-I_2$.
		\item Montrer que $\mathbb H$ est une sous-algèbre non commutative de $\M_2(\C)$. Quelle est sa dimension ?
	\end{enumerate}
	Pour un élément $Q=aI_2+bI+cJ+dK$ on définit son \textit{conjugué quaternionique} $\overline Q=aI_2-bI-cJ-dK$ et on pose $\norme{Q}=\sqrt{Q\overline Q}$.
	\begin{enumerate}[resume]
		\item Montrer que $\norme{\cdot}$ définit une norme sur $\mathbb H$.
		\item Montrer que $\forall (Q_1,Q_2)\in \mathbb H^2,\ \overline{Q_1Q_2}=\overline Q_2\times\overline Q_1$. L'application $Q\mapsto \overline Q$ est-elle $\R$-linéaire ? $\C$-linéaire ?
		\item Montrer que $(\mathbb H,+,\times)$ est un corps.
		\item Déterminer le centre de $\mathbb H$ (cf. \ref{Centre d'un anneau}).
	\end{enumerate}
	
	\section{Une définition de $\C$ \ccinp{2}}
	\label{Une définition de C}
	\textcolor{blue}{\hyperref[Une définition de C corrigé]{[Corrigé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Montrer que l'application $\fonction{\Phi}{\C}{\M_2(\R)}{z}{\begin{pmatrix}
				\Re(z)&-\Im(z)\\
				\Im(z)&\Re(z)\\
		\end{pmatrix}}$
		est un morphisme d'algèbre injectif.
		\item Pour $\theta\in \R$ on pose $A_\theta=
		\begin{pmatrix}
			0&-\theta\\
			\theta&0\\
		\end{pmatrix}$.
		Calculer $\exp{(A_\theta)}$.
	\end{enumerate}
	
	\section{$\R$-algèbre commutative intègre de dimension finie \centraleponts{4}}
	\label{R algèbre commuttative intègre de dimension finie}
	\textcolor{blue}{\hyperref[R algèbre commutative intègre de dimension finie corrigé]{[Corrigé]}}\\
	Soit $\K$ une $\R$-algèbre commutative intègre de dimension finie $n\geq 2$.
	\begin{enumerate}
		\item Soit $a\in \K\backslash\{0\}$. Montrer que $f:x\mapsto ax$ est un automorphisme et en déduire que $a$ est inversible.
		\item Soit $a\in \K\backslash\R$. Montrer que $(1,a)$ est libre et que $(1,a,a^2)$ est liée.
		\item Montrer l'existence de $i\in \K$ tel que $i^2=-1_\K$, puis que $\K$ est isomorphe à $\C$ en tant que $\R$-algèbre.
	\end{enumerate}
	
	\section{Théorie algébrique des corps \xens{4} (HP)}
	\label{Théorie algébrique des corps}
	\textcolor{blue}{\hyperref[Théorie algébrique des corps corrigé]{[Corrigé]}}\\
	Soit $\mathbb L$ un corps commutatif et $\K$ un sous corps quelconque de $\mathbb L$. On dit que $a\in \mathbb L$ est \textit{algébrique sur $\K$} s'il existe un polynôme $P\in \K[X]$ non nul qui annule $a$.
	\begin{enumerate}
		\item Montrer que $\K[X]$ est un anneau principal, c'est à dire que tous ses idéaux sont de la forme $P\K[X]$ avec $P\in \K[X]$.
		\item Soit $a\in \mathbb L$ algébrique sur $\K$. Montrer l'existence d'un unique polynôme unitaire $\mu\in \K[X]$ tel que
		$$\forall P\in \K[X],\ P(a)=0\implies \mu|P$$
		Vérifier que $\mu$ est irréductible et que l'ensemble $\K[a]=\{P(a),\ P\in \K[X]\}$ est à la fois un $\K$-espace vectoriel de dimension $\deg\mu$ et un sous-corps de $\mathbb L$. ($\mu$ est appelé \textit{polynôme minimal} de $a$ sur $\K$)
		\item On dit qu'un nombre complexe $a$ est \textit{algébrique} s'il est algébrique sur le corps $\Q$. Démontrer que $a=\sqrt 2+\sqrt[3]2$ est algébrique et déterminer son polynôme minimal.\\
		\textit{On pourra utiliser la relation de multiplicativité des degrés (cf. \ref{Extension de corps}).}
	\end{enumerate}
	
	\section{Famille $\Q$-libre \etoile{4} (HP)}
	\label{Famille Q-libre}
	\textcolor{blue}{\hyperref[Famille Q-libre corrigé]{[Corrigé]}}\\
	Dans tout cet exercice on travaille dans $\C$ munit de sa structure de $\Q$-espace vectoriel.\\
	Soient $p_1,\dots,p_n$ des nombres premiers distincts. Pour toute famille $\F=(z_1,\dots,z_p)$ d'éléments de $\C$ on note $\Q(z_1,\dots,z_p)$ le plus petit sous-corps de $\C$ qui contient $\Q$ et $\F$. Pour $\alpha\in \C$ on note $m_\alpha:x\in \C\mapsto\alpha x$ et enfin, si $\alpha$ est algébrique on note $\pi_\alpha$ son polynôme minimal (cf. \ref{Théorie algébrique des corps}). On fixe dans tout l'exercice $\alpha$ un nombre algébrique.
	\begin{enumerate}
		\item Montrer que $\Q(\alpha)$ est un $\Q$-espace vectoriel de dimension finie dont on précisera la dimension en fonction de $\pi_\alpha$. Montrer que $m_\alpha$ est un endomorphisme de $\Q(\alpha)$.
		\item Montrer que si $d\in \N$ n'est pas le carré d'un entier alors $\Q(\sqrt d)$ est un $\Q$-espace vectoriel de dimension $2$ et en donner une base.
		\item Montrer que $\pi_{m_\alpha}=\pi_\alpha$ et en déduire que $\chi_{m_\alpha}$ est une puissance de $\pi_\alpha$.
		\item Soient $1\leq i\ne j\leq n$. Montrer $\Tr(m_{\sqrt{p_ip_j}})=0$ puis que la matrice $(\Tr(m_{\sqrt{p_ip_j}}))_{1\leq i,j\leq n}$ est inversible.
		\item En déduire que $(\sqrt p_1,\dots,\sqrt p_n)$ est libre dans $\Q$.
	\end{enumerate}
	
	\section{Corps des nombres algébriques \centraleponts{4}}
	\label{Corps des nombres algébriques}
	\textcolor{blue}{\hyperref[Corps des nombres algébriques corrigé]{[Corrigé]}}\\
	On note $\mathbb A=\{z\in \C\ |\ \exists P\in \Q[X]\backslash\{0\},\ P(z)=0\}$ l'ensemble des nombres algébriques.\\\\
	Montrer que $(\mathbb A,+,\times)$ est un corps.\\
	\textit{\underline{Indication}: pour la stabilité par somme et par produit on pourra poser pour} $(x,y)\in \mathbb A^2$,\\


	\[V=\bigl(\begin{smallmatrix}
		1 & x & x^{2} & \cdots & x^{n-1} & y & xy & x^{2}y & \cdots & x^{n-1}y & \cdots & x^{n-1}y^{m-1}
	\end{smallmatrix}\bigr)\]

		\textit{avec $n$ et $m$ les degrés de polynômes unitaires de $\Q[X]$ qui annulent $x$ et $y$ respectivement, puis montrer qu'il existe deux matrices $A,B\in \M_{nm}(\Q)$ telles que $AV^\top=xV^\top$ et $BV^\top=yV^\top$.}
	
	\section{Théorème de Kronecker \xens{4}}
	\label{Théorème de Kronecker}
	\textcolor{blue}{\hyperref[Théorème de Kronecker corrigé]{[Corrigé]}}\\
	Soit $P=\displaystyle\prod_{i=1}^d(X-\lambda_i)\in \Z[X]$ unitaire dont toutes les racines sont non nulle et de module inférieur à $1$. Soit $k\in \N^*$.
	\begin{enumerate}
		\item Montrer que $\lambda_1^k,\dots,\lambda_d^k$ sont les racines d'un polynôme non nul $Q_k$ unitaire à coefficients entiers.
		\item Montrer que les coefficients de $Q_k$ sont bornées.
		\item En déduire que $\lambda_1,\dots,\lambda_d$ sont des racines de l'unité.
	\end{enumerate}
	
	\section{Irrationalité de $e$ (1) \centraleponts{2}}
	\label{Irrationalité de e (1)}
	\textcolor{blue}{\hyperref[Irrationalité de e (1) corrigé]{[Corrigé]}}\\
	En utilisant $e=\displaystyle\sum_{k=0}^{+\infty}\frac{1}{k!}$ montrer que $e$ est irrationnel.
	
	\section{Irrationalité de $e$ (2) \telecom{2}}
	\label{Irrationalité de e (2)}
	\textcolor{blue}{\hyperref[Irrationalité de e (2) corrigé]{[Corrigé]}}\\
	On pose pour $n\in \N,\ I_n=\displaystyle\int_0^1x^ne^xdx$.\\
	\begin{enumerate}
		\item Déterminer la limite de $(I_n)_{n\in \N}$.
		\item En déduire que $e$ est irrationnel.
	\end{enumerate}
	
	\section{\underline{Irrationalité de $\pi$} \xens{4}}
	\label{Irrationalité de pi}
	\textcolor{blue}{\hyperref[Irrationalité de pi corrigé]{[Corrigé]}}\\
	On pose pour $n\in \N,\ I_n=\displaystyle\int_0^\pi x^n(x-\pi)^n\sin (x)dx$.
	\begin{enumerate}
		\item Soit $n\in \N$. Montrer que $I_n$ est un polynôme en $\pi$.
		\item En déduire que $\pi$ est irrationnel.
	\end{enumerate}
	
	\section{\underline{Critère de transcendance de Liouville} \xens{4}}
	\label{Critère de transcendance de Liouville}
	\textcolor{blue}{\hyperref[Critère de transcendance de Liouville corrigé]{[Corrigé]}}\\
	On dit qu'un nombre complexe est \textit{transcendant} s'il n'est pas algébrique, autrement dit s'il n'est racine d'aucun polynôme non nul à coefficients rationnels.
	\begin{enumerate}[leftmargin=*]
		\item Soit $\alpha\in\C$ un nombre algébrique. On se donne $(p_n)\in \Z^\N$ et $(q_n)\in \left(\N^*\right)^\N$ tels que la suite de rationnels $\displaystyle\left(\frac{p_n}{q_n}\right)_{n\in \N}$ converge
		vers $\alpha$ par valeurs différentes de $\alpha$.\\
		Montrer qu'il existe un entier naturel non nul $d$ tel que $\displaystyle\frac{1}{q_n^d}\unfty{=}\bigO{\alpha-\frac{p_n}{q_n}}$.
		\item En déduire que la constante de Liouville $\displaystyle\sum_{n=1}^{+\infty}\frac{1}{10^{n!}}$ est un nombre transcendant.
	\end{enumerate}
	
	
\newpage
\chapter{Arithmétique}
\section{Infinité des nombres premiers \ccinp{2}}
\label{Infinité des nombres premiers}
\textcolor{blue}{\hyperref[Infinité des nombres premiers corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Montrer qu'il existe une infinité de nombres premiers.
	\item Montrer qu'il existe une infinité de nombres premiers congrus à $3$ modulo $4$.
\end{enumerate}

\section{Version faible du théorème de progression arithmétique de Dirichlet \xens{3}}
\label{Version, faible du théorème de progression arithmétique de Dirichlet}
\textcolor{blue}{\hyperref[Version faible du théorème de progression arithmétique de Dirichlet corrigé]{[Corrigé]}}\\
Pour $n\in \N^*$ on note $\Phi_n=\displaystyle\prod_{\substack{1\leq k\leq n\\\pgcd(k,n)=1}}{\left(X-e^{\frac{2ik\pi}{n}}\right)}$ le $n$-ième polynôme cyclotomique.
\begin{enumerate}[leftmargin=*]
	\item Montrer que $X^n-1=\displaystyle\prod_{d|n}{\Phi_d}$.
	\item Montrer que $\forall n\in \N^*,\Phi_n\in \Z[X]$.
	\item Soient $a\in \Z$ et $n\in \N^*$. Que peut-on dire d'un nombre premier $p$ qui divise $\Phi_n(a)$, mais aucun des $\Phi_d(a)$ pour $d$ un diviseur strict positif de $n$ ?
	\item En déduire que pour $n\in \N^*$ fixé, il existe une infinité de nombre premier congrus à $1$ modulo $n$.
\end{enumerate}


\section{Racine carré d'un nombre premier \telecom{2}}
\label{Racine carré d'un nombre premier}
\textcolor{blue}{\hyperref[Racine carré d'un nombre premier corrigé]{[Corrigé]}}\\
Soit $p$ un nombre premier. Démontrer que $\sqrt p$ est irrationnel.

\section{Une suite périodique \centraleponts{3}}
\label{Une suite périodique}
\textcolor{blue}{\hyperref[Une suite périodique corrigé]{[Corrigé]}}\\
Soit $(P,Q)\in \Z[X]^2$ tel que $P\wedge Q=1$. On pose pour $n\in \N,\ u_n=P(n)\wedge Q(n)$. Montrer que $(u_n)_{n\in \N}$ est périodique.

\section{\underline{Racines de l'unité} \ccinp{2}}
\label{Racines de l'unité}
\textcolor{blue}{\hyperref[Racines de l'unité corrigé]{[Corrigé]}}\\
Soit $(m,n)\in \N^*$. Montrer que $\U_n\cap \U_m=\U_{m\wedge n}$.

\section{Plus petit nombre premier ne divisant pas un entier donné}
\label{Plus petit nombre premier ne divisant pas un entier donné}
\textcolor{blue}{\hyperref[Plus petit nombre premier ne divisant pas un entier donné corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Montrer que tout entier $n>6$ s'écrit comme la somme de deux entiers premiers entre eux, strictement supérieurs à $1$.
	\item Soit $(p_n)_{n\geq 1}$ la suite strictement croissante des nombres premiers. Montrer que pour tout $k>2$, on a $p_{k+1}+p_{k+2}\leq p_1p_2\dots p_k$.
	\item Pour $n\in \N^*$, on note $q_n$ le plus petit nombre premier qui ne divise pas $n$. Montrer que la suite de terme général $\displaystyle\frac{q_n}{n}$ tend vers $0$.
\end{enumerate}

\section{Théorème de Kurshak \xens{4}}
\label{Théorème de Kurshak}
\textcolor{blue}{\hyperref[Théorème de Kurshak corrigé]{[Corrigé]}}\\
Pour quelles valeurs entières de $n\geq m$ a-t-on $\displaystyle\sum\limits_{i=m}^{n}{\frac{1}{i}}\in \N$ ?

\section{Valuation $p$-adique de $\displaystyle\binom{p^n}{k}$ \xens{3}}
\label{Valuation padique de binom(p^n)(k)}
\textcolor{blue}{\hyperref[Valuation padique de binom(p^n)(k) corrigé]{[Corrigé]}}\\
Soient $p$ un nombre premier, $n\in \N^*$ et $k\in \crblanc{1}{p^n-1}$. Calculer la valuation $p$-adique de $\displaystyle\binom{p^n}{k}$.

\section{Une équation dans $\N$ \centraleponts{3}}
\label{Une équation dans N}
\textcolor{blue}{\hyperref[Une équation dans N corrigé]{[Corrigé]}}\\
Résoudre dans $\N,\ a^b=b^a$
\begin{itemize}
	\item Par une étude de fonction;
	\item Par un raisonnement arithmétique.
\end{itemize}

\section{Triplets pythagoriciens \ccinp{2}}
\label{Triplets pythagoriciens}
\textcolor{blue}{\hyperref[Triplets pythagoriciens corrigé]{[Corrigé]}}\\
Le but de cet exercice est de déterminer tous les triplets $(a,b,c)\in (\N^*)^3$ qui vérifient
$$a^2+b^2=c^2$$
\begin{enumerate}
	\item Montrer qu'on peut se ramener à $a,b,c$ premiers entre eux dans leur ensemble.
	\item Montrer qu'on peut se ramener à $a,b,c$ premiers entre eux deux à deux.
	\item Montrer que parmi $a,b$ et $c$ il y en a deux qui sont pairs et un qui est impair.
	\item Montrer que $c$ est impair. On suppose désormais que c'est $b$ qui est pair.
	\item Montrer alors que, dans ce cas, $(a,b,c)$ est un triplet pythagoricien si et seulement s'il existe deux entiers de parité différente et premier entre eux, $u$ et $v$ avec $u>v$ tels que:
	$$a=u^2-v^2\ \ ,\ \ b=2uv,\ \ c=u^2+v^2$$
	\item En déduire l'ensemble des triplets pythagoriciens.
\end{enumerate}

\section{Théorème de Sophie Germain \centraleponts{3}}
\label{Théorème de Sophie Germain}
\textcolor{blue}{\hyperref[Théorème de Sophie Germain corrigé]{[Corrigé]}}\\
Soit $p$ un nombre premier de Sophie Germain, c'est à dire un nombre premier impair tel que $q=2p+1$ soit premier. On souhaite prouver que qu'il n'existe pas de triplet $(x,y,z)\in \Z^3$ tel que $xyz\not\equiv 0[p]$ et $x^p+y^p+z^p=0$. Pour cela on raisonne par l'absurde et on se donne une telle solution $(x,y,z)$.
\begin{enumerate}
	\item Montrer que l'on peut se ramener à $x,y,z$ premiers entre eux deux à deux.
	\item
	\begin{enumerate}[label=\alph*.]
		\item Soit $k\in \N$. Montrer si le produit de deux entiers premiers entre eux est une puissance $k$-ième alors chacun des facteurs est une puissance $k$-ième.
		\item Montrer qu'il existe deux entiers $a$ et $\alpha$ tels que $y+z=a^p$ et $\displaystyle\sum_{k=0}^{p-1}(-z)^{p-1-k}y^k=\alpha^p$. Etablir alors l'existence de deux entiers $b$ et $c$ tels que $x+y=c^p$ et $x+z=b^p$.
	\end{enumerate}
	\item Montrer que si $m\in \Z$ n'est pas divisible par $q$ alors $m^p\equiv \pm1[q]$. En déduire qu'un et un seul des entiers $x,y,z$ est divisible par $q$. On supposera dans la suite que c'est $x$.
	\item Etablir successivement les congruences suivantes, toutes modulo $q$:
	$$b^p+c^p-a^p\equiv 0\ ;\ y\equiv c^p\ ;\ a\equiv 0\ ;\ \alpha^p\equiv py^{p-1}$$
	Aboutir à une contradiction et conclure.
\end{enumerate}
\textit{Il y a beaucoup de choses intéressantes à dire à propos de ce résultat. Sophie Germain (1776-1831) est quasiment la seule femme mathématicienne de son temps. Elle suivit les cours de l’École polytechnique par correspondance, car les femmes n’y
	étaient pas admises et c’est sous le pseudonyme masculin de Maurice Leblanc qu’elle écrivait à Gauss pour lui faire part de ses découvertes arithmétiques. Le théorème de Sophie Germain, démontré
	en 1823, est une résolution partielle du grand théorème de Fermat: Pour $n\geq 3$ et $(x,y,z)\in (\N^*)^3$ l'équation $x^n+y^n=z^n$ n'admet pas de solution.}

\section{Une équation diophantienne \centraleponts{3}}
\label{Une équation diophantienne}
\textcolor{blue}{\hyperref[Une équation diophantienne corrigé]{[Corrigé]}}\\
Soient $n\in \N$ et $\alpha \in \N$ tels que $\alpha >n\geq 2$. On cherche à montrer que l'équation
$$x_1^2+\dots+x_n^2=\alpha x_1\dots x_n$$
n'admet pas de solution entière non triviale (autre que $(0,\dots,0)$).\\
Pour cela on raisonne par l'absurde : On se donne une solution $(x_1,\dots,x_n)$ non triviale pour laquelle on suppose sans perte de généralité que $x_1\leq \dots \leq x_n$.
\begin{enumerate}
	\item Montrer que l'on peut trouver une autre solution de l'équation sous la forme $(x_1,\dots,x_{n-1},y)$ avec $y<x_n$.
	\item Conclure.
\end{enumerate}

\section{Calcul d'une somme \centraleponts{3}}
\label{Calcul d'une somme}
\textcolor{blue}{\hyperref[Calcul d'une somme corrigé]{[Corrigé]}}\\
Soit $(a,b)\in \Z \times \N^*$
\begin{enumerate}
	\item Montrer que le quotient de la division euclidienne de $a$ par $b$ est $\displaystyle{\left\lfloor \frac{a}{b}\right\rfloor}$.\\
	On suppose dans la suite que $a\wedge b=1$.
	\item Montrer que l'application $\fonction{\varphi}{\Z/b\Z}{\Z/b\Z}{\overline k}{\overline{ak}}$ est bijective.
	\item Montrer que $\displaystyle\sum\limits_{k=1}^{b-1}{\left\lfloor \frac{ka}{b}\right\rfloor}=\frac{(a-1)(b-1)}{2}$.
\end{enumerate}

\section{\underline{Nombres de Mersenne} \ccinp{2}}
\label{Nombres de Mersenne}
\textcolor{blue}{\hyperref[Nombres de Mersenne corrigé]{[Corrigé]}}\\
Pour $n\in \N^*$ on appelle $n^{ème}$ nombre de Mersenne l'entier $M_n=2^n-1$.
\begin{enumerate}[leftmargin=*]
	\item \begin{enumerate}[label=\alph*.]
		\item Soient $n\in N^*$ et $a$ un diviseur positif de $n$. Montrer que $2^a-1$ divise $M_n$.
		\item En déduire que si $M_n$ est un nombre premier, alors $n$ est un nombre premier.
	\end{enumerate}
	\item Soient $p$ et $q$ des nombres premiers avec $p$ impair. On suppose que $q$ divise $M_p$.
	\begin{enumerate}[label=\alph*.]
		\item Montrer que $q$ est impair. En déduire que $2^{q-1}\equiv 1 [q]$.
		\item En considérant l'ordre de $\overline{2}$ dans $(\Z/q\Z)^*$, montrer que $q\equiv 1[p]$ puis que $q\equiv 1[2p]$.
	\end{enumerate}
	\item Soient $p$ un nombre premier impair et $n\in \N^*$ divisant $M_p$. En utilisant la décomposition en facteurs premiers de $n$, montrer que $n\equiv 1[2p]$.
	\item Proposer un algorithme Python utilisant les résultats de l'exercice permettant de déterminer si le $n^{ème}$ nombre de Mersenne est premier.
\end{enumerate}
\textit{\underline{Remarque:} cet exercice montre que les nombres de Mersenne sont de bons candidats pour la recherche de grands nombres premiers... et pas qu'un peu! Le plus grand nombre premier connu à ce jour est un nombre de Mersenne. Il a été découvert le 21 Octobre 2024 grâce au projet the Great Internet Mersenne Prime Search (GIMPS) qui rassemble les ordinateurs du monde pour la recherche de nombres de Mersenne premiers. Il s'agit de $2^{136279841}-1$ qui comporte pas moins de 41 024 320 chiffres!}

\section{\underline{Un exercice pour les années impaires} \xens{4}}
\label{Un exercice pour les années impaires}
\textcolor{blue}{\hyperref[Un exercice pour les années impaires corrigé]{[Corrigé]}}\\
Existe-t-il une application $f:\N \longmapsto \N$ telle que $\forall n\in \N,\ f(f(n))=n+2025$ ?

\section{\underline{Equation du second degré dans $\Z/n\Z$} \telecom{2}}
\label{Equation du second degré dans Z/nZ}
\textcolor{blue}{\hyperref[Equation du second degré dans Z/nZ corrigé]{[Corrigé]}}\\
Résoudre dans $\Z/85\Z$ l'équation $x^2-3x+\overline{7}=\overline{0}$.

\section{Un problème de congruence \centraleponts{3}}
\label{Un problème de congruence}
\textcolor{blue}{\hyperref[Un problème de congruence corrigé]{[Corrigé]}}\\
Montrer que $\forall n\in \N^*,\ 10^{10^n}\equiv 4[7]$.

\section{Un multiple de 2026 qui ne s'écrit qu'avec des $2$ \xens{3}}
\label{Un multiple de 2026}
\textcolor{blue}{\hyperref[Un multiple de 2026 corrigé]{[Corrigé]}}\\
Montrer qu'il existe un multiple de 2026 dont l'écriture décimale ne comporte que des $2$.

\section{Somme des puissances $k$-ièmes dans $\Z/p\Z$ \xens{3}}
\label{Somme des puissances k iemes dans Z/pZ}
\textcolor{blue}{\hyperref[Somme des puissances k iemes dans Z/pZ corrigé]{[Corrigé]}}\\
Soient $p$ un nombre premier et $k\in \N^*$. Calculer $S_k=\displaystyle\sum\limits_{x\in \Z/p\Z}{x^k}$\\
On pourra pour cela étudier les cas :
\begin{enumerate}[label=(\roman*)]
	\item $k$ est multiple de $p-1$,
	\item $d=\pgcd(k,p-1)<p-1$ en calculant $y^kS_k$ pour $y\in (\Z/p\Z)^*$.
\end{enumerate}
\textit{On admettra qu'un polynôme à coefficients dans $\Z/p\Z$ a toujours moins de racines que son degré.}

\section{Théorème de Wilson \telecom{2}}
\label{Théorème de Wilson}
\textcolor{blue}{\hyperref[Théorème de Wilson corrigé]{[Corrigé]}}\\
Soit $p$ un entier supérieur à $2$. Montrer l'équivalence:
$$(p-1)!\equiv -1[p] \Longleftrightarrow p \text{ est premier.}$$

\section{Problème Putnam (2011) \xens{5}}
\label{Problème Putnam}
\textcolor{blue}{\hyperref[Problème Putnam corrigé]{[Corrigé]}}\\
Soit $p$ un nombre premier impair.\\
Montrer qu'il existe au moins $\displaystyle\frac{p+1}{2}$ valeurs de $n$ dans $\crblanc{0}{p-1}$ telles que $p$ ne divise pas $\displaystyle\sum_{k=0}^{p-1}k!n^k$.\\
\textit{On admettra qu'un polynôme à coefficient dans $\Z/p\Z$ a toujours moins de racines que son degré.}

ylo\section{Critère d'Euler \centraleponts{4}}
\label{Critère d'Euler}
\textcolor{blue}{\hyperref[Critère d'Euler corrigé]{[Corrigé]}}\\
Soit $p$ un nombre premier distinct de 2. On pose $\mathbb{F}_p=\Z/p\Z$ et $\mathbb F_p^*=\mathbb F_p\backslash \{\overline{0}\}$.
\begin{enumerate}[leftmargin=*]
	\item On pose $\mathcal C=\{x^2,x\in \mathbb F_p^*\}$. Montrer que $\Card(\mathcal C)=\displaystyle\frac{p-1}{2}$.
	\item Soit $a\in\F_p^*$. Montrer que $a^{\frac{p-1}{2}}\in \{-\overline 1,\overline 1\}$.
	\item Soient $d\in \N^*$ et $P\in \Z_{d-1}[X]$. Soit $(a_1,\dots,a_d)\in \Z^d$ telle que les $a_i$ soient distincts modulo $p$ et telle que $\forall i\in \crblanc{1}{d},\ p|P(a_i)$. Montrer que $\forall n\in \Z,\ p|P(n)$.
	\item Montrer alors que $a^{\frac{p-1}{2}}=
	\begin{cases}
		\overline 1 & \mbox{si } a\in \mathcal C\\
		-\overline 1 & \mbox{sinon}
	\end{cases}$
\end{enumerate}

\section{\underline{Indicatrice d'Euler} \centraleponts{3}}
\label{Indicatrice d'Euler}
\textcolor{blue}{\hyperref[Indicatrice d'Euler corrigé]{[Corrigé]}}\\
Soit $n\in \N^*$. On note $\varphi$ la fonction indicatrice d'Euler.
\begin{enumerate}[leftmargin=*]
	\item Soit $d$ un diviseur positif de $n$. Montrer qu'il y a $\varphi(d)$ éléments d'ordre $d$ dans le groupe $(\Z/n\Z,+)$.
	\item Montrer que $n=\displaystyle\sum\limits_{d|n}{\varphi(d)}$ où la somme porte sur les diviseurs positifs de $n$.
	\item En déduire un programme Python permettant de calculer $\varphi(n)$.
\end{enumerate}

\section{Une minoration de l'indicatrice d'Euler \centraleponts{3}}
\label{Une minoration de l'indicatrice d'Euler}
\textcolor{blue}{\hyperref[Une minoration de l'indicatrice d'Euler corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Soient $n_1,\dots,n_k$ des entiers distincts supérieurs ou égaux à $2$. Montrer que
	$$\prod\limits_{i=1}^{k}{\left( 1-\frac{1}{n_i}\right)}\geq \frac{1}{k+1}$$
	\item On note $\varphi$ la fonction indicatrice d'Euler. Montrer que
	$$\forall n\in \N^*,\ \varphi(n)\geq \frac{n\ln{2}}{\ln{n}+\ln{2}}$$
\end{enumerate}

\section{\underline{Formule d'inversion de Möbius} \centraleponts{3}}
\label{Formule d'inversion de Möbius}
\textcolor{blue}{\hyperref[Formule d'inversion de Möbius corrigé]{[Corrigé]}}\\
Une fonction arithmétique est une fonction de $\N^*$ dans $\C$.
On note $1$, $\delta$,  $I$ les fonctions arithmétiques: 
$$\fonction{1}{\N^*}{\C}{n}{1} \quad \fonction{\delta}{\N^*}{\C}{n}{\begin{cases}
		1 &\text{si }n=1\\
		0 &\text{sinon}
\end{cases}} \quad \fonction{I}{\N^*}{\C}{n}{n}$$
On note $\mu$ la fonction de Möbius définie sur $\N^*$ par:
$$\mu(n)=\begin{cases}
	0 & \mbox{si } n\mbox{ est divisible par le carré d'un nombre premier}\\
	(-1)^k & \mbox{si } n\mbox{ est le produit de } k\mbox{ nombre premiers distincts}
\end{cases}$$
On dit qu'une fonction arithmétique $f$ est multiplicative quand : $$\begin{cases}
	f(1)\ne0\\
	\forall (m,n)\in(\N^*)^2, \, m\wedge n=1 \Longrightarrow f(mn)=f(m)f(n)
\end{cases}$$
Si $f$ et $g$ sont deux fonctions arithmétiques, le produit de convolution de $f$ et $g$ est la fonction la fonction arithmétique notée $f*g$ définie par : $$\forall n \in\N^* ,\;(f*g)(n)=\sum_{d|n}f(d)g\left(\frac{n}{d}\right)$$
\begin{enumerate}
	\item Montrer que $\mu$ est multiplicative.
	\item Montrer que $\mu*1=\delta$.
	\item Soit $f$ et $F$ deux fonctions arithmétiques telles que pour tout $n\in\N^*$, $\displaystyle F(n)=\sum_{d|n}f(d)$.
	\\ Montrer que pour tout $n\in\N^*$, $$f(n)=\sum_{d|n}\mu(d)F\left(\frac{n}{d}\right)$$
	\item Démontrer que $\varphi=\mu*I$ où $\varphi$ désigne la fonction indicatrice d'Euler.
\end{enumerate}


\section{Probabilité que deux entiers soient premiers entre eux \xens{4}}
\label{Probabilité que deux entiers soient premiers entre eux}
\textcolor{blue}{\hyperref[Probabilité que deux entiers soient premiers entre eux corrigé]{[Corrigé]}}\\
Pour tout $n\in \N^*$, on note $d_n$ le nombre de couple $(a,b)\in \crblanc{1}{n}^2$ tel que $a$ et $b$ soient premiers entre eux.
Soit $\mu$ la fonction de Möbius (définie dans l'exercice précédent).
\begin{enumerate}[leftmargin=*]
	\item Montrer que $d_n=\displaystyle{\sum\limits_{d=1}^{n}{\mu(d)\left\lfloor \frac{n}{d}\right\rfloor^2}}$. On pourra utiliser la formule du crible (cf. \ref{Formule du crible})
	\item En déduire que $\displaystyle{\frac{d_n}{n^2} \unfty{\longrightarrow} \frac{6}{\pi^2}}$.
\end{enumerate}

\section{Limite d'une fonction arithmétique multiplicative \centraleponts{3}}
\label{Limite d'une fonction arithmétique multiplicative}
\textcolor{blue}{\hyperref[Limite d'une fonction arithmétique multiplicative corrigé]{[Corrigé]}}\\
Une fonction $f:\N^*\to \C$ est dite \textit{multiplicative} lorsque $\forall n,m\in \N^*,\ n\wedge m=1\implies f(mn)=f(m)f(n)$.\\
On note $Q=\{p^k,\ p\text{ premier et }k\in \N^*\}$.
\begin{enumerate}
	\item On se donne une fonction $f:\N^*\to\C$ multiplicative vérifiant :
	$$\forall \varepsilon>0,\ \exists N\in \N^*,\ \forall q\in Q,\ q\geq N\implies |f(q)|\leq \varepsilon$$
	Montrer que $\unfty\lim f(n)=0$.\\
	\underline{Indication :} on pourra commencer par montrer que $f$ est bornée.
	\item On note $\varphi$ la fonction indicatrice d'Euler. Montrer que $\varphi$ est multiplicative et en déduire que :
	$$\forall \delta\in ]0,1[,\ \unfty\lim\frac{n^{1-\delta}}{\varphi(n)}=0$$
	\item Que peut-on dire pour $\delta=0$ ?
\end{enumerate}

\section{Fonctions arithmétiques réelles additives \centraleponts{4}}
\label{Fonctions arithmétiques réelles addtives}
\textcolor{blue}{\hyperref[Fonctions arithmétiques réelles additives corrigé]{[Corrigé]}}\\
On considère une fonction $f:\N^*\to \R$ croissante. On suppose que
$$\forall (m,n)\in \left(\N^*\right)^2,\ m\wedge n=1\implies f(mn)=f(m)+f(n)$$
On souhaite montrer qu'il existe $c\geq 0$ tel que $\forall n\in \N^*,\ f(n)=c\ln(n)$. On pose pour $n\in \N^*$ et $k\in \N$, $A_k(n)=\displaystyle\sum_{i=0}^kn^i$ et si $k\geq 1$, $B_k(n)=n^k-A_{k-1}(n)$. On pose aussi $B_0(n)=1$. On fixe $n\geq 3$ un entier et $k\in \N^*$.
\begin{enumerate}
	\item Montrer que $n\wedge A_k(n)=n\wedge B_k(n)=1$.
	\item Montrer que $f(A_k(n))\geq kf(n)$ puis que $f(n^{k+1})\geq kf(n)$.
	\item Montrer que $f(B_k(n))\leq kf(n)$ puis que $f(n^{k-1})\leq kf(n)$.
	\item En déduire que $f(n^k)=kf(n)$. Etendre l'égalité aux cas $n=1$ et $n=2$.
	\item Conclure.
\end{enumerate}


\section{Une majoration de la somme des diviseurs d'un entier \centraleponts{2}}
\label{Une majorattion de la somme des diviseurs d'un entier}
\textcolor{blue}{\hyperref[Une majoration de la somme des diviseurs d'un entier corrigé]{[Corrigé]}}\\
On note pour $n\in \N^*$, $\sigma(n)$ la somme des diviseurs positifs de $n$. Montrer que $\sigma(n)\leq n+n\ln n$.

\section{Entiers algébriques \centraleponts{2}}
\label{Entiers algébriques}
\textcolor{blue}{\hyperref[Entiers algébriques corrigé]{[Corrigé]}}\\
On note $\tilde{\Z}[X]$ l'ensemble des polynômes $\mathbf{unitaires}$ à coefficients entiers et $\tilde{\mathbb{A}}=\{x\in \C,\ \exists P\in \tilde{\Z}[X],P(x)=0\}$ l'ensemble des entiers algébriques.\\
Montrer que $\tilde{\mathbb{A}} \cap \Q=\Z$.

\section{\underline{Majoration de la primorielle} \centraleponts{3}}
\label{Majoration de la primorielle}
\label{Produit des nombres premiers inférieur à un entier donné}
\textcolor{blue}{\hyperref[Majoration de la primorielle corrigé]{[Corrigé]}}\\
On veut montrer que pour tout $n\in\N^*$, $\displaystyle\prod_{\substack{p\leq n\\p \textit{ premier}}}p\leq4^n$.
\begin{enumerate}
	\item Traiter les cas $n\in\{1,2,3\}$.
\end{enumerate}
On suppose à présent $n\geq 4$ et le résultat connu au rang $k$ pour tout entier $k$ compris entre $1$ et $n-1$.
\begin{enumerate}[resume]
	\item Etablir le résultat au rang $n$ quand $n$ est pair.
	\item Soit $n=2m+1$ avec $m\in\N$. Justifier que $\displaystyle\prod_{\substack{m+1\leq p\leq2m+1\\p \textit{ premier}}}p$ divise $\displaystyle\binom{2m+1}{m}$ et montrer que $\displaystyle\binom{2m+1}{m}\leq4^m$.
	\item Conclure.
\end{enumerate}

\section{\underline{Théorèmes de Mertens} \centraleponts{3}}
\label{Théorème de Mertens}
\textcolor{blue}{\hyperref[Théorèmes de Mertens corrigé]{[Corrigé]}}\\
Soit $n\in \N^*$. Une somme indicée sur $p$ indique que la sommation est effectuée sur les nombres premiers.
\\ On admettra le résultat de l'exercice \ref{Majoration de la primorielle} et celui de la formule de Legendre \ref{legendre}.
\begin{enumerate} 
	\item \textit{Premier théorème de Mertens}
	\begin{enumerate}[label=\alph*.]
		\item Montrer que : $$\frac{n}{p}-1<\nu_p(n!)\leq\frac{n}{p}\frac{n}{p(p-1)}$$
		\item Etablir que : $$\sum_{k=1}^n \ln k\unfty=n\ln n-n+\bigO{\ln n}$$
		\item Justifier que $\displaystyle n!=\prod_{p\leq n}p^{\nu_p(n!)}$ et en déduire que :
		$$n\sum_{p\leq n}\frac{\ln p}{p}-n\ln 4<\ln(n!)\leq n\sum_{p\leq n}\frac{\ln p}{p}+n\sum_{p\leq n}\frac{\ln p}{p(p-1)}$$
		\item Justifier que la série $\displaystyle\sum_{k\geq2}\frac{\ln k}{k(k-1)}$ converge.
		\item Déduire de ce qui précède que $\displaystyle\sum_{p\leq n}\frac{\ln p}{p}\unfty=\ln n+\bigO{1}$
	\end{enumerate}
	
	\item \textit{Deuxième théorème de Mertens} 
	\begin{enumerate}[label=\alph*.]
		\item Soit $(a_n)_{n\geq 2}$ une suite de nombres réels. Pour $t\geq 2$, on pose $A(t)=\displaystyle\sum_{2\leq k\leq t}a_k:=\sum_{k=2}^{\lfloor t\rfloor}a_k$.
		Soit $b:[2,+\infty[\to\R$ une fonction de classe $\mathcal C^1$.
		\\Montrer que pour tout entier $n\geq2$, $$\sum_{k=2}^na_kb(k)=A(n)b(n)-\int_2^nb'(t)A(t)dt$$
		\item On pose pour tout $t\geq 2$, $$R(t)=\sum_{p\leq t}\frac{\ln p}{p}-\ln t$$
		Montrer que $\displaystyle\sum_{p\leq n}\frac{1}{p}=1+\ln(\ln n)-\ln(\ln 2)+\frac{R(n)}{\ln n}+\int_2^n\frac{R(t)}{t(\ln t)^2}dt$
		\item Etablir que $\displaystyle\sum_{p\leq n}\frac{1}{p}\unfty=\ln(\ln n)+c+\bigO{\frac{1}{\ln n}}$ pour un réel $c\in\R$ à préciser.
	\end{enumerate}
\end{enumerate}

\section{\underline{Théorèmes de Tchebychev} \centraleponts{3}}
\label{Théorèmes de Tchebychev}
\textcolor{blue}{\hyperref[Théorèmes de Tchebychev corrigé]{[Corrigé]}}\\
Soit $n\in \N^*$. Une somme indicée sur $p$ indique que la sommation se fait sur les nombres premiers.\\
On note pour $x\geq 1,\ \pi(x)=\displaystyle\sum_{p\leq x}1:=\sum_{\substack{1\leq p\leq \lfloor x\rfloor\\p\text{ premier}}}1=\Card(\{p\in \crblanc{1}{x},\ p\text{ est premier}\})$.\\
On admettra le résultat des exercices \ref{Produit des nombres premiers inférieur à un entier donné} et celui de la formule de Legendre \ref{legendre}.
\begin{enumerate}
	\item \textit{Minoration de Tchebytchev}
	\begin{enumerate}[label=\alph*.]
		\item Montrer que $\displaystyle \nu_p\left(\binom{2n}{n}\right)\leq \left\lfloor\frac{\ln(2n)}{\ln p}\right\rfloor$.
		\item Montrer que $\pi(2n)\geq \displaystyle\frac{\ln\left(\binom{2n}{n}\right)}{\ln(2n)}$.
		\item En déduire que APCR $\pi(2n)\geq \displaystyle c\frac{n}{\ln n}$ pour une constante $c$ à préciser.
		\item Conclure en déterminant une minoration asymptotique de $\pi(n)$.
	\end{enumerate}
	\item \textit{Introduction d'une fonction auxiliaire}\\
	On note pour $x\geq 1$ $\theta(x)=\displaystyle\sum_{p\leq x}\ln p$.
	\begin{enumerate}[label=\alph*.]
		\item Montrer que $\theta(x)\leq \pi(x)\ln x$.\\
		On fixe $\varepsilon>0$.
		\item
		\begin{enumerate}[label=\roman*.]
			\item Montrer que $\theta(x)\geq (1-\varepsilon)\left(\pi(x)-\pi(x^{1-\varepsilon})\right)\ln x$.
			\item Montrer que $\pi(x^{1-\varepsilon})\uxfty=\smallo{\pi(x)}$.
			\item En déduire que $\forall \eta>0,\ \exists x_0\geq 1,\ \forall x\geq x_0,\ \theta(x)\geq (1-\varepsilon)\pi(x)\ln x+\eta\pi(x)\ln x$.
		\end{enumerate}
		\item Démontrer que $\theta(x)\uxfty\sim\pi(x)\ln x$.
	\end{enumerate}
	\item \textit{Majoration de Tchebytchev}
	\begin{enumerate}[label=\alph*.]
		\item Montrer que $\theta(2n)-\theta(n)\leq n\ln 4$.
		\item Montrer que pour tout $x\geq 1,\ \theta(2x)-\theta(2\lfloor x\rfloor)\leq 2\ln(2x)$ et $\theta(x)-\theta(\lfloor x\rfloor)\leq \ln x$.
		\item En déduire qu'il existe une constante $C>0$ telle que $\forall x\geq 1,\ \theta(2x)-\theta(x)\leq x\ln 4+C\ln x$.
		\item Montrer que $\forall n\in \N,\ \theta(x)\leq \displaystyle \theta\left(\frac{x}{2^n}\right)+x\ln 2\sum_{k=0}^{n-1}\frac{1}{2^k}+nC\ln x$.
		\item Conclure en déterminant une majoration asymptotique de $\theta(x)$ puis de $\pi(x)$.
	\end{enumerate}
\end{enumerate}

\newpage
\chapter{Dénombrement}
\section{Identité de Vandermonde \centraleponts{2}}
\label{Identité de Vandermonde}
\textcolor{blue}{\hyperref[Identité de Vandermonde corrigé]{[Corrigé]}}\\
Soient $r,m,n$ des entiers naturels. Montrer que : $$\sum_{k=0}^r\binom{n}{k}\binom{m}{r-k}=\binom{n+m}{r}$$
\textit{Remarque : Formule de Chu-Vandermonde}\\
Pour $\alpha\in\C$, on pose $$\forall n\in\N, \binom{\alpha}{n}=\frac{\alpha(\alpha-1)\dots(\alpha-n+1)}{n!}$$
On a également $$\forall n\in \N, \forall (a,b)\in\C ,\sum_{k=0}^n\binom{a}{k}\binom{b}{n-k}=\binom{a+b}{n}$$

\section{Nombre de Fibonacci \xens{3}}
\label{Nombre de Fibonacci}
\textcolor{blue}{\hyperref[Nombre de Fibonacci corrigé]{[Corrigé]}}\\
Déterminer le nombre $a_n$ de manière de recouvrir un damier de dimension $2\times n$ avec des pièces de dimension $1\times 2$. Montrer que si $n$ est assez grand $a_n$ est la partie entière de $\displaystyle\frac{1}{2}+\frac{1}{\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^{n+1}$.

\section{Matrices orthogonales à coefficients entiers \telecom{2}}
\label{Matrices orthogonales à coefficients entiers}
\textcolor{blue}{\hyperref[Matrices orthogonales à coefficients entiers corrigé]{[Corrigé]}}\\
Quel est le cardinal de $\O_n(\R) \cap \M_n(\Z)$ ?

\section{Dérangement \centraleponts{3}}
\label{Dérangement}
\textcolor{blue}{\hyperref[Dérangement corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Calculer $\displaystyle\sum\limits_{k=0}^p(-1)^k\binom{n}{k}\binom{n-k}{p-k}$ pour $0\leq p \leq n$.
	\item Soit $D_n$ le nombre de permutations de $\mathcal{S}_n$ n'ayant pas de point fixe. Montrer que $\displaystyle\sum\limits_{k=0}^n\binom{n}{k}D_{k}=n!$ \\(on pose $D_0=1$)
	\item Etablir, par une preuve combinatoire, que pour tout $n\geq 2$, $D_{n+1}=n(D_n+D_{n-1})$ 
	\item En déduire que pour tout $n\geq 2$, $D_n=nD_{n-1}+(-1)^n$.
	\item Retrouver la valeur de $D_n$.    
	\item Montrer que $D_n=n!\displaystyle\sum\limits_{k=0}^n\frac{(-1)^k}{k!}$
	\item On considère la série entière $\displaystyle\sum\limits_{n=0}^{+\infty}\frac{D_n}{n!}z^n$, (série génératrice exponentielle de la suite $(D_n)_{n\in \N}$). On note $D$ sa somme.\\ Minorer son rayon de convergence $R$ et calculer $D(z)$ pour $|z|<R$.
	\item En déduire que $D_n$ est la partie entière de $\displaystyle\frac{n!}{e}+\frac{1}{2}$.
\end{enumerate}
\textit{\underline{Remarque}: Ce dernier résultat correspond au problème des rencontres ou encore problème de Montmort.}

\section{Dérangement partiel \centraleponts{3}}
\label{Dérangement partiel}
\textcolor{blue}{\hyperref[Dérangement partiel corrigé]{[Corrigé]}}\\
On pose $D_0=1$ et, pour $n\in\N^*$, on note $D_n$ le nombre de permutation de $\crblanc{1}{n}$ n'ayant pas de point fixe.
\begin{enumerate}
	\item Quel est le nombre moyen de points fixes de $\sigma\in S_n$ ?
	\item \begin{enumerate}[label=\alph*.]
		\item Montrer que $n!=\displaystyle\sum\limits_{k=0}^n\binom{n}{k}D_{k}$
		\item En déduire que $D_n=n!\displaystyle\sum\limits_{k=0}^n\frac{(-1)^k}{k!}$
		\item Montrer que le nombre de permutations de $\crblanc{1}{n}$ admettant exactement $p$ points fixes est : $\displaystyle\frac{n!}{p!}\sum_{k=0}^{n-p}\frac{(-1)^k}{k!}$.
		\\ En déduire que : $$\sum_{p=1}^n\frac{1}{(p-1)!}\sum_{k=0}^{n-p}\frac{(-1)^k}{k!}=1$$
	\end{enumerate}
\end{enumerate}

\section{Nombres de Bell \centraleponts{3}}
\label{Nombres de Bell}
\textcolor{blue}{\hyperref[Nombres de Bell corrigé]{[Corrigé]}}\\
Pour tout $n\in\N^*$, on note $B_n$ le nombre de partitions de l'ensemble $\crblanc{1}{n}$. Par convention on pose $B_0=1$.
\begin{enumerate}[leftmargin=*]
	\item Calculer $B_1$, $B_2$ et $B_3$.
	\item Montrer que $\forall n\in \N,\ B_{n+1}=\displaystyle\sum_{k=0}^n\binom{n}{k}B_{n-k}$.
	\item On pose $f(z)=\displaystyle\sum\limits_{n=0}^{+\infty}\frac{B_n}{n!}z^n$.
	\\Montrer que le rayon de convergence $R$ de cette série entière n'est pas nul. Calculer $f(z)$ pour $z\in]-R,R[$.
	\\Exprimer $B_n$ comme somme d'une série.
\end{enumerate}
\underline{Remarque:} Il en résulte des calculs précédents que le rayon de convergence de la série entière définissant $f$ est en fait infini. Cette formule peut servir de point de départ à l'étude asymptotique de la suite $(B_n)$. Le lecteur intéressé pourra par exemple étudier la première épreuve du concours Mines-Ponts de 2002 qui se propose d'obtenir un équivalent de $B_n$.%

\section{Nombres de Catalan \centraleponts{3}}
\label{Nombres de Catalan}
\textcolor{blue}{\hyperref[Nombres de Catalan corrigé]{[Corrigé]}}\\
On s'intéresse à des chaînes de caractères constitués uniquement des deux caractères, parenthèse ouvrante et parenthèse fermante. On dit qu'un mot est \textit{bien parenthésé} s'il commence par une parenthèse ouvrante et qu'à toute parenthèse ouvrante est associée une (unique) parenthèse fermante qui lui est postérieure.
\\Par exemple le mot ()(()) est bien parenthésé. En revanche, le mot ())() n'est pas bien parenthésé.
\\Un mot bien parenthésé est ainsi forcément constitué d'un nombre pair de caractères: chaque parenthèse qui s'ouvre doit se refermer.
Pour tout entier $n\geq 1$, on note $C_n$ le nombre de mots bien parenthésés de longueurs $2n$. On pose par commodité $C_0=1$.
\begin{enumerate}
	\newcounter{num}
	\item Montrer que $\forall n\in\N$, $C_{n+1}=\displaystyle\sum\limits_{k=0}^{n}C_kC_{n-k}$.
	\item Montrer que, pour tout entier naturel $n$, $C_n\leq 2^{2n}.$ Que peut-on dire du rayon de convergence de la série entière $\displaystyle\sum_{n\in \N} C_nx^n$.
	\setcounter{num}{\value{enumi}}
\end{enumerate}
Pour tout $x\in]-\displaystyle\frac{1}{4},\frac{1}{4}[$, on pose $F(x)=\displaystyle\sum\limits_{n=0}^{+\infty}C_nx^n$.
\begin{enumerate}
	\setcounter{enumi}{\value{num}}
	\item Montrer que, pour tout $x\in]-\displaystyle\frac{1}{4},\frac{1}{4}[$, $F(x)=1+x(F(x))^2$
	\item Montrer que la fonction $f:x\in]-\displaystyle\frac{1}{4},\frac{1}{4}[\ \mapsto2xF(x)-1$ ne s'annule pas.
	\item Déterminer, pour tout $x\in]-\displaystyle\frac{1}{4},\frac{1}{4}[$, une expression de $F(x)$ en fonction de $x$.
	\item Déterminer le développement en série entière de la fonction $u\mapsto\sqrt{1-u}$. On écrira les coefficients sous la forme d'un quotient de factorielles et de puissances de 2.
	\item Montrer que, pour tout entier naturel $n$, $$C_n=\frac{(2n)!}{(n+1)!n!}$$
\end{enumerate}

\section{Nombres de parties \ccinp{2}}
\label{Nombres de parties}
\textcolor{blue}{\hyperref[Nombres de parties corrigé]{[Corrigé]}}\\
Soit $E$ un ensemble possédant $n\in \N^*$ éléments. On désigne par $\mathcal P(E)$ l'ensemble des parties de E.
\begin{enumerate}
	\item Déterminer le nombre de couples $(A,B)\in(\mathcal P(E))^2$ tels que $A\subset B$
	\item Déterminer le nombre de couples $(A,B)\in(\mathcal P(E))^2$ tels que $A\cap B=\emptyset$
	\item Déterminer le nombre de triplets $(A,B,C)\in(\mathcal P(E))^3$ tels que $A$, $B$ et $C$ soient deux à deux disjoints et vérifient $A\cup B\cup C=E$.
\end{enumerate}

\section{Nombre de surjection \centraleponts{3}}
\label{Nombre de surjection}
\textcolor{blue}{\hyperref[Nombre de surjection corrigé]{[Corrigé]}}\\
Soient $(n,p)\in(\N^*)^2$, on note $S(n,p)$ le nombre de surjections d'un ensemble à $n$ éléments vers un ensemble à $p$ éléments.
\begin{enumerate}[leftmargin=*]
	\item Vérifier que $S(n,p)=n!$ si $n=p$ et $S(n,p)=0$ si $n<p$.
	\\On pose par convention $S(0,0)=1,\ S(0,p)=S(n,0)=0\ \text{si}\ (n,p)\in(\N^*)^2$.
	\item Montrer que $p^n=\displaystyle\sum\limits_{k=0}^p \binom{p}{k}S(n,k)$.
	\item En déduire une expression de $S(n,p)$.
\end{enumerate}

\section{\underline{Formule de Legendre} \centraleponts{3}}
\label{Formule de Legendre}
\label{legendre}
\textcolor{blue}{\hyperref[Formule de Legendre corrigé]{[Corrigé]}}\\
Démontrer que pour tout $p$ premier et $n\in\N$, $v_p(n!)=\displaystyle\sum\limits_{k=1}^{+\infty}\left\lfloor{\frac{n}{p^k}}\right\rfloor$

\section{Théorème de Hall \xens{4}}
\label{Théorème de Hall}
\textcolor{blue}{\hyperref[Théorème de Hall corrigé]{[Corrigé]}}\\
Soient $n\in \N^*$ et $E$ un ensemble fini. Considérons $A_1,\dots,A_n\subset E$. Montrer que les assertions suivantes sont équivalentes:
\begin{enumerate}[label=(\roman*)]
	\item $\exists (x_1,\dots,x_n)\in \displaystyle\prod_{k=1}^nA_k,\ \forall (i,j)\in \crblanc{1}{n}^2,(i\ne j\implies x_i\ne x_j)$
	\item $\forall I\subset \crblanc{1}{n},\ \Card\left(\displaystyle\bigcup\limits_{i\in I}{A_i}\right)\geq \Card(I)$
\end{enumerate}

\section{\underline{Formule du Crible} \xens{3}}
\label{Formule du crible}
\textcolor{blue}{\hyperref[Formule du crible corrigé]{[Corrigé]}}\\
Soient $E$ un ensemble fini et $(A_i)_{1\leq i\leq n}$ une famille de parties de $E$. Montrer que:
\\ $${\displaystyle \operatorname {Card} \left(\bigcup _{i=1}^{n}A_{i}\right)=\sum _{k=1}^{n}\left((-1)^{k+1}\sum _{1\leq i_{1}<i_{2}<\cdots <i_{k} \leq n}\operatorname {Card} \left(\bigcap _{j=1}^{k}A_{i_{j}}\right)\right)}$$

\section{Cardinal de GL$_n(K)$ et SL$_n(K)$ \etoile{4} (HP)}
\label{Cardinal de GL_n et SL_n}
\textcolor{blue}{\hyperref[Cardinal de Gl_n et Sl_n corrigé]{[Corrigé]}}\\
Soit $K$ un corps commutatif fini de cardinal $q$.
\begin{enumerate}
	\item Déterminer le cardinal de GL$_n(K)$.
	\item En déduire le cardinal de SL$_n(K)$.
\end{enumerate}

\section{$\Q$ est dénombrable \ccinp{2}}
\label{Q est dénombrable}
\textcolor{blue}{\hyperref[Q est dénombrable corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Montrer que $\Z$ est dénombrable.
	\item Montrer que $\N^2$ est dénombrable.
	\item En déduire que le produit cartésiens d'un nombre fini d'ensembles au plus dénombrables est au plus dénombrable.
	\item Montrer que $\Q$ est dénombrable.
\end{enumerate}

\section{$\R$ n'est pas dénombrable \xens{3}}
\label{R n'est pas dénombrable}
\textcolor{blue}{\hyperref[R n'est pas dénombrable corrigé]{[Corrigé]}}\\
Soit $f:\N^*\to [0,1[$. Pour chaque $k\in \N^*$, on note $u_k$ la $k$-ième décimale du développement décimal propre de $f(k)$ et on pose $v_k=0$ si $u_k=1$ et $v_k=1$ sinon. Montrer que $y=0,v_1v_2\dots v_n\dots$ n'a pas d'antécédent par $f$. Que dire ?

\section{Dénombrabilité des nombres algébriques \centraleponts{3}}
\label{Dénombrabilité des nombres algébriques}
\textcolor{blue}{\hyperref[Dénombrabilité des nombres algébriques corrigé]{[Corrigé]}}\\
Montrer que l'ensemble des nombres complexes qui sont racine d'un polynôme non nul à coefficients rationnels est dénombrable.

\section{Théorème de Cantor \etoile{3}}
\label{Théorème de Cantor}
\textcolor{blue}{\hyperref[Théorème de Cantor corrigé]{[Corrigé]}}\\
Soit $E$ un ensemble. Montrer qu'il n'existe pas de surjection de $\mathcal P(E)$ dans $E$.\\
En déduire que l'ensemble des parties d'un ensemble dénombrable n'est pas dénombrable.

\section{Fonction qui intervertit rationnels et irrationnels \centraleponts{3}}
\label{Fonction qui intervertit rationnels et irrationnels}
\textcolor{blue}{\hyperref[Fonction qui intervertit rationnels et irrationnels corrigé]{[Corrigé]}}\\
Existe-t-il une fonction continue $f:\R\to\R$ telle que $f(\Q)\subset \R\backslash\Q$ et $f(\R\backslash\Q)\subset \Q$.

\section{Support d'une famille sommable \telecom{2}}
\label{Support d'une famille sommable}
\textcolor{blue}{\hyperref[Support d'une famille sommable corrigé]{[Corrigé]}}\\
Soit $(a_i)_{i\in I}$ une famille sommable de nombres complexes. Montrer que son support $S=\{i\in I,\ a_i\ne 0\}$ est au plus dénombrable.

\section{Théorème de Froda \centraleponts{3}}
\label{Théorème de Froda}
\textcolor{blue}{\hyperref[Théorème de Froda corrigé]{[Corrigé]}}\\
Soit $f$ une fonction monotone sur un intervalle $I$ de $\R$.
On définit l'oscillation de $f$ en $x\in I$ par $\omega(x)=\left|\lim\limits_{x^+}f-\lim\limits_{x^-}f\right|$.\\
Justifier que $\omega$ est bien définie sur $I$ puis montrer que l'ensemble des points de discontinuité de $f$ est au plus dénombrable.

\section{Ouverts de $\R$ \xens{4}}
\label{Ouverts de R}
\textcolor{blue}{\hyperref[Ouverts de R corrigé]{[Corrigé]}}\\
Montrer que tout ouvert de $\R$ s'écrit comme réunion au plus dénombrable d'intervalles ouverts et que de plus, ces intervalles peuvent être pris disjoints.

\section{Ensemble discret \telecom{2}}
\label{Ensemble discret}
\textcolor{blue}{\hyperref[Ensemble discret corrigé]{[Corrigé]}}\\
Soit $E$ un espace vectoriel normé. On dit qu'un point $x$ d'une partie $A$ de $E$ est isolé lorsqu'il existe un voisinage $\mathcal V(x)$ de $x$ tel que $\mathcal V(x)\cap A=\{x\}$. Lorsque tous les points de $A$ sont isolés, on dit que $A$ est un ensemble discret.\\
\textit{Remarque : Un point qui n'est pas isolé est aussi appelé point d'accumulation.}
\begin{enumerate}
	\item L'ensemble $A=\displaystyle\left\{\frac{1}{n},\ n\in \N^*\right\}$ est-t-elle une partie discrète de $\R$ ? Même question pour $B=A\cup\{0\}$.
	\item Montrer qu'une partie discrète de $\R$ est au plus dénombrable.
	\item Une partie dénombrable de $\R$ est-elle forcément discrète ?
\end{enumerate}

\section{Ensemble parfait}
\label{Ensemble parfait}
\textcolor{blue}{\hyperref[Ensemble parfait corrigé]{[Corrigé]}}\\
Soit $E$ un espace vectoriel normé. Un sous-ensemble $A$ de $E$ est dit \textit{parfait} s'il est fermé et n'a aucun point isolé (cf. \ref{Ensemble discret}).

\subsection{Poussière de Cantor}
On admet (cf. \ref{Développement décimal propre d'un réel}) que tout réel $x\in [0,1[$ s'écrit de manière unique $x=\displaystyle\sum_{n=1}^{+\infty}\frac{a_n}{3^n}$ où $(a_n)_{n\in \N^*}$ une suite à valeurs dans $\{0,1,2\}$ qui ne stationne pas à $2$. On définit \textit{l'ensemble de Cantor} $K=\displaystyle\left\{\sum_{n=1}^{+\infty}\frac{a_n}{3^n}\ |\ \forall n\in \N^*,\ a_n\in \{0,2\}\right\}$.\\
Montrer que $K$ est un ensemble parfait, d'intérieur vide, compact et non dénombrable.

\subsection{Parfaits de $\R$}
\begin{enumerate}[leftmargin=*]
	\item Donner un exemple d'ensemble parfait de $\R$.
	\item Montrer qu'un ensemble parfait non vide de $\R$ n'est pas dénombrable.
	\item En déduire que l'intervalle $]0,1[$ n'est pas réunion dénombrable de segments disjoints.
\end{enumerate}

\subsection{Théorème de Cantor-Bendixson}
Montrer que toute partie fermée de $\R$ se décompose de façon unique comme réunion disjointe d'une partie dénombrable et d'un ensemble parfait.

\newpage
\chapter{Probabilités}
Dans toute cette section les variables aléatoires sont définies sur un univers $\Omega$ au plus dénombrable.\\

\section{Somme de variables de Bernoulli indépendantes \etoile{2}}
\label{Somme de variables de Bernoulli indépedantes}
\textcolor{blue}{\hyperref[Somme de variables de Bernoulli indépendantes corrigé]{[Corrigé]}}\\
Soient $X_1,\dots,X_n$ des variables indépendantes et identiquement distribuées suivant une loi de Bernoulli de paramètre $p\in]0,1[$. Montrer que $X=X_1+\dots+X_n$ suit une loi binomiale de paramètres $n$ et $p$.

\section{Approximation d'une loi de Poisson par des lois binomiales \ccinp{1}}
\label{Approximation d'une loi de Poisson par des lois binomiales}
\textcolor{blue}{\hyperref[Approximation de la loi de Poisson par des lois binomiales corrigé]{[Corrigé]}}\\
Soit $(X_n)_{n\geq1}$ une suite de variables aléatoires telle que, $\forall n\geq1,\ X_n\sim\B(n,p_n)$. On suppose que la suite $(np_n)_{n\geq1}$ converge vers un réel $\lambda>0$.
\\ Démontrer que pour tout $k\in\N$, $\lim\limits_{n\to +\infty}\p(X_n=k)=\displaystyle\frac{\lambda^ke^{-\lambda}}{k!}$

\section{Inégalité de Markov et inégalité de Bienaymé-Tchébychev \ccinp{1}}
\label{Inégalité de Markov et inégalité de Bienaymé-Tchebychev}
\textcolor{blue}{\hyperref[Inégalité de Markov et inégalité de Bienaymé-Tchébychev corrigé]{[Corrigé]}}\\
Enoncer et démontrer les inégalités de Markov et de Bienaymé-Tchébychev.

\subsection{Utilisation de l'inégalité de Bienaymé-Tchebychev}
Soit $X$ une variable aléatoire discrète telle que  $\E(X)=10$ et $\V(X)=5$. Montrer que :
$$\forall n\in\N,\ n\geq 50 \implies \p(10-n< X<10+n)\geq 0.99$$

\section{Paradoxe des anniversaires \ccinp{2}}
\label{Paradoxe des anniversaires}
\textcolor{blue}{\hyperref[Paradoxe des anniversaires corrigé]{[Corrigé]}}\\
On considère une classe de $n$ élèves. Pour chaque élève, on suppose que chaque jour de l'année a la même probabilité d'être le jour de son anniversaire et on ne prend pas en compte les années bissextiles.
\\Calculer la probabilité $p_n$ que deux élèves au moins de cette classe aient leur anniversaire le même jour. A partir de combien d'élèves cette probabilité devient-elle supérieure à $0,5$? Combien vaut-elle si $n=50$ ?

\subsection{Généralisation \centraleponts{3}}
On cherche désormais la probabilité $p_{k,n}$ que $k$ élèves d'une classe de $n$ élèves aient la même date d'anniversaire.
\begin{enumerate}
	\item Ecrire un programme python calculant $p_{k,n}$.
	\item Déterminer une expression de $p_{k,n}$ à l'aide de la formule du crible (cf. \ref{Formule du crible}).
\end{enumerate}

\section{Variable aléatoire presque sûrement nulle/constante \ccinp{2}}
\label{Variable aléatoire presque sûrement nulle/constante}
\textcolor{blue}{\hyperref[Variable aléatoire presque sûrement nulle/constante]{[Corrigé]}}\\
\begin{enumerate}
	\item Soit $X$ une variable aléatoire discrète telle que $\E(|X|)=0$. 
	\\Montrer que $X$ est presque sûrement nulle, c'est-à-dire que $\p(X=0)=1$.
	\item Soit $X$ une variable aléatoire discrète telle que $\V(X)=0$.\\
	Montrer que $X$ est presque sûrement constante, c'est à dire qu'il existe $a\in X(\Omega),\ \p(X=a)=1$.
\end{enumerate}

\section{Loi de Pascal \telecom{2}}
\label{Loi de Pascal}
\textcolor{blue}{\hyperref[Loi de Pascal corrigé]{[Corrigé]}}\\
Soit $(X_n)_{n\geq 1}$ une suite de variables indépendantes et identiquement distribuées selon une loi de Bernoulli de paramètre $p\in]0,1[$.
\\Pour $r\in\N^*$, on définit une variable aléatoire $T_r$ à valeurs dans $\N^*\cup\{+\infty\}$ en posant: $$T_r=\min(\{n\in\N^*|X_1+\dots+X_n=r\})$$
\begin{enumerate}
	\item Reconnaître la loi de $T_r$ lorsque $r=1$.
	\item Soit $r>1$. Pour $n\in\N^*$ déterminer $\p(X_1+\dots+X_n=r-1)$ et en déduire $\p(T_r=n)$.
	\item Montrer que l'événement $\{T_r=+\infty\}$ est négligeable.
\end{enumerate}

\section{\underline{Lemme de Borel-Cantelli et loi du zéro-un de Borel} \centraleponts{3}}
\label{Lemme de Borel-Cantelli et loi du zéro-un de Borel}
\textcolor{blue}{\hyperref[Lemme de Borel-Cantelli et loi du zéro-un de Borel corrigé]{[Corrigé]}}\\
Soit $(A_n)_{n\in\N}$ une suite d'événements d'un espace probabilisé $(\Omega,\A,\p)$. On pose $B_n=\displaystyle\bigcup\limits_{k\geq n}A_k$ et $A=\displaystyle\bigcap\limits_{n\in\N}B_n$.
\begin{enumerate}
	\item On suppose que la série $\displaystyle\sum\limits_{n\in\N}\p(A_n)$ converge.
	\begin{enumerate}[label=\alph*.]
		\item Montrer que $\p(A)=\lim\limits_{n\to+\infty}\p(B_n)$.
		\item En déduire que $\p(A)=0$.
	\end{enumerate}
	\item On suppose que les $A_n$ sont mutuellement indépendants et que la série $\displaystyle\sum\limits_{n\in\N}\p(A_n)$ diverge.
	\begin{enumerate}[label=\alph*.]
		\item Soit $(n,p)\in\N^2$. Montrer que $$\p\left(\bigcap\limits_{k=n}^{n+p}\overline{A_k}\right)\leq \exp\left(-\sum\limits_{k=n}^{n+p}\p(A_k)\right)$$
		\item En déduire que $\p(A)=1$.
	\end{enumerate}
\end{enumerate}

\section{\underline{Formule d'antirépartition} \telecom{3}}
\label{Formule d'antirépartition}
\textcolor{blue}{\hyperref[Formule d'antirépartition corrigé]{[Corrigé]}}\\
Soit X une variable aléatoire à valeurs dans $\N$. Montrer que $X$ admet une espérance finie si et seulement si la série $\displaystyle\sum\limits_{n\in \N}\p(X>n)$ converge et que, dans ce cas, $\E(X)=\displaystyle\sum\limits_{n=0}^{+\infty}\p(X>n)$.

\section{Loi de Poisson \ccinp{2}}
\label{Loi de Poisson}
\textcolor{blue}{\hyperref[Loi de Poisson corrigé]{[Corrigé]}}\\
On considère un péage composé de $m$ guichets. On note $N$ la variable aléatoire égale au nombre de voitures utilisant le péage en 1h. $N$ suit une loi de Poisson de paramètre $\lambda>0$. Le choix du guichet se fait de manière aléatoire et indépendamment des autres voitures. On note $X$ la variable aléatoire égale au nombre de voitures ayant pris le guichet n°1.
\begin{enumerate}
	\item Calculer la probabilité conditionnelle $\p(X=k|N=n)$ pour $0\leq k \leq n$.
	\item Montrer que $\p(X=k)=e^{-\lambda}\displaystyle\frac{1}{k!}\left( \frac{\lambda}{m}\right)^k\sum\limits_{n=0}^{+\infty}\lambda^n\left(1-\frac{1}{m}\right)^n\frac{1}{n!}$.
	\item Donner la loi de X.
	\item Espérance et variance de X ?
\end{enumerate}

\section{Maximum de deux lois géométriques indépendantes \ccinp{1}}
\label{Maximum de deux lois géométriques indépendantes}
\textcolor{blue}{\hyperref[Maximum de deux lois géométriques indépendantes corrigé]{[Corrigé]}}\\
Soient $X$ et $Y$ deux variables aléatoires indépendantes suivant des lois géométriques de paramètres respectifs $p$ et $q$ non égaux à $0$ ou $1$.
\begin{enumerate}
	\item Déterminer l'espérance de $M=\min(X,Y)$.
	\item Déterminer l'espérance de $Z=\max(X,Y)$.
\end{enumerate}

\section{Max et min de lois géométriques iid \centraleponts{4}}
\label{Max et min de lois géométriques iid}
\textcolor{blue}{\hyperref[Max et min de lois géométriques iid corrigé]{[Corrigé]}}\\
Soit $(X_n)_{n\geq1}$ une suite de variables indépendantes et identiquement distribuées selon une loi géométrique de paramètre $p\in]0,1[$. Pour $n\in\N^*$, on pose: $$Y_n=\min\{X_1,\dots,X_n\} \text{ et } Z_n=\max\{X_1,\dots,X_n\} $$
\begin{enumerate}
	\item Calculer $\E(Y_n)$.
	\item Déterminer un équivalent de $\E(Z_n)$ lorsque $n$ tend vers l'infini.
\end{enumerate}

\section{\underline{Formule de Wald} \centraleponts{3}}
\label{Formule de Wald}
\textcolor{blue}{\hyperref[Formule de Wald corrigé]{[Corrigé]}}\\
Soit $X$ une variable aléatoire à valeurs dans $\N$ sur un espace probabilisé $(\Omega,\A,\p)$. On considère une suite $(X_n)_{n\in\N^*}$ de variables aléatoires indépendantes sur $(\Omega,\A,\p)$ suivant la même loi que $X$.
\\ On se donne une autre variable aléatoire $N$ à valeurs dans $\N^*$ sur le même espace probabilisé $(\Omega,\A,\p)$ indépendante des variables aléatoires précédentes.
\\ On pose $S=\displaystyle\sum\limits_{k=1}^{N}X_k$, c'est-à-dire: $$\forall \omega\in\Omega,\ S(\omega)=\sum\limits_{k=1}^{N(\omega)}X_k(\omega)$$
\begin{enumerate}
	\item Justifier que $S$ est bien une variable aléatoire discrète.
	\item On note $G_X,G_N$ et $G_S$ les fonctions génératrices respectives de $X,N$ et $S$ Montrer que pour tout $t\in[0,1],G_S(t)=G_N\circ G_X(t)$.
	\item On suppose que les variables aléatoires $X$ et $N$ admettent des espérances finies. Montrer qu'il en est de même pour $S$ et exprimer son espérance en fonction de celles de $N$ et $X$.
	\item On suppose que les variables aléatoires $X$ et $N$ admettant des variances finies. Montrer qu'il en est de même pour $S$ et exprimer sa variance en fonction des espérances et des variances de $N$ et $X$.
\end{enumerate}

\section{Loi binomiale aléatoire \centraleponts{3}}
\label{Loi binomiale aléatoire}
\textcolor{blue}{\hyperref[Loi binomiale aléatoire corrigé]{[Corrigé]}}\\
Sur un espace probabilisé $(\Omega,\A,\p)$, on considère une suite $(X_n)_{n\in \N}$ de variables aléatoires et $p\in ]0,1[$ telles que\\
$\forall n\in \N,\ X_n\sim \B(n,p)$.\\
On considère aussi une variable aléatoire $N$ indépendante des variables $X_n$ et telle que $N+1\sim \mathcal{G}(q)$ avec $q\in ]0,1[$.\\
Pour toute issue $\omega$ de l'univers $\Omega$, on pose $Y(\omega)=X_{N(\omega)}(\omega)$.\\
Justifier que $Y$ est bien une variable aléatoire discrète et déterminer sa loi.

\section{\underline{Somme de lois de Poisson} \ccinp{1}}
\label{Somme de lois de Poisson}
\textcolor{blue}{\hyperref[Somme de lois de Poisson corrigé]{[Corrigé]}}\\
Soient $\lambda_1,\dots,\lambda_n$ des réels et $X_1,\dots,X_n$ des variables aléatoires indépendantes telles que $\forall i\in \crblanc{1}{n},\ X_i\sim\mathcal P(\lambda_i)$.\\
Montrer que $S=\displaystyle\sum_{i=1}^nX_i$ suit une loi de Poisson dont on précisera le paramètre.

\section{Obtenir trois pile consécutifs \centraleponts{3}}
\label{Obtenir trois pile consécutifs}
\textcolor{blue}{\hyperref[Obtenir trois pile consécutifs corrigé]{[Corrigé]}}\\
\begin{enumerate}
	\item Montrer que $X^3-X^2-X-1=(X-a)(X-b)(X-\overline{b})$ avec $a\in]1;2[$, $b\in\C\setminus\R$ et $|b|<1$.
	\item On lance une infinité de fois une pièce équilibrée. On note $p_n$ la probabilité pour que la séquence PPP apparaisse pour la première fois au $n$-ième lancer. Exprimer $p_{n+3}$ en fonction de $p_n$, $p_{n+1}$ et $p_{n+2}$.
	\item Donner une expression et un équivalent de $p_n$.
\end{enumerate}

\section{\underline{Lancer de dés équitable} \centraleponts{3}}
\label{Lancer de dés équitable}
\textcolor{blue}{\hyperref[Lancer de dés équitable corrigé]{[Corrigé]}}\\
On lance deux dés à 6 faces numérotées de un à six et on note $X$ la somme obtenue. Les deux lancées sont supposés indépendants.
\\ En truquant convenablement les 2 dés, $X$ peut-elle suivre la loi uniforme sur $\crblanc{2}{12}$ ?

\subsection{Généralisation \etoile{4}}

On lance $n$ dés à $m$ faces numérotées de $1$ à $m$. On note pour $k\in \crblanc{1}{n},\ X_k$ la variable aléatoire qui donne le numéro obtenu pour le $k$-ième dé. On note $S=\displaystyle\sum_{k=1}^nX_k$.\\
En truquant convenablement les dés, $S$ peut-elle suivre une loi uniforme sur $\crblanc{n}{nm}$ ?

\section{Espérance conditionnelle \centraleponts{3}}
\label{Espérance conditionnelle}
\textcolor{blue}{\hyperref[Espérance conditionnelle corrigé]{[Corrigé]}}\\
Soit $X$ une variable aléatoire discrète sur un espace probabilisé $(\Omega,\A,\p)$ à valeurs dans $\K$.
\\Pour $A$ un événement non négligeable et sous réserve d'existence, on introduit l'espérance conditionnelle de $X$ sachant $A$ comme étant égale à l'espérance de $X$ pour la probabilité conditionnelle $\p_A$, c'est-à-dire: $$\E_A(X)=\displaystyle\sum\limits_{x\in X(\Omega)} x\p_A(X=x)$$
\\On suppose que $X$ est d'espérance finie.
\begin{enumerate}
	\item Justifier l'existence de $\E_A(X)$ et vérifier que $\E_A(X)=\displaystyle\frac{\E(\mathbbm{1}_A\cdot X)}{\p(A)}$.
	\item Soit $(A_i)_{i\in I}$ un système complet d'événements non négligeables. Vérifier que $\E(X)=\displaystyle\sum\limits_{i\in I}\E_{A_i}(X)\p(A_i)$
\end{enumerate}

\section{\underline{Problème du collectionneur} \ccinp{2}}
\label{Problème du collectionneur}
\textcolor{blue}{\hyperref[Problème du collectionneur corrigé]{[Corrigé]}}\\
Un étudiant en classe préparatoire achète des cartes \textcopyright Pokémon TCG Pocket pour compléter un set contenant $n$ cartes, où $n$ est un entier naturel supérieur ou égal à 2. Lorsqu'on achète une carte, celle-ci est cachée et on ne peut la choisir. On s'intéresse au nombre d'achats nécessaires pour le set. On suppose que la répartition des $n$ cartes est uniforme au cours de tous les achats, si bien qu'on assimile l'expérience aléatoire "acheter une carte" à un tirage avec remise d'une carte (numéroté entre $1$ et $n$) dans un booster contenant les $n$ cartes.
\\On note $T_n$ la variable aléatoire égale au nombre de tirages nécessaires pour obtenir pour la première fois au moins chacune des $n$ cartes. A chaque tirage, on appelle succès le fait d'avoir tiré un carte non encore obtenue.
Pour $i\in\crblanc{1}{n}$, on note $X_{n,i}$ la variable aléatoire donnant le nombre de tirages nécessaires pour obtenir pour la première fois $i$ numéros différents, en comptant à partir du succès précédent. Par convention, $X_{n,1}=1$ et on a ainsi $T_n=\displaystyle\sum\limits_{i=1}^nX_{n,i}$.
\begin{enumerate}
	\item Soit $i\in\crblanc{2}{n}$.Déterminer la loi de $X_{n,i}$ et donner, sous réserve d'existence, l'espérance et la variance de $X_{n,i}$.
	\item Justifier que les variables aléatoires $(X_{n,i})_{i\in\crblanc{1}{n}}$ sont mutuellement indépendantes.
	\item En déduire une expression de $\E(T_n)$ et $\V(T_n)$ faisant intervenir $H_n=\displaystyle\sum\limits_{k=1}^n\frac{1}{k}$ et $S_n=\displaystyle\sum\limits_{k=1}^n\frac{1}{k^2}$.
	\item Donner des équivalents simples de $\E(T_n)$ et $\V(T_n)$.
\end{enumerate}

\subsection{Généralisation}
Soient $X_1,\dots,X_n$ des variables aléatoires i.i.d. suivant la loi d'une variable aléatoire $X$ fixée à valeur dans $\N$. On note $R_n=\Card(\{X_1,\dots,X_n\})$.
\begin{enumerate}
	\item Soit $a\in \R_+$. Montrer que $\E(R_n)\leq a+n\p(X\geq a)$.
	\item En déduire que $\E(R_n)\unfty=\smallo{n}$.
	\item Montrer que si $X$ admet une espérance finie, on a même $\E(R_n)\unfty=\bigO{\sqrt n}$.\\
	\textit{Indication : Penser à l'inégalité de Markov}
\end{enumerate}

\section{Problème de la ruine du joueur}
\label{Problème de la ruine du joueur}
\textcolor{blue}{\hyperref[Problème de la ruine du joueur corrigé]{[Corrigé]}}\\
Deux personnes $A$ et $B$ jouent à pile ou face avec une pièce qui a une probabilité $p$ de tomber sur pile et $1-p$ de tomber sur face. Le joueur $A$ possède au début du jeux un capital de $a$ euros et le joueur $B$ un capital de $b$ euros. A chaque lancer, si la pièce tombe sur pile le joueur $A$ donne un euro au joueur $B$, sinon c'est le joueur $B$ qui donne un euro au joueur $A$. Ils continuent de jouer jusqu'à que l'un d'eux soit ruiné.
\begin{enumerate}
	\item Quelle est la probabilité que le joueur $A$ finisse ruiné ?
	\item Et si le joueur $A$ joue au même jeux contre le casino ?
\end{enumerate}

\section{Passager d'un avion}
\label{Passager d'un avion}
\textcolor{blue}{\hyperref[Passager d'un avion corrigé]{[Corrigé]}}\\
100 passagers s'apprêtent à monter dans un avion de 100 places. Le premier passager entre et s'assied à une place au hasard. Les suivants prennent leur place si elle est disponible, et choisissent une place libre au hasard sinon. Quelle est la probabilité que le dernier passager se retrouve à sa place ?

\section{Fonction de répartition \centraleponts{4}}
\label{Fonction de répartition}
\textcolor{blue}{\hyperref[Fonction de répartition corrigé]{[Corrigé]}}\\
Soit $X$ une variable aléatoire discrète à valeurs réelles.
\\On appelle \textit{fonction de répartition} de la variable $X$, l'application $F_X:\R\to\R$ définie par: $$F_X(x)=\p(X\leq x) \ \text{pour tout } x\in\R.$$
\begin{enumerate}
	\item Montrer que la fonction $F_X$ est croissante et déterminer ses limites en $\pm \infty$.
	\item Montrer que la fonction $F_X$ est continue à droite en tout point.
	\item A quelle condition $F_X$ est-elle continue en un point $a$ de $\R$?
\end{enumerate}

\section{\underline{Loi sans mémoire} \telecom{2}}
\label{Loi sans mémoire}
\textcolor{blue}{\hyperref[Loi sans mémoire corrigé]{[Corrigé]}}\\
Soit $X$ une variable aléatoire à valeurs dans $\N^*$. Montrer que $X$ suit une loi géométrique si et seulement si $$\forall (n,k)\in\N^2, \ \p(X>n+k|X>n)=\p(X>k)$$

\section{Caractérisation de la loi de Poisson par l'espérance}
\label{Caractérisation de la loi de Poisson par l'espérance}
\textcolor{blue}{\hyperref[Caractérisation de la loi de Poisson par l'espérance corrigé]{[Corrigé]}}\\
Soit $(\Omega,A,\p)$ un espace probabilisé.
\begin{enumerate}
	\item Soient $\lambda>0$ et $N$ une variable aléatoire suivant la loi de Poisson $\mathcal{P}(\lambda)$.
	\\Montrer que si $g$ est une fonction de $\N$ dans $\N$, alors $\E(Ng(N))=\lambda\E(g(N+1))$.
	\item Soit $T$ une variable aléatoire telle que $T(\Omega)=\N$ et telle que $\E(Tg(T))=\lambda\E(g(T+1))$ pour toute fonction $g$ de $\N$ dans $\N$ telle que $\E(g(T+1))<+\infty$. Montrer que $T$ suit une loi de Poisson.
\end{enumerate}

\section{\underline{Loi Zéta} \ccinp{2}}
\label{Loi Zéta}
\textcolor{blue}{\hyperref[Loi Zéta corrigé]{[Corrigé]}}\\
Soit $s>1$ un nombre réel et soit $X$ une variable aléatoire à valeurs dans $\N^*$ dont la loi est donnée par :\\
$\forall n\in \N^*,\ \p(X=n)=\displaystyle\frac{\lambda}{n^s}$ pour un certain réel $\lambda$. On pose $\zeta(s)=\displaystyle\sum\limits_{n=1}^{+\infty}\frac{1}{n^s}$. Si $n\in\N^*$, on note $\{n|X\}$ l'événement "$n$ divise $X$" et $\{n\nmid X\}$ l'événement complémentaire. Enfin on note $(p_n)_{n\in \N^*}$ la suite strictement croissante des nombres premiers.
\begin{enumerate}
	\item Que vaut $\lambda$ ?
	\item Calculer $\p(n|X)$ pour $n\in\N^*$.
	\item Soit $(\alpha_i)_{i\in\N^*}$ une suite d'entiers naturels. Montrer que les événements $\{p_1^{\alpha_1}|X\},\{p_2^{\alpha_2}|X\},\dots$ sont mutuellement indépendants.
	\item Soit $r\geq1$ un entier. Montrer que $\p\left(\displaystyle\bigcap\limits_{i=1}^r\{p_i\nmid X\}\right)=\displaystyle\prod\limits_{i=1}^r(1-p_i^{-s})$.
	\item En déduire que $\zeta(s)^{-1}=\lim\limits_{n\to+\infty}\displaystyle\prod\limits_{k=1}^n \left(1-p_k^{-s}\right)$.
	\item Est-ce que la famille $\left(\displaystyle\frac{1}{p}\right)_{p\in\mathcal{P}}$, où $\mathcal{P}$ désigne l'ensemble des nombres premiers, est sommable ?
\end{enumerate}

\section{Taux de panne \centraleponts{2}}
\label{Taux de panne}
\textcolor{blue}{\hyperref[Taux de panne corrigé]{[Corrigé]}}\\
Soient $(\Omega,\A,\p)$ un espace probabilisé et $X$ une variable aléatoire définie sur $\Omega$ et à valeurs dans $\N^*$ vérifiant: $\forall n\in\N^*, \ \p(X\geq n)>0$.
\\On définit le \textit{taux de panne} de $X$ par la suite $(x_n)_{n\geq1}$ avec : $x_n=\p(X=n|X\geq n)$.
\begin{enumerate}
	\item Montrer que si l'on pose $\p(Y=n)=\displaystyle\frac{1}{n(n+1)}$ pour tout $n\in\N^*$, on définit une loi de probabilité. Déterminer le taux de panne de $Y$.
	\item Dans le cas général, établir que $\forall n\geq 2, \ \p(X\geq n)=\displaystyle\prod\limits_{k=1}^{n-1}(1-x_k)$.
	\item En déduire une expression de $\p(X=n)$ en fonction des $x_k$ valable pour tout $n\geq 1$.
	\item Déterminer les variables aléatoires discrètes à taux de panne constant.
\end{enumerate}

\section{Matrice aléatoire (1) \ccinp{2}}
\label{Matrice aléatoire 1}
\textcolor{blue}{\hyperref[Matrice aléatoire 1 corrigé]{[Corrigé]}}\\
Soient $X_1$ et $X_2$ des variables aléatoires indépendantes suivant une même loi géométrique de paramètre $p\in]0,1[$. On pose $$A=\begin{pmatrix}
	X_1 &1\\
	0 & X_2
\end{pmatrix}$$
\begin{enumerate}
	\item Montrer que $A$ est presque sûrement inversible.
	\item Trouver la probabilité pour que $A$ soit diagonalisable.
\end{enumerate}

\section{Matrice aléatoire (2)}
\label{Matrice aléatoire 2}
\textcolor{blue}{\hyperref[Matrice aléatoire 2 corrigé]{[Corrigé]}}\\
Soient $\lambda>0$ et $Y$ une variable aléatoire à valeurs dans $\Z$ tels que :
\begin{itemize}
	\item $\forall n\in \N,\ \p(Y=n)=\p(Y=-n)$;
	\item $|Y|\sim\mathcal P(\lambda)$
\end{itemize}
On pose $A=\begin{pmatrix}
	0&Y&1\\
	Y&0&1\\
	Y&1&0
\end{pmatrix}$
\begin{enumerate}
	\item Donner la loi de $\rg(A)$.
	\item Calculer la probabilité que $A$ soit diagonalisable.
\end{enumerate}

\section{Matrice aléatoire (3)}
\label{Matrice aléatoire 3}
\textcolor{blue}{\hyperref[Matrice aléatoire 3 corrigé]{[Corrigé]}}\\
Soient $X,Y,Z,T$ des variables aléatoires i.i.d. de loi de Benoulli $\B(p)$. On pose :
$$A=\begin{pmatrix}
	X&X&X&X\\
	X&Y&Y&Y\\
	X&Y&Z&Z\\
	X&Y&Z&T
\end{pmatrix}$$
\begin{enumerate}
	\item Donner la loi de $\Tr(A)$
	\item Calculer la probabilité que $A$ soit inversible.
	\item Calculer la probabilité que $A$ soit diagonalisable.
\end{enumerate}

\section{Matrice aléatoire (4)}
\textcolor{blue}{\hyperref[Matrice aléatoire 4 corrigé]{[Corrigé]}}\\
\label{Matrice aléatoire 4}
Soient $\lambda >0$, $X$ et $Y$ deux variables aléatoires i.i.d. de loi $\mathcal P(\lambda)$.\\
On pose $A=\begin{pmatrix}
	(-1)^X&1\\
	(-1)^Y&1
\end{pmatrix}$
\begin{enumerate}
	\item Calculer la probabilité que $A$ soit inversible.
	\item Calculer la probabilité que $A$ soit diagonalisable.
\end{enumerate}

\section{Matrice aléatoire (5)}
\label{Matrice aléatoire 5}
\textcolor{blue}{\hyperref[Matrice aléatoire 5 corrigé]{[Corrigé]}}\\
Soient $p,q\in ]0,1[$ et $X,Y$ deux variables aléatoires telles que $X\sim\mathcal G(p)$ et $Y\sim\mathcal G(q)$.\\
On pose $A=\begin{pmatrix}
	X&-Y\\
	Y&-X
\end{pmatrix}$
Calculer la probabilité que $A$ soit diagonalisable.

\section{Matrice aléatoire (6)}
\label{Matrice aléatoire 6}
\textcolor{blue}{\hyperref[Matrice aléatoire 6 corrigé]{[Corrigé]}}\\
Soit $p$ un nombre premier et $X_0,\dots,X_{p-1}$ des variables aléatoires i.i.d. de loi $\B\left(\displaystyle\frac{1}{2}\right)$. On pose :
$$A=\begin{pmatrix}
	X_0&X_1&\cdots&X_{p-1}\\
	X_{p-1}&X_0&\cdots&X_{p-2}\\
	\vdots&\vdots&\ddots&\vdots\\
	X_1&X_2&\cdots&X_0
\end{pmatrix}$$
Quelle est la probabilité que $A$ soit inversible ?

\section{Matrice aléatoire (7)}
\label{Matrice aléatoire 7}
\textcolor{blue}{\hyperref[Matrice aléatoire 7 corrigé]{[Corrigé]}}\\
Soit $M\in \M_n(\C)$ une matrice aléatoire dont les coefficients sont choisis uniformément dans $\{0,1\}$. Montrer que la probabilité que $M$ soit inversible est minorée par une constante indépendante de $n$.

\section{Matrice de Rademacher}
\label{Matrice de Rademacher}
\textcolor{blue}{\hyperref[Matrice de Rademacher corrigé]{[Corrigé]}}\\
Une variable aléatoire $X$ suit une loi de Rademacher si $X(\Omega)=\{-1,1\}$ et $\p(X=-1)=\p(X=1)=\displaystyle\frac{1}{2}$. Soit $M=(X_{ij})_{1\leq i,j\leq n}$ une matrice aléatoire telle que les $X_{ij}$ suivent des lois de Rademacher mutuellement indépendantes. On note $p_n$ la probabilité que les colonnes de $M$ forment une base orthogonale de $\M_{n,1}(\R)$.\\
Montrer qu'il existe une suite $(\varepsilon_n)_{n\in \N}$ de limite nulle et $N\in \N$ tels que $\forall n\geq N,\ p_n\leq e^{-n}\sqrt{2\pi n}(1+\varepsilon_n)$.

\section{Vecteur propre aléatoire \centraleponts{3}}
\label{Vecteur propre aléatoire}
\textcolor{blue}{\hyperref[Vecteur propre aléatoire corrigé]{[Corrigé]}}\\
Soient $A=\begin{pmatrix}
	0 &-1&0\\2&1&-2\\1&-1&1\end{pmatrix}$ et $U=\begin{pmatrix}X\\Y\\Z\end{pmatrix}$
avec $X,Y,Z$ trois variables aléatoires indépendantes, $X$ et $Z$ suivant $\mathcal{G}(p)$ avec $p\in]0,1[$ et $Y$ suivant $\mathcal{P}(\lambda)$ avec $\lambda\in\R^*_+$.
\\ Déterminer la probabilité que $U$ soit un vecteur propre de $A$.

\section{Equation différentielle à coefficients aléatoires}
\label{Equation différentielle à coefficients aléatoires}
\textcolor{blue}{\hyperref[Equation différentielle à coefficients aléatoires corrigé]{[Corrigé]}}\\
Soient $A,B,C$ trois variables aléatoires i.i.d. suivant une loi de Poisson de paramètre $\lambda>0$. On considère l'équation différentielle $Ay''+By'+Cy=0$.\\
Montrer que la probabilité $p(\lambda)$ que les solutions de cette équation différentielle s'annulent une infinité de fois tend vers $1$ pour $\lambda\to +\infty$.

\section{Série entière aléatoire}
\label{Série entière aléatoire}
\textcolor{blue}{\hyperref[Série entière aléatoire corrigé]{[Corrigé]}}\\
Soit $(X_n)_{n\in \N}$ une suite de variable aléatoire i.i.d. suivant une loi géométrique de paramètre $p$. Que peut-on dire du rayon de convergence de la série entière $\displaystyle\sum_{n\in \N}X_nz^n$ ?

\section{Permutation aléatoire}
\label{Permutation aléatoire}
\textcolor{blue}{\hyperref[Permutation aléatoire corrigé]{[Corrigé]}}\\
Soient $X_1,\dots,X_{n-1}$ des variables aléatoires mutuellement indépendantes telles que $\forall i\in \crblanc{1}{n-1},\ X_i\sim\mathcal U(\crblanc{i}{n})$. Déterminer la loi de la permutation aléatoire :
$$(n,X_n)\circ\cdots\circ(1,X_1)$$

\section{Permutations composées d'un grand cycle}
\label{Permutations composées d'un grand cycle}
\textcolor{blue}{\hyperref[Permutations composées d'un grand cycle corrigé]{[Corrigé]}}\\
Soit $n\in \N^*$. Soit $\sigma$ une variable aléatoire suivant la loi uniforme sur $\mathcal S_{2n}$. On note $p_n$ la probabilité que $\sigma$ ait dans sa décomposition en cycles à supports disjoints un cycle de longueur supérieure ou égale à $n$. Montrer que $(p_n)_{n\in \N^*}$ converge et calculer sa limite.

\section{Loi conjointe (1) \ccinp{2}}
\label{Loi conjointe 1}
\textcolor{blue}{\hyperref[Loi conjointe 1 corrigé]{[Corrigé]}}\\
Soient $X$ et $Y$ deux variables aléatoires à valeurs dans $\N$ et $p\in]0,1[$.
\\On suppose que la loi conjointe de X et Y vérifie:$$\p(X=k,Y=n)=\begin{cases} 
	\displaystyle\binom{n}{k}a^np(1-p)^n &\mbox{si } k\leq n \\
	0 &\mbox{sinon} 
\end{cases} \text{avec} \ a\in\R. $$ 
\begin{enumerate}
	\item Déterminer la valeur de $a$.
	\item Déterminer la loi marginale de $Y$.
	\item Sachant que : $\forall x\in]-1,1[,\ \displaystyle\sum\limits_{n=k}^{+\infty}\binom{n}{k}x^{n-k}=\frac{1}{(1-x)^{k+1}}$,\\Reconnaître la loi de $X$.
	\item Les variables $X$ et $Y$ sont-elles indépendantes ?
\end{enumerate}

\section{Loi conjointe (2) \centraleponts{3}}
\label{Loi conjointe 2}
\textcolor{blue}{\hyperref[Loi conjointe 2 corrigé]{[Corrigé]}}\\
Soient $n\in\N$ et $X$ et $Y$ deux variables aléatoires à valeurs dans $\crblanc{0}{n}$. On pose $\p(X=i,Y=j)=\lambda \displaystyle\binom{n}{i}\binom{n}{j}$ pour $(i,j)\in\crblanc{0}{n}^2$.
\begin{enumerate}
	\item Déterminer $\lambda$.
	\item Donner les lois marginales de $X$ et $Y$.
	\item $X$ et $Y$ sont-elles indépendantes ?
	\item On considère la matrice $B=(\p(X=i,Y=j))_{0\leq i,j \leq n}$. Expliciter $B$, puis calculer $B^p$ pour $p\in\N^*$.
	\item $B$ est-elle diagonalisable ? Déterminer ses valeurs propres et les sous-espaces propres associés.
\end{enumerate}

\section{Fonction caractéristique \centraleponts{2}}
\label{Fonction caractéristique}
\textcolor{blue}{\hyperref[Fonction caractéristique corrigé]{[Corrigé]}}\\
On appelle \textit{Fonction caractéristique} d'une variable aléatoire $X$ prenant des valeurs dans $\Z$, l'application $\varphi_X:\R\to\C$ donnée par $\varphi_X(t)=\E(e^{itX})$
\begin{enumerate}
	\item Vérifier que $\varphi_X$ est définie, continue sur $\R$ et $2\pi$-périodique.
	\item On suppose que $X$ admet une espérance. Vérifier que $\varphi_X$ est de classe $\mathcal{C}^1$ sur $\R$ et exprimer $\E(X)$ à l'aide de $\varphi_X'$.
	\item On suppose $X$ admet également une variance. Vérifier que $\varphi_X$ est de classe $\mathcal{C}^2$ sur $\R$ et exprimer $\V(X)$ à l'aide des dérivées de $\varphi_X$.
\end{enumerate}

\section{Fonction génératrice des moments \centraleponts{5}}
\label{Fonction génératrice des moments}
\textcolor{blue}{\hyperref[Fonction génératrice des moments corrigé]{[Corrigé]}}\\
Soit $X$ une variable aléatoire discrète réelle. On note $I_X$ l'ensemble des $t\in\R$ pour lesquels la variable $e^{tX}$ admet une espérance finie et l'on pose $$M_X(t)=\E(e^{tX}) \ \text{pour tout } t\in I_X$$
\begin{enumerate}
	\item Montrer que $I_X$ est un intervalle contenant $0$.
	\item On suppose que $0$ est intérieur à $I_X$. Montrer que la variable aléatoire $X$ admet un moment à tout ordre (c'est à dire que $X^n$ est d'espérance finie quel que soit $n\in \N$) et que, sur un intervalle centré en $0$, $$M_X(t)=\displaystyle\sum\limits_{n=0}^{+\infty}\frac{\E(X^n)}{n!}t^n$$
\end{enumerate}

\section{Inégalité de Jensen \ccinp{1}}
\label{Inégalité de Jensen}
\textcolor{blue}{\hyperref[Inégalité de Jensen corrigé]{[Corrigé]}}\\
Soient $f:\R\to\R$ une fonction dérivable et convexe ainsi que $X$ une variable aléatoire réelle admettant une espérance finie.
\begin{enumerate}
	\item Montrer que $f(X)\geq f(\E(X))+f'(\E(X))(X-\E(X))$.
	\item En déduire que si $f(X)$ admet une espérance finie alors $\E(f(X))\geq f(\E(X))$.
\end{enumerate}

\section{Inégalité de Hölder \telecom{2}}
\label{Inégalité de Hölder}
\textcolor{blue}{\hyperref[Inégalité de Hölder corrigé]{[Corrigé]}}\\
Soient $p,q\in ]1,+\infty[$ tels que $\displaystyle\frac{1}{p}+\frac{1}{q}=1$.
\begin{enumerate}
	\item Montrer que $\forall x,y\in \R_+,\ xy\leq \displaystyle\frac{x^p}{p}+\frac{y^q}{q}$.
	\item Soient $X,Y$ deux variables aléatoires réelles positives d'espérance finies. Montrer que :
	$$\E(XY)\leq \E(X^p)^{1/p}\E(Y^q)^{1/q}$$
	\textit{On pourra commencer par traiter le cas $\E(X^p)=\E(Y^q)=1$.}
	\item Quelle inégalité retrouve-t-on lorsque $p=q=2$ ?
\end{enumerate}

\section{Modes de convergences \centraleponts{3}}
\label{Modes de convergences}
\textcolor{blue}{\hyperref[Modes de convergences corrigé]{[Corrigé]}}\\
Soit $(X_n)$ une suite de variables aléatoires discrètes réelles sur $\Omega$, $X:\Omega\mapsto \R$ une variable aléatoire discrète.\\
\textit{Convergence en probabilité}. On dit que $(X_n)$ converge en probabilité vers X si : $$\forall \varepsilon>0 \,, \quad \displaystyle\lim_{n\to+\infty} \p(|X_n-X|>\varepsilon)=0.$$
\begin{enumerate}
	\item \textit{Convergence presque sûre} On dit que $(X_n)$ converge presque sûrement vers $X$ s'il existe un événement $A$ presque sûr tel que $(X_n)$ converge simplement vers $X$ sur $A$
	\begin{enumerate}[label=\alph*.]
		\item Supposons que $(X_n)$ converge presque sûrement vers $X$. Montrer que $(X_n)$ converge en probabilité vers $X$.
		\item Supposons que pour tout $\varepsilon>0$, $\displaystyle\sum\p(|X_n-X|>\varepsilon)$ converge. Montrer que $(X_n)$ converge presque sûrement vers $X$.
		\item Supposons que $(X_n)$ convergence en probabilité vers $X$, montrer qu'il existe une sous-suite de $(X_n)$ qui converge presque sûrement vers $X$. 
		\item Montrer que la réciproque de a. est fausse. 
		\\On pourra choisir les $X_n$ indépendants et suivant une loi de Bernoulli de paramètre $p_n$ tel que $p_n$ tend vers 0 et la série de terme général $p_n$ diverge.
	\end{enumerate}
	\item \textit{Convergence $\mathcal{L}^1$} Si les $X_n$ et $X$ admettent une espérance, on dit que $(X_n)$ converge dans $\mathcal{L}^1$ vers $X$ quand la suite $(\E(|X_n-X|))_{n\in\N}$ converge vers 0.
	\\Montrer que la convergence dans $\mathcal{L}^1$ implique la converge en probabilité et que la réciproque est fausse.
	\item \textit{Convergence en loi} On suppose ici que les $X_n$ et $X$ sont à valeurs dans $\Z$. On dit que $(X_n)$ converge en loi quand vers $X$  si pour tout $k\in\Z$, $\displaystyle\lim_{n\to+\infty}\p(X_n=k)=\p(X=k)$.
	\\ Montrer que la convergence en probabilité implique la convergence en loi et que la réciproque est fausse.
\end{enumerate}

\section{Marche aléatoire sur $\Z^d$ \etoile{3}}
\label{Marche aléatoire sur Z^d}
\textcolor{blue}{\hyperref[Marche aléatoire sur Z^d corrigé]{[Corrigé]}}\\
Dans les exercices suivant, on s'intéresse à

\subsection{\underline{Le cas $d=1$} \etoile{2}}
On considère une puce se déplaçant sur une droite graduée par les entiers relatifs. Sa position à l'instant initial $t=0$ est $k=0$. A chaque instant $t\in\N^*$, elle se déplace aléatoirement de sa position $k\in\Z$ à la position $k+1$ ou $k-1$.
Soit $p\in]0,1[$. On définit sur un espace probabilisé $(\Omega,\A,\p)$ une suite de variables aléatoires indépendantes et identiquement distribuées $(X_t)_{t\in\N^*}$ dont la loi est définie par: $$\forall t\in\N^*,\ \p(X_t=1)=p \ \text{et } \p(X_t=-1)=1-p.$$
\\ Enfin, pour tout $n\in\N^*$, on pose $S_n=\displaystyle\sum\limits_{t=1}^nX_t$.
\begin{enumerate}
	\item Pour tout $t\in\N^*$, déterminer la loi de la variable aléatoire $Y_t=\displaystyle\frac{X_t+1}{2}$. En déduire que pour tout $n\in\N^*$, la variable aléatoire $\displaystyle\sum\limits_{t=1}^nY_t$ suit une loi binomiale dont on précisera les paramètres.
	\item En déduire que pour tout $n\in\N^*$, $\p(S_n=0)=\begin{cases} \displaystyle\binom{n}{\frac{n}{2}}(p(1-p))^{\frac{n}{2}} &\mbox{si }n\mbox{ est pair}\\
		0 & \mbox{sinon}
	\end{cases}$ 
	\item Déterminer la limite de la suite $(\p(S_{2n}=0))_{n\in \N}$ selon les valeurs de $p$ et interpréter le résultat.
\end{enumerate}
On souhaite maintenant savoir combien de fois en moyenne la puce passe par l'origine lors de son parcours. Pour cela, on note pour tout $j\in\N, \ O_{2j}$ la variable aléatoire égale à 1 si la puce est à l'origine à l'instant $t=2j$, 0 sinon. Pour tout $n\in\N$, on pose $T_n=\displaystyle\sum\limits_ {j=0}^nO_{2j}$.
\begin{enumerate}[resume]
	\item Soit $j\in\N$. Déterminer la loi de la variable aléatoire $O_{2j}$. En déduire que: $$\forall n\in \N,\ \E(T_n)=\displaystyle\sum\limits_{j=0}^n \binom{2j}{j}(p(1-p))^j$$
	\item On suppose dans cette question que $p\ne\displaystyle\frac{1}{2}$. Calculer $\lim\limits_{n\to+\infty}\E(T_n)$ et interpréter ce résultat.
	\item On suppose dans cette question que $p=\displaystyle\frac{1}{2}$. Montrer par récurrence que: $$\forall n\in\N,\ \E(T_n)=\displaystyle\frac{2n+1}{2^{2n}}\binom{2n}{n}$$
	En déduire $\lim\limits_{n\to+\infty}\E(T_n)$ et interpréter.
\end{enumerate}

\subsection{Le cas $d=2$}


\subsection{Le cas général}
\textit{\underline{Remarque :} Une marche alétoire sera dite récurrente si et seulement si la propabilité que la particule repasse à l'origine $O$ pour un certain instant t ultérieur fini vaut $1$.
	Cette propriété de récurrence dépend fortement de la dimension de l'espace, le théorème de Polya énonce que : la marche aléatoire est récurrente si et seulement si $d=1$ ou $d=2$.}


\section{Matrice de covariances \telecom{2}}
\label{Matrice de covariances}
\textcolor{blue}{\hyperref[Matrice de covariance corrigé]{[Corrigé]}}\\
Soient $X_1,\dots,X_n$ des variables aléatoires discrètes réelles de $L^2$. On appelle \textit{matrice de covariance} du vecteur aléatoire $X=\begin{pmatrix} X_1\\ \vdots\\ X_n \end{pmatrix}$ la matrice: $$\Sigma=(\operatorname{Cov}(X_i,X_j))_{1\leq i,j\leq n}$$
\begin{enumerate}
	\item Soit $A=\begin{pmatrix} a_1\\ \vdots\\ a_n \end{pmatrix}\in \M_{n,1}(\K)$.
	\\Exprimer la variance de $A^\top X$ en fonction de la matrice $\Sigma$ et de $A$.
	\item Montrer que $\Sigma$ est diagonalisable et que ses valeurs propres sont toutes positives
	\item Déterminer une condition nécessaire et suffisante pour que $\Sigma$ soit inversible.
	\\ \textit{\underline{Remarque}: On ne manquera pas de voir la ressemblance entre matrice de covariances et matrice de Gram, la covariance étant quasi assimilable à un produit scalaire.}
\end{enumerate}

\section{Maximisation de la variance sous contrainte \ccinp{2}}
\label{Maximisation de la variance sous contrainte}
\textcolor{blue}{\hyperref[Maximisation de la variance sous contrainte corrigé]{[Corrigé]}}\\
On considère $n$ ampoules s'allumant aléatoirement de manière indépendante, la probabilité que la $i$-ème s'allume étant de $p_i$. On note $Y$ la variable aléatoire donnant le nombre d'ampoules qui sont allumées.
\begin{enumerate}
	\item Donner l'espérance et la variance de $Y$.
	\item On note $m=\mathbb{E}(Y)$. Déterminer, à m fixé, les $p_i$ tels que $\mathbb{V}(Y)$ soit maximale.
	\\ Quelle loi suit $Y$ pour de tels $p_i$ ?
\end{enumerate}

\section{Inégalité de Kosmanek \ccinp{1}}
\label{Inégalité de Kosmanek}
\textcolor{blue}{\hyperref[Inégalité de Kosmanek corrigé]{[Corrigé]}}\\
Soit $(\Omega,\mathcal P(\Omega),\p)$ un espace probabilisé fini.
\begin{enumerate}
	\item Soit $C$ un événement. Montrer que $\displaystyle\V(\mathbbm 1_C)\leq \frac{1}{4}$.
	\item Soient $A$ et $B$ deux événements. Montrer que $\displaystyle|\operatorname{Cov}(\mathbbm 1_A,\mathbbm 1_B)|\leq \frac{1}{4}$.
	\item En déduire que $|\p(A\cap B)-\p(A)\p(B)|\leq\displaystyle\frac{1}{4}$.
	\\Quand a-t-on égalité ?
\end{enumerate}

\section{\underline{Inégalité de Cantelli} \centraleponts{3}}
\label{Inégalité de Cantelli}
\textcolor{blue}{\hyperref[Inégalité de Cantelli corrigé]{[Corrigé]}}\\
Soient $\lambda\in\R^*_+$ et $X$ une variable aléatoire réelle discrète possédant un moment d'ordre 2.
\begin{enumerate}
	\item On suppose que $\E(X)=0$.
	\begin{enumerate}
		\item Montrer que pour tout $u\in\R_+,\ \E((X+u)^2)=\V(X)+u^2$.
		\item Montrer que pour tout $u\in\R_+,\ \p(X\geq \lambda)\leq \displaystyle\frac{\V(X)+u^2}{(\lambda+u)^2}$.
		\item En déduire que $\p(X\geq \lambda)\leq\displaystyle\frac{\V(X)}{\lambda^2+\V(X)}$.
	\end{enumerate}
	\item On ne suppose plus maintenant $\E(X)=0$. Montrer que $\p(X-\E(X)\geq \lambda)\leq \displaystyle\frac{\V(X)}{\lambda^2+\V(X)}$.
\end{enumerate}

\section{\underline{Inégalité de Hoeffding} \centraleponts{3}}
\label{Inégalité de Hoeffding}
\textcolor{blue}{\hyperref[Inégalité de Hoeffding corrigé]{[Corrigé]}}\\
On considère une variable aléatoire discrète $X$ centrée et à valeurs dans $[-1,1]$.
\begin{enumerate}
	\item Montrer que $$\forall t\in\R,\forall x\in[-1,1],\ e^{tx}\leq \frac{1}{2}(1-x)e^{-t}+\frac{1}{2}(1+x)e^t$$
	\item Montrer que $$\forall t\in\R, \operatorname{ch}(t)\leq e^{\frac{t^2}{2}}$$
	\item En déduire que $$\forall t\in\R, \E(e^{tX})\leq e^{\frac{t^2}{2}}$$
\end{enumerate}
On considère une variable aléatoire réelle discrète $Y$.
\begin{enumerate}[resume]
	\item Montrer que $$\forall t\in \R^*_+, \forall \varepsilon\in\R^*_+, \p(Y\geq\varepsilon)\leq e^{-t \varepsilon}\E(e^{tY})$$
\end{enumerate}
On considère maintenant des variables aléatoires discrètes réelles centrées $X_1,\dots, X_n$ indépendantes telles que $|X_k|\leq c_k$ pour tout $k\in\crblanc{1}{n} \ (c_k>0)$. On pose $S=\displaystyle\sum\limits_{k=1}^n X_k$
\begin{enumerate}[resume]
	\item Montrer que pour tout $\varepsilon\in\R^*_+$, $$\p(|S|\geq\varepsilon)\leq 2\exp\left(-\frac{\varepsilon^2}{2\displaystyle\sum\limits_{k=1}^nc_k^2}\right)$$
\end{enumerate}

\section{\underline{Théorème d'approximation de Weierstrass} \centraleponts{3}}
\label{Théorème d'approximation de Weierstrass}
\textcolor{blue}{\hyperref[Théorème d'approximation de Weierstrass corrigé]{[Corrigé]}}\\
Le but de cet exercice est de démontrer le théorème d'approximation de Weierstrass:\\
Toute fonction continue sur un segment est limite uniforme sur ce segment de fonctions polynomiales.\\\\
Pour tout $n\in \N^*$ on pose pour $k\in \crblanc{0}{n},\ b_k^n(X)=\displaystyle\binom{n}{k}X^k(1-X)^{n-k}$. (Polynômes de Bernstein)\\
On note $I=[0,1]$ et on muni $\mathcal{C}=\mathcal{C}^0(I,\C)$ de sa norme uniforme.\\
On définit alors pour tout $f\in\mathcal{C}$ et tout $n\in \N$,
$$\fonction{B_n(f)}{I}{\C}{x}{\displaystyle\sum\limits_{k=0}^nf\left(\frac{k}{n}\right)b_k^n(x)}$$
Enfin, On fixe $t\in [0,1]$, on considère une suite de variable aléatoire $(X_n)_{n\in \N^*}$ indépendantes et identiquement distribuées de loi $\B(t)$ et on pose pour tout $n\in \N^*,\ S_n=\displaystyle\sum\limits_{k=1}^nX_k$.
\begin{enumerate}
	\item Soit $n\in \N^*$.\\
	Justifier qu'il existe une variable aléatoire discrète $R_n$ telle que $B_n(f)(t)=\E(f(R_n))$.
	\item Montrer que pour un réel $\delta>0$ bien choisit,
	$$\forall n\in \N^*,\ \E(|f(t)-f(R_n)|)\leq \displaystyle\frac{\varepsilon}{2}+2{||f||}_\infty\p(|t-R_n|>\delta)$$
	\item Montrer que $\forall n\in \N^*,\ \p(|t-R_n|>\delta)\leq \displaystyle\frac{1}{4n\delta^2}$.
	\item Démontrer que $(B_n(f))_{n\in \N^*}$ converge uniformément vers $f$ sur $I$.
	\item En déduire le théorème de d'approximation de Weierstrass.
\end{enumerate}

\newpage
\chapter{Endomorphismes d'un espace euclidien}
Dans toute cette section $n$ désigne un entier naturel non nul.\\\\

\section{Equations matricielles \ccinp{2}}
\label{Equations matricielles}
\textcolor{blue}{\hyperref[Equations matricielles corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Existe-t-il deux matrices $A,B\in \M_n(\K)$ telles que $AB-BA=I_n$ ?
	\item Existe-t-il une matrice $M\in \M_2(\R)$ telle que $M^2=\begin{pmatrix}0&1\\1&0\end{pmatrix}$ ?
	\item Existe-t-il une matrice $N\in \M_2(\K)$ telle que $N^2=\begin{pmatrix}0&1\\0&0\end{pmatrix}$ ?
\end{enumerate}

\section{\underline{Equation matricielle faisant intervenir la comatrice} \centraleponts{3}}
\label{Equation matricielle faisant intervenir la comatrice}
\textcolor{blue}{\hyperref[Equation matricielle faisant intervenir la comatrice corrigé]{[Corrigé]}}\\
Résoudre dans $\M_n(\R)$ l'équation $A=\Com(A)$.

\section{\underline{Matrice de rotation} \ccinp{2}}
\label{Matrice de rotation}
\textcolor{blue}{\hyperref[Matrice de rotation corrigé]{[Corrigé]}}\\
Soient $\B$ une base orthonormée d'un espace euclidien $E$ de dimension 3 ainsi que $u\in\mathcal L(E)$ tel que $\operatorname{Mat}_\B(u)=\displaystyle\frac{1}{3}\begin{pmatrix}
	2 & 2 & 1\\
	-2 &1 &2\\
	1& -2& 2
\end{pmatrix}$
\\Montrer que $u$ est une rotation.
\\Question Bonus: Donner son axe et son angle.

\section{Produit mixte et produit vectoriel \centraleponts{2}}
\label{Produit mixte et produit vectoriel}
\textcolor{blue}{\hyperref[Produit mixte et produit vectoriel corrigé]{[Corrigé]}}\\
Soit $E$ un espace euclidien orienté de dimension $n\geq1$.
\begin{enumerate}
	\item Soient $\B$ et $\B'$ deux bases orthonormées directes de $E$. Montrer que $\det_\B(\B')=1$.
	\item En déduire que $\det_\B=\det_{\B'}$.
	\item Soient $x_1,\dots,x_{n-1}\in E^{n-1}$. Montrer que l'application $\fonction{\varphi}{E}{\R}{x}{\det(x_1,\dots,x_{n-1},x)}$ est une forme linéaire sur $E$.
	\item En déduire qu'il existe un unique $u\in E$ tel que $$\forall x\in E, \varphi(x)=\langle x,u\rangle$$
\end{enumerate}
On appelle $u$ le produit vectoriel des vecteurs $x_1,\dots, x_{n-1}$ et on le note $u=x_1\wedge x_2\wedge\dots x_{n-1}$.
\begin{enumerate}[resume]
	\item Montrer que l'application $$(x_1,\dots,x_{n-1})\in E^{n-1}\mapsto x_1\wedge x_2\wedge\dots\wedge x_{n-1}$$ est une application $(n-1)$-linéaire alternée.
\end{enumerate}

\section{Une caractérisation de la trace}
\label{Une caractérisation de la trace}
\textcolor{blue}{\hyperref[Une caractérisation de la trace corrigé]{[Corrigé]}}\\
Soit $\varphi:\M_n(\R)\to\R$ une application linéaire vérifiant
$$\forall A,B\in \M_n(\R),\ \varphi(AB)=\varphi(BA)$$
Montrer que $\varphi$ est proportionnelle à la trace.

\section{\underline{Hyperplan de $\M_n(\K)$} \xens{4}}
\label{Hyperplan de Mn(K)}
\textcolor{blue}{\hyperref[Hyperplan de Mn(K) corrigé]{[Corrigé]}}\\
Montrer que tout hyperplan de $\M_n(\K)$ rencontre GL$_n(\K)$.

\section{\underline{Exemple de symétrie orthogonale} \ccinp{1}}
\label{Exemple de syméttrie orthogonale}
\textcolor{blue}{\hyperref[Exemple de symétrie orthogonale corrigé]{[Corrigé]}}\\
Montrer que $\fonction{s}{\M_n(\R)}{\M_n(\R)}{M}{M^\top}$ est une symétrie orthogonale pour le produit scalaire sur $\M_n(\R)$ pour le produit scalaire défini par $\langle A,B\rangle=\Tr(A^\top B)$ pour $A,B\in\M_n(\R)$.

\section{\underline{Caractérisations des projections orthogonales} \ccinp{2}}
\label{Caractérisations des projections orthogonales}
\textcolor{blue}{\hyperref[Caractérisations des projections orthogonales corrigé]{[Corrigé]}}\\
Soient $E$ un espace euclidien et $p$ une projection de $E$. Etablir l'équivalence des trois propriétés suivantes:
\begin{enumerate}[label=\roman*.]
	\item $p$ est orthogonale
	\item $\forall x,y \in E, \; \langle p(x),y\rangle=\langle x,p(y)\rangle$
	\item $\forall x \in E, \; \norme{p(x)}\leq \norme{x}$ 
\end{enumerate}

\section{Norme d'une base orthogonale \xens{3}}
\label{Norme d'une base orthogonale}
\textcolor{blue}{\hyperref[Norme d'une base orthogonale corrigé]{[Corrigé]}}\\
Soit $(e_1,\dots e_n)$ une base orthogonale de $\R^n$
\begin{enumerate}
	\item Montrer qu'il existe un vecteur $u$ de $\R^n$ non nul tel que les projetés orthogonaux de $e_1,\dots,e_n$ sur $\text{Vect}(u)$ aient la même norme.
	\item Montrer que cette norme commune est indépendante du vecteur $u$ choisi et l'exprimer en fonction de $\norme{e_1},\dots,\norme{e_n}$.
\end{enumerate}

\section{\underline{Matrice de Hilbert} \centraleponts{3}}
\label{Matrice de Hilbert}
\textcolor{blue}{\hyperref[Matrice de Hilbert corrigé]{[Corrigé]}}\\
\begin{enumerate}
	\item Montrer que l'application $\fonction{\proscal{\cdot}{\cdot}}{\R_{n-1}[X]^2}{\R}{(P,Q)}{\displaystyle\int_0^1P(t)Q(t)}$ définie un produit scalaire sur $\R_{n-1}[X]$.
	\item Montrer que la matrice de Hilbert $H=\displaystyle\left(\frac{1}{i+j+1}\right)_{0\leq i,j\leq n-1}=
	\begin{pmatrix}
		1&\displaystyle\frac{1}{2}&\dots&\displaystyle\frac{1}{n}\\
		\displaystyle\frac{1}{2}&\displaystyle\frac{1}{3}&\dots&\displaystyle\frac{1}{n+1}\\
		\vdots&\vdots&\ddots&\vdots\\
		\displaystyle\frac{1}{n}&\displaystyle\frac{1}{n+1}&\dots&\displaystyle\frac{1}{2n+1}
	\end{pmatrix}$
	est symétrique réelle définie positive.\\
\end{enumerate}

\section{\underline{Racine carrée d'un endomorphisme auto-adjoint positif} \telecom{2}}
\label{Racine carrée d'un endomorphisme auto-adjoint positif}
\textcolor{blue}{\hyperref[Racine carrée d'un endomorphisme auto-adjoint positif corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Soit $f$ un endomorphisme auto-adjoint positif d'un espace euclidien $E$. Montrer qu'il existe $g\in\S^+(E)$ tel que $g^2=f$.
	\item Soit $f$ un endomorphisme auto-adjoint défini positif d'un espace euclidien $E$. Montrer qu'il existe un unique $g\in\S^{++}(E)$ tel que $g^2=f$.
\end{enumerate}

\section{\underline{Inégalité de convexité} \telecom{2}}
\label{Inégalité de convexité}
\textcolor{blue}{\hyperref[Inégalité de convexité corrigé]{[Corrigé]}}\\
Soit $M\in\S^+_n(\R)$.\\
Démontrer que $\displaystyle\frac{\Tr(M)}{n}\geq \det(M)^{1/n}$.

\section{\underline{Inégalité à propos des matrices orthogonales} \telecom{3}}
\label{Inégalité à propos des matrices orthogonales}
\textcolor{blue}{\hyperref[Inégalité à propos des matrices orthogonales corrigé]{[Corrigé]}}\\
Soit $A\in \O_n(\R)$. Montrer que\\
$$\left|\sum_{1\leq i,j\leq n}m_{i,j}\right|\leq n \leq\sum_{1\leq i,j\leq n}|m_{i,j}|\leq n^{3/2}$$

\section{Inégalité de la trace}
\label{Inégalité de la trace}
\textcolor{blue}{\hyperref[Inégalité de la trace corrigé]{[Corrigé]}}\\
Soient $A,B\in\S_n^+(\R)$. Montrer que $0\leq\Tr(AB)\leq\Tr(A)\Tr(B)$.

\section{\underline{Inégalité de Hadamard} \centraleponts{3}}
\label{Inégalité de Hadamard}
\textcolor{blue}{\hyperref[Inégalité de Hadamard corrigé]{[Corrigé]}}\\
Soit $S=(s_{ij})\in\mathcal S_n^+(\R)$.
On pourra se servir librement de l'inégalité arithmético-géométrique cf. I-47 tome Analyse.
\begin{enumerate}
	\item Montrer que $\det(S)\leq \left(\displaystyle\frac{1}{n}\Tr(S)\right)^n$.
	\item Soient $\alpha=(\alpha_1,\dots,\alpha_n)\in\R^n$, $D=\text{diag}(\alpha_1,\dots,\alpha_n)$ et $S_{\alpha}=DSD$.\\
	Vérifier que $S_{\alpha}\in\S_n^+(\R)$ et exprimer $\Tr(S_{\alpha})$.
	\item On suppose $S$ inversible. Vérifier que les coefficients diagonaux de $S$ sont strictement positifs et, en introduisant des réels $\alpha_1,\dots,\alpha_n$ pertinemment choisis, établir que: $$\det(S)\leq \prod_{i=1}^n s_{ii}$$
	\item Justifier que l'inégalité précédente est encore vraie lorsque $S$ n'est pas inversible.
\end{enumerate}

\section{Perturbations}
\label{Perturbations}
\textcolor{blue}{\hyperref[Perturbations corrigé]{[Corrigé]}}\\
Soit $A\in \M_n(\R)$ la matrice Atilla $A=\begin{pmatrix}
	1&\cdots&\cdots&1\\
	\vdots&\ddots&\adots&\vdots\\
	\vdots&\adots&\ddots&\vdots\\
	1&\cdots&\cdots&1
\end{pmatrix}$.\\
Quelles sont les matrices $M\in \O_n(\R)$ telles que $A+M$ soit inversible ?

\section{Somme de Cesàro de matrices orthogonales}
\label{Somme de Cesàro de matrices orthogonales}
\textcolor{blue}{\hyperref[Somme de Cesaro de mattrices orthogonales corrigé]{[Corrigé]}}\\
Soit $A\in \O_n(\R)$ telle que $-1\notin \text{Sp}(A)$.
\begin{enumerate}
	\item Montrer que la suite $\left(\displaystyle\frac{1}{p+1}\sum_{k=0}^pA^k\right)_{p\in \N}$ converge et déterminer sa limite.
	\item La suite $(A^p)_{p\in \N}$ converge-t-elle ?
\end{enumerate}

\section{\underline{Transformation de Cayley} \centraleponts{3}}
\label{Transformation de Cayley}
\textcolor{blue}{\hyperref[Transformation de Cayley corrigé]{[Corrigé]}}\\
Soit $A\in \A_n(\R)$.
\begin{enumerate}
	\item Déterminer les valeurs propres possibles de la matrice $A$.
	\item En déduire que les matrices $A+I_n$ et $A-I_n$ sont inversibles.
	\item Montrer que la matrice $\Omega=(A+I_n)(A-I_n)^{-1}$ est orthogonale. Et calculer $\det(\Omega)$
	\item Démontrer que l'application $\fonction{\varphi}{\A_n(\R)}{\{\Omega\in \O_n(\R)|-1\notin \text{Sp}(\Omega)\}}{A}{(A+I_n)(A-I_n)^{-1}}$ est bijective.
	\item En déduire que si $\Omega$ est une matrice orthogonale et que $-1\notin \text{Sp}(\Omega)$ alors $\det(\Omega)=1$. Pouvait-t-on constater cela d'une manière plus directe ?
\end{enumerate}

\section{Optimisation (1) \telecom{2}}
\label{Optimisation 1}
\textcolor{blue}{\hyperref[Optimisation 1 corrigé]{[Corrigé]}}\\
Calculer le minimum de $\fonction{\Phi}{\R^2}{\R}{(a,b)}{\displaystyle\int_0^{\pi}\left(\sin(x)-ax-bx^2\right)^2dx}$.

\section{Optimisation (2) \telecom{2}}
\label{Optimisation 2}
\textcolor{blue}{\hyperref[Optimisation 2 corrigé]{[Corrigé]}}\\
Déterminer $\displaystyle\inf_{(a,b)\in \R^2}\int_{-\infty}^{+\infty}(t^2+at+b)^2e^{-t^2}dt$.

\section{Optimisation (3)}
\label{Optimisation 3}
\textcolor{blue}{\hyperref[Optimisation 3 corrigé]{[Corrigé]}}\\
Soient $E$ un espace euclidien et $x_1,\dots,x_p$ des vecteurs de $E$. Pour $x\in E$, on pose $f(x)=\displaystyle\sum\limits_{i=1}^p\norme{x-x_i}^2$. Montrer que $f$ atteint son minimum en $m=\displaystyle\frac{1}{p}\sum\limits_{i=1}^px_i$.

\section{Caractérisation des isométries anti-involutives \telecom{2}}
\label{Caractérisation des isométries anti-involutives}
\textcolor{blue}{\hyperref[Caractérisation des isométries anti-involutives corrigé]{[Corrigé]}}\\
Soient $E$ un espace euclidien et $f$ un endomorphisme de $E$. Montrer que deux des trois propriétés suivantes entraînent la troisième:
\begin{itemize}
	\item $f$ est une isométrie vectorielle.
	\item $f^2=-\Id_E$
	\item $f(x)$ est orthogonal à $x$ pour tout $x\in E$.
\end{itemize}

\section{Symétrie de l'espace \centraleponts{3}}
\label{Symétrie de l'espace}
\textcolor{blue}{\hyperref[Symétrie de l'espace corrigé]{[Corrigé]}}\\
Soient $a\in\R^*$, $u$ un vecteur unitaire de $\R^3$ munit de sa structure euclidienne usuelle.
\begin{enumerate}
	\item Montrer que l'application $f_a$ définie par: $f_a(x)=x+a\langle x,u\rangle u$ est un endomorphisme.
	\item Montrer qu'il existe un unique $a'\ne 0$ vérifiant: $\forall x\in\R^3, \norme{f_{a'}(x)}=\norme{x}$.\\ Donner la nature de $f_{a'}$ (on pourra s'intéresser à $f_{a'}^2$).
	\item Montrer que $f_a$ est un endomorphisme auto-adjoint et déterminer ses éléments propres.
\end{enumerate}

\section{Réflexion et rotation dans un plan \centraleponts{3}}
\label{Réflexion et rotation dans un plan}
\textcolor{blue}{\hyperref[Réflexion et rotation dans un plan corrigé]{[Corrigé]}}\\
Soit $E$ un plan orienté.
\begin{enumerate}
	\item \begin{enumerate}[label=\alph*.]
		\item Montrer que la composée de deux réflexion du plan $E$ est une rotation.
		\item Justifier que toute rotation peut s'écrire comme le produit de deux réflexion dont l'une peut être choisie arbitrairement.
	\end{enumerate}
	\item Soient $r$ une rotation et $s$ une réflexion de $E$.
	\\ Simplifier les composées $s\circ r\circ s$ et $r\circ s\circ r$.
	\item A quelle condition peut-on affirmer qu'une rotation et qu'une réflexion commutent.
\end{enumerate}

\section{Similitude entre matrices orthogonales \centraleponts{3}}
\label{Similitude entre matrices orthogonales}
\textcolor{blue}{\hyperref[Similitude entre matrices orthogonales corrigé]{[Corrigé]}}\\
Soit $(A,B)\in \O_n(\R)^2$. 
\\ Montrer que $A$ et $B$ sont semblables si et seulement $\chi_A=\chi_B$.

\section{\underline{Propriété de l'adjoint} \ccinp{2}}
\label{Propriété de l'adjoint}
\textcolor{blue}{\hyperref[Propriété de l'adjoint corrigé]{[Corrigé]}}\\
Soit $f$ un endomorphisme d'un espace euclidien $E$.
Montrer que:
\begin{enumerate}
	\item $\Tr(f)=\Tr(f^*)$
	\item $\det(f)=\det(f^*)$
	\item $\chi_f=\chi_{f^*}$
	\item $\text{Sp}(f)=\text{Sp}(f^*)$
	\item pour tout $\lambda\in\text{Sp}(f),\dim E_{\lambda}(f)=\dim E_{\lambda}(f^*)$
	\item $\text{rg}(f)=\text{rg}(f^*)=\text{rg}(f\circ f^*)=\text{rg}(f^*\circ f)$.
\end{enumerate}

\section{Autour de l'adjoint \centraleponts{3}}
\label{Autour de l'adjoint}
\textcolor{blue}{\hyperref[Autour de l'adjoint corrigé]{[Corrigé]}}\\
Soient $E$ un espace euclidien et $f$ un endomorphisme de $E$ tel que $\text{Im}(u)\subset \text{Ker}(u)$.
\begin{enumerate}
	\item Montrer que $\text{Ker}(f+f^*)=\text{Ker}(f)\cap\text{Ker}(f^*)$.
	\item Supposons que $\text{Ker}(u)=\text{Im}(u)$. Montrer que $u+u^*$ est inversible.
\end{enumerate}

\section{Somme d'une matrice orthogonale et de sa transposée \centraleponts{3}}
\label{Somme d'une matrice orthogonale et de sa transposée}
\textcolor{blue}{\hyperref[Somme d'une matrice orthogonale et de sa transposée corrigé]{[Corrigé]}}\\
Soit $A\in\M_n(\R)$. Démontrer que les propriétés suivantes sont équivalentes :
\begin{itemize}
	\item Il existe $B\in\O_n(\R)$ telle que $A=B+B^\top$,
	\item $A\in\mathcal{S}_n(\R),\ \operatorname{Sp}(A)\subset[-2,2]$ et les valeurs propres de $A$ dans $]-2,2[$ sont de multiplicité paire.
\end{itemize}

\section{\underline{Déterminant d'une exponentielle} \ccinp{1}}
\label{Déterminant d'une exponentielle}
\textcolor{blue}{\hyperref[Déterminant d'une exponentielle corrigé]{[Corrigé]}}\\
Soit $A\in\M_n(\K)$.
\\ Montrer que $\det(e^A)=e^{\Tr(A)}$.

\section{Exponentielle de matrices antisymétriques \xens{3}}
\label{Exponentielle de matrices antisymétriques}
\textcolor{blue}{\hyperref[Exponentielle de matrices antisymétriques corrigé]{[Corrigé]}}\\
Montrer que $\exp:\A_n(\R)\to \O_n(\R)$ est une application surjective. 
\\
\textit{\underline{Remarque:} La décomposition polaire sur $\C$ est donnée par l'application $\fonction{\varphi}{\R^*_+\times\R}{\C^*}{(r,\theta)}{re^{i\theta}}$. $\exp:\A_n(\R)\to \O_n(\R)$ permet de comprendre d'où vient le nom de décomposition polaire pour les matrice inversible.}

\section{Théorème spectral \etoile{3}}
\label{Théorème spectral}
\textcolor{blue}{\hyperref[Théorème spectral corrigé]{[Corrigé]}}\\
Enoncer et démontrer le théorème spectral.

\section{Réduction simultanée \xens{4}}
\label{Réduction simultanée}
\textcolor{blue}{\hyperref[Réduction simultanée corrigé]{[Corrigé]}}\\
Soient $A\in \mathcal S^{++}_n(\R)$ et $B\in\mathcal{S}^+_n(\R)$
\begin{enumerate}
	\item Montrer qu'il existe $P\in \text{GL}_n(\R)$ et $D\in\M_n(\R)$ diagonale à coefficients diagonaux strictement positifs  telles que: $$A=P^\top P \quad \text{et} \quad B=P^\top DP$$
	\item Montrer que le polynôme $x\mapsto \det(xA-B)$ est scindé sur $\R$
	\item Montrer que $\det(A+B)\geq \det(A)+\det(B)$
	\item Montrer que $\forall t\in]0,1[, \det(A)^t\det(B)^{1-t}\leq \det(tA+(1-t)B)$
\end{enumerate}

\section{Matrices antisymétriques réelles}
On fixe dans les exercices qui suivent une matrice $A\in \A_n(\R)$.

\subsection{Rang et déterminant}
\label{Rang et déterminant}
\textcolor{blue}{\hyperref[Rang et déterminant corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Montrer que si $A$ est de taille impaire alors elle n'est pas inversible.
	\item Montrer que $\T{Sp}_\R(A)\subset\{0\}$.
	\item En déduire que le rang de $A$ est pair et que $\det A\geq 0$.
\end{enumerate}

\subsection{Spectre}
\label{Spectre}
\textcolor{blue}{\hyperref[Spectre corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Montrer que $\T{Sp}_\R(A)\subset\{0\}$.
	\item Exprimer $\T{Sp}_\C(A^2)$ en fonction de $\T{Sp}_\C(A)$.
	\item En déduire que $\T{Sp}_\C(A)\subset i\R$.
\end{enumerate}

\subsection{Diagonalisabilité (1)}
\label{Diagonalisabilité (1)}
\textcolor{blue}{\hyperref[Diagonalisabilité (1) corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Montrer que $A^2$ est diagonalisable.
	\item Montrer que $\Ker A^2=\Ker A$.
	\item En déduire que $A$ est diagonalisable dans $\M_n(\C)$.
\end{enumerate}

\subsection{Diagonalisabilité (2)}
\label{Diagonalisabilité (2)}
\textcolor{blue}{\hyperref[Diagonalisabilité (2) corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Montrer que $\Ker A=(\Ima A)^\bot$.
	\item En déduire qu'il existe $O\in \O_n(\R)$ et $B\in \A_n(\R)$ inversible telle que
	$$A=O\begin{pmatrix}
		0&0\\
		0&B
	\end{pmatrix}O^\top$$
	\item En déduire que le rang de $A$ est pair.
	\item Montrer que $A$ est diagonalisable dans $\M_n(\C)$.
\end{enumerate}

\subsection{Réduction}
\label{Réduction}
\textcolor{blue}{\hyperref[Réduction corrigé]{[Corrigé]}}\\
Montrer que, par le biais d'une matrice de passage orthogonale, $A$ est semblable à une matrice diagonale par blocs avec sur sa diagonale des zéros et/ou différents blocs $M_\alpha=\begin{pmatrix}
	0&-\alpha\\
	\alpha&0
\end{pmatrix}$ où $\alpha\in \R^*_+$.

\section{Endomorphismes normaux}
\label{Endomorphismes normaux}
\textcolor{blue}{\hyperref[Endomorphismes normaux corrigé]{[Corrigé]}}\\
Soit $u$ un endomorphisme d'un espace euclidien $E$. On suppose que $u$ est un endomorphisme \textit{normal}, c'est-à-dire que $u^*\circ u=u\circ u^*$.
\begin{enumerate}
	\item Montrer que $u$ et $u^*$ ont les mêmes éléments propres.
	\item Montrer que les sous-espaces propres de $u$ sont deux à deux orthogonaux.
\end{enumerate}

\section{Réduction des normaux}
\label{Réduction des normaux}
\textcolor{blue}{\hyperref[Réduction des normaux corrigé]{[Corrigé]}}\\
Soit $u$ un endomorphisme d'un espace euclidien $E$. On suppose que $u$ est un endomorphisme \textit{normal}, c'est-à-dire que $u^*\circ u=u\circ u^*$.
\begin{enumerate}
	\item On suppose dans cette question uniquement $\dim E=2$ et $\chi_u$ irréductible. Soit $\B$ une base orthonormée de $E$. Montrer que la matrice de $u$ dans la base $\B$ est de la forme $\begin{pmatrix}
		a&-b\\
		b&a
	\end{pmatrix}$ avec $(a,b)\in \R\times \R^*$.
	\item Soit $F$ un sous-espace vectoriel de $E$ stable par $u$. Montrer que $F^\bot$ est également stable par $u$ puis que les endomorphismes induits $u_F$ et $u_{F^\bot}$ sont des endomorphismes normaux de $F$ et $F^\bot$ respectivement.
	\item Déduire de ce qui précède qu'il existe une base orthonormée de $E$ dans laquelle la matrice de $u$ est diagonale par blocs avec des blocs diagonaux de la forme $\begin{pmatrix}\lambda\end{pmatrix}$ avec $\lambda\in \R$ ou $\begin{pmatrix}
		a&-b\\
		b&a
	\end{pmatrix}$ avec $(a,b)\in \R\times \R^*$.
	\item Que peut-t-on dire si $u$ est autoadjoint ? anti-autoadjoint ? orthogonal ?
\end{enumerate}

\section{Caractérisation des matrices symétriques positives \centraleponts{3}}
\label{Caractérisation des matrices symétriques positives}
\textcolor{blue}{\hyperref[Caractérisation des matrices symétriques positives corrigé]{[Corrigé]}}\\
On munit $\M_n(\R)$ muni de son produit scalaire canonique. On note $\A_n(\R)$ l'ensemble des matrices antisymétriques et $\S_n^+(\R)$ l'ensemble des matrices symétriques positives.
\\ Soit $A\in\M_n(\R)$: $$\forall U\in\O_n(\R), \quad \Tr(AU)\leq\Tr(A)$$
\begin{enumerate}
	\item Déterminer le supplémentaire orthogonal de $\A_n(\R)$.
	\item Soit $B\in\A_n(\R)$. Montrer que $\forall x\in\R,\quad\exp(xB)\in\O_n(\R)$.
	\item Montrer que $A\in\S_n^+(\R)$.
	\item Etudier la réciproque.
\end{enumerate}

\section{Matrices entières positives}
\label{Matrices entières positives}
\textcolor{blue}{\hyperref[Matrices entières positives corrigé]{[Corrigé]}}\\
Soient $E$ un ensemble fini et $A_1,\dots,A_n$ des parties de $E$. Montrer que $A=(|A_i\cap A_j|)_{1\leq i,j\leq n}$ est symétrique réelle définie positive.

\section{Matrices binaires positives}
\label{Matrices binaires positives}
\textcolor{blue}{\hyperref[Matrices binaires positives corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Déterminer les matrices de $\mathcal S_n^{++}(\R)$ à coefficients dans $\{0,1\}$.
	\item Déterminer les matrices de $\mathcal S_n^+(\R)$ à coefficients dans $\{0,1\}$ et les dénombrer.
\end{enumerate}

\section{\underline{Inégalité de Hoffman-Wielandt} \centraleponts{3}}
\label{Inégalité de Hoffman-Wielandt}

\textcolor{blue}{\hyperref[Inégalité de Hoffman-Wielandt corrigé]{[Corrigé]}}\\
On munit $\M_n(\R)$ de la norme euclidienne canonique $\norme{.}_F$.
\\Pour toute matrice symétrique réelle $M\in S_n(\R)$, on note $\lambda_1(M)\geq\dots\geq\lambda_n(M)$ ses valeurs propres rangées dans l'ordre décroissant.
\\Soient $A$ et $B$ deux matrices de $S_n(\R)$.
\begin{enumerate}
	\item Montrer que, pour tout $M\in\M_n(\R)$ et pour tout $P$ et $Q$ dans $\O_n(\R)$, on a: $\norme{PMQ}_F=\norme{M}_F$.
	\item On note $D_A=\text{diag}(\lambda_1(A),\dots,\lambda_n(A))$ et $D_B=\text{diag}(\lambda_1(B),\dots,\lambda_n(B))$. Montrer qu'il existe une matrice orthogonale $P=(p_{i,j})$ telle que $\norme{A-B}_F=\norme{D_AP-PD_B}_F$.
	\item Montrer que $\norme{A-B}_F^2=\displaystyle\sum\limits_{1\leq i,j\leq n}p_{i,j}^2(\lambda_i(A)-\lambda_j(B))^2$
\end{enumerate}
On note $\B_n(\R)$ l'ensemble des matrices bistochastiques de $\M_n(\R)$.
\\On pose $\fonction{f}{\M_n(\R)}{\R}{M}{\displaystyle\sum\limits_{1\leq i,j\leq n }m_{i,j}(\lambda_i(A)-\lambda_j(B))^2}$.
\begin{enumerate}[resume]
	\item Justifier que $f$ admet un minimum sur $\B_n(\R)$.
\end{enumerate}
On se propose de montrer que ce minimum est atteint en la matrice identité.
\begin{enumerate}[resume]
	\item Soit $(i,j,k)\in\crblanc{1}{n}^3$ tel que $j\geq i$ et $k\geq i$. Montrer que, pour $M\in\M_n(\R)$ et pour $x\in\R^+$, $$f(M+xE_{i,i}+xE_{j,k}-xE_{i,k}-xE_{j,k})-f(M)=2x(\lambda_i(A)-\lambda_j(A))(\lambda_k(B)-\lambda_i(B))\leq 0$$
	\item Soient $n\geq 2$ et $M=(m_{i,j})\in\B_n(\R)$ une matrice différente de l'identité. On note $i_0$ le plus petit entier appartenant à $\crblanc{1}{n}$ tel que $m_{i_0,i_0}\ne1$. Montrer qu'il existe une matrice $M'=(m_{i,j}')\in\B_n(\R)$ telle que $f(M')\leq f(M)$ et $m'_{j,j}=1$ pour tout $j\in\crblanc{1}{i_0}$.
	\item En déduire que $\min\{f(M)|M\in\B_n(\R)\}=f(I_n)$.
	\item En déduire que $\forall (A,B)\in\mathcal{S}_n(\R)^2, \displaystyle\sum\limits_{i=1}^n(\lambda_i(A)-\lambda_i(B))^2\leq\norme{A-B}_F^2$.
\end{enumerate}

\section{Distance aux matrices de rang au plus $r$}
\label{Distance aux matrices de rang au plus r}
\textcolor{blue}{\hyperref[Distance aux matrices de rang au plus r corrigé]{[Corrigé]}}\\
On munit $\M_n(\R)$ de sa structure euclidienne canonique. Pour tout $r\in \crblanc{0}{n}$ on note $B_r$ l'ensemble des matrices de $\M_n(\R)$ de rang au plus $r$.
\begin{enumerate}
	\item Soit $A\in \mathcal S_n(\R)$. Montrer que la distance entre $A$ et $B_r$ est atteinte, et la calculer en fonction du spectre de $A$.
	\item Soit $A\in \M_n(\R)$. Montrer que la distance entre $A$ et $B_r$ est atteinte, et la calculer en fonction du spectre de $A$.
\end{enumerate}

\section{\underline{Théorème de Courant-Fischer} \centraleponts{3}}
\label{Théorème de Courant-Fischer}
\textcolor{blue}{\hyperref[Théorème de Courant-Fischer corrigé]{[Corrigé]}}\\
Soit $u$ un endomorphisme autoadjoint d'un espace euclidien $E$ de dimension $n\geq1$ dont le produit scalaire est noté $\langle.,.\rangle$.\\ On note $\lambda_1\geq \lambda_2\geq\dots\geq\lambda_n$ les valeurs propres de $u$ comptées avec multiplicité, $S$ la sphère unité de $E$ et, pour tout $p\in\crblanc{1}{n}$, $\F_p$ l'ensemble de tous les sous-espaces vectoriels de $E$ de dimension $p$. \\Soit $p\in\crblanc{1}{n}$. Etablir que : $$\lambda_p=\sup\limits_{F\in\F_p}\left(\inf\limits_{x\in F\cap S}\langle u(x),x\rangle\right)=\inf\limits_{F\in\F_{n+1-p}}\left(\sup\limits_{x\in F\cap S}\langle u(x),x\rangle\right)$$

\section{Principe de Ky-Fan}
\label{Principe de Ky-Fan}
\textcolor{blue}{\hyperref[Principe de Ky-Fan corrigé]{[Corrigé]}}\\
Soient $E$ un espace euclidien de dimension $n$ et $u\in \mathcal S(E)$. Si $V$ est un sous-espace vectoriel de $E$, on note $u_V:V\to V$ l'endomorphisme obtenu comme composition de $u$ avec la projection orthogonale sur $V$. On note $\lambda_1\geq \lambda_2\geq\cdots\geq\lambda_n$ les valeurs propres de $u$ comptées avec multiplicité et pour tout $p\in \crblanc{1}{n},\ \F_p$ l'ensemble de tous les sous-espaces vectoriels de $E$ de dimension $p$.\\
Etablir que : $$\forall p\in \crblanc{1}{n},\ \sum_{i=1}^p\lambda_i=\max\limits_{V\in \F_p}\Tr(u_V)$$

\section{Théorème de Cartan-Dieudonnée \xens{4}}
\label{Théorème de Cartan-Dieudonnée}
\textcolor{blue}{\hyperref[Théorème de Cartan-Dieudonnée corrigé]{[Corrigé]}}\\
Soient $E$ un espace vectoriel euclidien de dimension $n$ et $f\in \O_n(E)$.
Notons $F=\ker(f-\Id_E)$ et $p(f)=n-\dim(F)$.
\\Montrer que $f$ s'écrit comme produit de $p(f)$ réflexions, et qu'on ne peut pas faire moins.
\\Pour démontrer ce résultat, on pourra procéder par récurrence forte sur $p(f)$.
\\Par convention, Id$_E$ est le produit de $0$ réflexions.
\\\underline{Remarque:} L'entier $p(f)$ est appelée \textit{codimension} de l'espace des invariants de $f$.

\section{Relation d'ordre des matrices symétriques}
\label{Relation d'ordre des matrices symétriques}
\textcolor{blue}{\hyperref[Relation d'ordre des matrices symétriques corrigé]{[Corrigé]}}\\
Soient $A$ et $B$ dans $\mathcal{S}_n(\R).$ On dit que $A\leq B$ lorsque $B-A\in\mathcal{S}_n^+(\R)$
\begin{enumerate}
	\item Vérifier qu'il s'agit bien d'une relation d'ordre. La relation d'ordre est-elle totale ?
	\item Soient $E=\mathcal{S}_n^{++}(\R)$ et $\fonction{f}{E}{E}{A}{A^{-1}}$
	\\ Montrer que $f$ est décroissante.
\end{enumerate}

\newpage
\chapter{Décomposition matricielle}
Dans toute cette section $n$ désigne un entier supérieur ou égal à 1.\\
\section{\underline{Décomposition de Dunford} \centraleponts{3}}
\label{Décomposition de Dunford}
\textcolor{blue}{\hyperref[Décomposition de Dunford corrigé]{[Corrigé]}}\\
Soit $M\in \M_n(\C)$. On note pour $\lambda\in \text{Sp}(M),\ F_\lambda=\Ker(M-\lambda I_n)^{m_\lambda}$, où $m_\lambda$ désigne la multiplicité de $\lambda$ en tant que valeur propre de $M$, le sous-espace caractéristique de $M$ associée à la valeur propre $\lambda$.
\begin{enumerate}
	\item Montrer que $\C^n=\displaystyle\bigoplus_{\lambda\in \text{Sp}(M)}F_\lambda$.
	\item Montrer qu'il existe un couple $(D,N)\in \M_n(\C)^2$ tel que
	\begin{enumerate}[label=(\roman*)]
		\item $M=D+N$;
		\item $D$ est diagonalisable;
		\item $N$ est nilpotente;
		\item $DN=ND$.
	\end{enumerate}
	\item On admet qu'un tel couple vérifie $(D,N)\in \C[M]^2$.\\
	Montrer que ce couple est unique.
\end{enumerate}
\textit{On admettra pour les applications qui suivent, si nécessaire, que $D$ et $N$ sont des polynômes en $M$.}

\subsection{Diagonalisabilité de l'exponentielle d'une matrice \centraleponts{3}}
Soit $A\in \M_n(\C)$. Montrer que $\exp(A)$ est diagonalisable si et seulement si $A$ l'est.

\subsection{Surjectivité de l'exponentielle matricielle \centraleponts{4}}
\begin{enumerate}[leftmargin=*]
	\item Soit $M\in \text{GL}_n(\C)$.\\
	Démontrer qu'il existe un unique couple $(D,U)\in \M_n(\C)^2$ tel que :
	\begin{itemize}
		\item $M=DU$;
		\item $D$ est diagonalisable;
		\item $U$ est unipotente, c'est à dire que $U-I_n$ est nilpotente;
		\item $DU=UD$.
	\end{itemize}
	\item Montrer $\exp: \M_n(\C)\to \text{GL}_n(\C)$ est surjective.
\end{enumerate}

\subsection{Opérateur de commutation \centraleponts{3}}
Soit $A\in \M_n(\C)$.\\
On pose $\fonction{\operatorname{Comm}_A}{\M_n(\C)}{\M_n(\C)}{B}{AB-BA}$.
\begin{enumerate}
	\item On suppose que $A$ est diagonalisable et on note $A=PDP^{-1}$ avec $D=\diag(\lambda_1,\dots,\lambda_n)$.\\
	Montrer que $\operatorname{Comm}_A$ est diagonalisable.
	\item On suppose que $\operatorname{Comm}_A$ est diagonalisable. Déterminer la décomposition de Dunford de $\operatorname{Comm}_A$ en fonction de celle de $A$ et en déduire que $A$ est diagonalisable.
\end{enumerate}

\subsection{Deux applications non continues}
Montrer que les applications $\fonction{\varphi}{\M_n(\C)}{\M_n(\C)}{M=D+N}{D}$ et $\fonction{\psi}{\M_n(\C)}{\M_n(\C)}{M=D+N}{N}$ ne sont pas continues.

\section{\underline{Décomposition polaire} \centraleponts{3}}
\label{Décomposition polaire}
\textcolor{blue}{\hyperref[Décomposition polaire corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Soient $E$ un espace euclidien de dimension $n$ et $u\in S^{++}(E)$. Soit $v\in S^{++}(E)$ tel que $v^2=u$.
	\begin{enumerate}[label=\alph*.]
		\item Déterminer $v$.
		\item Montrer que $v$ est un polynôme en $u$.
	\end{enumerate}
	\item Soit $A\in \text{GL}_n(\R)$.
	\begin{enumerate}[label=\alph*.]
		\item Montrer que $A^\top A\in S^{++}_n(\R)$.
		\item En déduire qu'il existe un unique couple $(O,S)\in \O_n(\R)\times S^{++}_n(\R)$ tel que $A=OS$.
	\end{enumerate}
	\item Soit $A\in \M_n(\R)$.\\
	Montrer qu'il existe un couple $(O,S)\in \O_n(\R)\times S^+_n(\R)$ tel que $A=OS$. Ce couple est-il unique ?
\end{enumerate}

\subsection{Sous-groupe compact maximal de GL$_n(\R)$ \centraleponts{3}}
On munit $\R^n$ de son produit scalaire usuel $\langle\ ,\ \rangle:(X,Y)\mapsto X^\top Y$ et on note $\normep{2}{.}$ la norme associée. Pour $A\in \M_n(\R)$ on note $\rho(A)=\max\{|\lambda|,\ \lambda\in \text{Sp}(A)\}$. On pose ${|||.|||}_2:A\in \M_n(\R)\mapsto \sup\limits_{\substack{X\in \R^n\\\normep{2}{X}=1}}\normep{2}{AX}$.
\begin{enumerate}
	\item Montrer que $\forall A\in \M_n(\R),\ {|||A|||}_2=\sqrt{\rho(A^\top A)}$.
	\item Montrer que $\O_n(\R)$ est un sous-groupe compact de GL$_n(\R)$.
	\item Soit $G$ un sous-groupe compact de GL$_n(\R)$ qui contient $\O_n(\R)$. On cherche à montrer que $G=\O_n(\R)$. Pour cela on fixe $A\in G$ et on note $A=OS$ sa décomposition polaire.
	\begin{enumerate}[label=\alph*.]
		\item Montrer que les valeurs propres de $S$ sont toutes inférieures à $1$.
		\item Montrer que $\text{Sp}(S)=\{1\}$.
	\end{enumerate}
	\item Conclure.
\end{enumerate}

\subsection{Enveloppe convexe des matrices orthogonales \centraleponts{3}}
On pourra librement utiliser dans cet exercice le théorème de projection sur un convexe compact cf. VIII-33 tome Analyse\\
On munit $\M_{n,1}(\R)$ de son produit scalaire usuel $\langle\ ,\ \rangle_2:(X,Y)\mapsto X^\top Y$. On note $\normep{2}{.}$ la norme associée à ce produit scalaire.\\
On munit ensuite $\M_n(\R)$ de la norme subordonnée ${|||.|||}_2:A\mapsto\sup\limits_{\substack{X\in \M_{n,1}(\R)\\\normep {2}{X}=1}}\normep{2}{AX}$ et on note $\B$ la boule unité fermée de $\M_n(\R)$ pour cette norme.
\begin{enumerate}
	\item Soit $A\in \M_n(\R)$. Exprimer ${|||A|||}_2$ en fonction des valeurs propres de $A^\top A$.
	\item Montrer que l'enveloppe convexe (cf. VIII-36 tome Analyse) de $\O_n(\R)$ est contenue dans $\B$.
	\item On suppose qu'il existe une matrice $M\in \B$ qui n'est pas dans l'enveloppe convexe de $\O_n(\R)$. On admet que Conv$(\O_n(\R))$ est une partie compacte de $\M_n(\R)$ (cf. VIII-37 tome Analyse) et on note $N$ le projeté convexe de $M$ sur Conv($\O_n(\R))$ pour le produit scalaire $\langle\ ,\ \rangle:(A,B)\in \M_n(\R)^2\mapsto \Tr(A^\top B)$ et on pose $A=(M-N)^\top$. On écrit enfin $A=US$ une représentation polaire de $A$.
	\begin{enumerate}[label=\alph*.]
		\item Montrer que $\forall V\in \text{Conv}(\O_n(\R)),\ \Tr(AV)\leq\Tr(AN)<\Tr(AM)$.
		\item En déduire que $\Tr(S)<\Tr(USM)$.
		\item Montrer que $\Tr(S)\geq \Tr(MUS)$ puis conclure.
	\end{enumerate}
\end{enumerate}

\subsection{Points extrémaux des matrices orthogonales \centraleponts{3}}
On munit $\M_{n,1}(\R)$ de son produit scalaire usuel $\langle\ ,\ \rangle:(X,Y)\mapsto X^\top Y$. On note $\normep{2}{.}$ la norme associée à ce produit scalaire.\\
On munit ensuite $\M_n(\R)$ de la norme subordonnée ${|||.|||}:A\mapsto\sup\limits_{\substack{X\in \M_{n,1}(\R)\\\normep {2}{X}=1}}\normep{2}{AX}$ et on note $\B$ la boule unité fermée de $\M_n(\R)$ pour cette norme. On dit qu'un élément $A\in \B$ est extrémal dans $\B$ si l'écriture $A=\displaystyle\frac{1}{2}(B+C)$ avec $B,C\in \B$ entraîne $A=B=C$. On cherche à déterminer l'ensemble des points extrémaux de $\B$.
\begin{enumerate}
	\item Soit $U\in \O_n(\R)$ qui s'écrit $U=\displaystyle\frac{1}{2}(V+W)$ avec $V,W\in \B$. Montrer que pour tout $X\in \M_{n,1}(\R)$ les vecteurs $VX$ et $WX$ sont liées et en déduire que $U$ est extrémale dans $\B$.
\end{enumerate}
Soit $A\in \B\backslash\O_n(\R)$.
\begin{enumerate}[resume]
	\item Montrer que l'on peut écrire $A=PDQ$ avec $P,Q\in \O_n(\R)$ et $D$ une matrice diagonale dont les coefficients diagonaux $d_1,\dots,d_n$ sont tous positifs ou nuls.
	\item Montrer que $\forall i\in \crblanc{1}{n},\ d_i\leq 1$ et que de plus, $\exists j\in \crblanc{1}{n},\ d_j<1$.
	\item En déduire l'existence de deux matrices distinctes $A_\alpha$ et $A_{-\alpha}$ de $\B$ telles que $A=\displaystyle\frac{1}{2}(A_\alpha+A_{-\alpha})$. Conclure.
\end{enumerate}

\subsection{Matrice extraite d'une matrice orthogonale}
Soit $M\in \M_n(\R)$. On dit que $M$ a la propriété $(P)$ si et seulement si il existe $U\in \O_{n+1}(\R)$ telle que $M$ soit la matrice extraite de $U$ en lui retirant sa dernière ligne et sa dernière colonne i.e :
$$\exists \alpha_1,\dots,\alpha_{2n+1}\in \R,\ U=\begin{pmatrix}
	&&&\alpha_{2n+1}\\
	&M&&\vdots\\
	&&&\alpha_{n+2}\\
	\alpha_1&\cdots&\alpha_n&\alpha_{n+1}
\end{pmatrix}\in \O_{n+1}(\R)$$
\begin{enumerate}
	\item On suppose que $M=\diag(\lambda_1,\dots,\lambda_n)$ est une matrice diagonale. Déterminer une condition nécessaire et suffisante portant sur ses coefficients pour que $M$ ait la propriété $(P)$.
	\item On suppose que $M\in \mathcal S_n(\R)$. Déterminer une condition nécessaire et suffisante pour que $M$ ait la propriété $(P)$.
	\item On suppose que $M\in \text{GL}_n(\R)$ et on écrit $M=OS$ sa décomposition polaire. Déterminer une condition nécessaire et suffisante portant sur $M^\top M$ pour que $M$ ait la propriété $(P)$.
\end{enumerate}

\subsection{Matrices qui respectent le volume de $k$-parallélépipèdes rectangles}

\section{\underline{Décomposition QR} \ccinp{2}}
\label{Décomposition QR}
\textcolor{blue}{\hyperref[Décomposition QR corrigé]{[Corrigé]}}\\
\begin{enumerate}[leftmargin=*]
	\item Montrer que $\O_n(\R)$ et $T_n^{++}(\R)$ (ensemble des matrices triangulaires supérieures donc tous les coefficients diagonaux sont strictement positifs) sont des sous-groupes de $(\text{GL}_n(\R),\times)$.
	\item Montrer que $\O_n(\R)\cap T_n^{++}(\R)=\{I_n\}$.
	\item Soit $A\in \text{GL}_n(\R)$. Soit $(e_1,\dots e_n)$ la base constituée des vecteurs colonnes de $A$.
	\\Soit $\B_{GS}=(v_1,\dots,v_n)$ la base orthonormée obtenue à partir de $\mathcal{B}$ grâce au procédé d'orthonormalisation de Gramm-Schmidt.
	\\ A l'aide de ces bases, montrer qu'il existe un unique couple de matrices $(Q,R)\in \O_n(\R)\times T_n^{++}(\R)$ tel que $A=QR$
\end{enumerate}

\subsection{Matrice de Householder}
\begin{enumerate}[leftmargin=*]
	\item Comment s'écrit la réflexion $h$ par rapport à l'hyperplan orthogonal à un vecteur non nul $a$ ?
	\item Montrer que sa matrice dans la base canonique est $H=I_n-\displaystyle\frac{2[a]^\top[a]}{\norme{a}^2}$
	\item Montrer que si $u$ et $v$ sont deux vecteurs distincts de $\R^n$ et de même norme, alors il existe une réflexion unique $h$ telle que $h(u)=v$.
	\item Etudier l'existence d'une réflexion $h$ transformant un vecteur $u$ de $\R^n$ en $v=\lambda e_1$.
	\item Plus généralement, soit $i$ un indice de $\crblanc{1}{n}$ et $u=(x_1,\dots,x_n)$. On pose $v=(x_1,\dots,x_ {j-1},\lambda,0,\dots,0)$ et $w=(0,\dots,0,x_j,\dots,x_n)$.
	\\Etudier l'existence d'une réflexion $h$ de $\R^n$ transformant $u$ en $v$.
	\\Que dire des vecteurs $e_1,\dots,e_{j-1}$ pour l'application $h$?
	\item Déduire de ce qui précède qu'il existe $n-1$ matrice de Householder $H_1,H_2,\dots,H_{n-1}$ telles que pour tout $j$ les coefficients sous-diagonaux des $j$ premières colonnes de $A_j=H_jH_{j-1}\dots H_1A$ soient nuls.
	\item Montrer comment des produits successifs par les matrices $H_j$ permettent d'obtenir séparément les composantes $Q$ et $R$ d'une décomposition $QR$ de la matrice $A$.
	\item Illustrer cette méthode en décomposant $A=\begin{pmatrix}
		-4&20&35&5\\
		-4&-30&-15&55\\
		-8&40&-80&-65\\
		23&-15&30&15
	\end{pmatrix}$
\end{enumerate}


\section{Décomposition LU \centraleponts{3}}
\label{Décomposition LU}
\textcolor{blue}{\hyperref[Décomposition LU corrigé]{[Corrigé]}}\\
Soit $A=(a_{ij})_{1\leq i,j\leq n}\in \M_n(\K)$. Pour $k\in \crblanc{1}{n}$, on appelle $k$-ième mineur principal de $A$ le déterminant de la matrice $A$ à laquelle on a supprimé les $n-k$ dernières lignes et colonnes : $\Delta_k=\det(a_{ij})_{1\leq i,j\leq k}$.\\
Montrer que les propositions suivantes sont équivalentes :
\begin{itemize}
	\item Les mineurs principaux de $A$ sont tous non nuls
	\item Il existe une matrice 
	$L=\begin{pmatrix}
		1&0&\dots&0\\
		&1&\ddots&\vdots\\
		&*&\ddots&0\\
		&&&1
	\end{pmatrix}$
	et une matrice $U=
	\begin{pmatrix}
		d_1&&&\\
		0&d_2&*&\\
		\vdots&\ddots&\ddots&\\
		0&\dots&0&d_n
	\end{pmatrix}$
	telles que $A=LU$
\end{itemize}

\subsection{$\K=\R$ ou $\C$ \centraleponts{2}} 
Montrer que l'ensemble des matrices de $\M_n(\R)$ (resp. $\M_n(\C)$) admettant une composition $LU$ est un ouvert de $\M_n(\R)$ (resp. $\M_n(\C)$).

\subsection{$\K=\Z/p\Z$ \etoile{2} (HP)}
Soit $p$ un nombre premier.\\
Dénombrer l'ensemble des matrices de $\Z/p\Z$ qui admettent une décomposition $LU$.

\subsection{Algorithme de calcul \ccinp{2}}
Soit $A\in \M_n(\K)$ admettant une décomposition $LU$.\\
Proposer des algorithmes python permettant de calculer le déterminant de $A$, déterminer son inverse, et permettant de résoudre un système du type $AX=Y$ pour $Y\in \M_{n,1}(\K)$.\\
Montrer que la complexité temporelle de l'algorithme de calcul du déterminant est de l'ordre de $\displaystyle\frac{2}{3}n^3$ en comptant seulement le nombre d'additions et de multiplications, pour lesquelles on considérera qu'elle prennent le même temps constant. Commenter.

\subsection{Décomposition de Cholesky}
Montrer que $S_n^{++}(\R)=\{A^\top A,\ A\in \text{GL}_n(\R)\}$.

\section{Lemme de Fitting}
\label{Lemme de Fitting}
\textcolor{blue}{\hyperref[Lemme de Fitting corrigé]{[Corrigé]}}\\
\begin{enumerate}
	\item Soit $E$ un espace vectoriel de dimension $n$ et $u\in\mathcal{L}(E)$, il existe $p\in\N$ tel que : $$E=\text{Ker}(u^p)\oplus\text{Im}(u^p). $$
	\item Soit $M\in\M_n(\K)$. Montrer que M est semblable à une matrice de la forme $\begin{pmatrix}
		N&0\\0&P
	\end{pmatrix}$ où $N$ est une matrice nilpotente et $P$ est une matrice carrée inversible.
\end{enumerate}
\textit{\underline{Remarque :} A l'aide de ce résultat, on peut calculer le cardinal du cône nilpotent sur un corps fini de cardinal 
	Le cardinal $n_d$ de l'ensemble des matrices carrés nilpotentes de taille $d$ sur un corps de cardinal $q$ est :$$n_d=q^{d(d-1)}.$$ Il permet également de démontrer le théorème Krull-Schmidt, qui requis des connaissances sur les produits directs de groupes.}

\section{\underline{Réduction de Jordan}}
\label{Réduction de Jordan}
\textcolor{blue}{\hyperref[Réduction de Jordan corrigé]{[Corrigé]}}\\
On appelle bloc de Jordan toute matrice de la forme :  $J_n(\lambda) =
\begin{pmatrix}
	\lambda & 1 & 0 & \cdots & 0 \\
	0 & \lambda & 1 & \ddots & \vdots \\
	0 & 0 & \lambda & \ddots & 0 \\
	\vdots & \vdots & \ddots & \ddots & 1 \\
	0 & 0 & \cdots & 0 & \lambda
\end{pmatrix}$

\begin{enumerate}
	\item Soit $M\in\M_n(\K)$ de polynôme caractéristique scindé sur $\K$. On pose pour tout $\lambda\in \text{Sp}(M),$$ F_\lambda=\text{Ker}(M-\lambda I_n)^{m_\lambda}$ Montrer que $\displaystyle\M_{n,1}(\K)=\bigoplus_{\lambda\in\text{Sp}(M)}F_\lambda$
	\item Soit $f\in\mathcal{L}(E)$ nilpotent d'indice $p$. Montrer que pour tout $x\in E$ tel que $f^{p-1}(x)\ne 0$. \\Montrer que $\mathcal{F}=(f^{p-1}(x),\dots,f(x),x)$ est libre. Ecrire la matrice de $f$ dans $\mathcal{F}$.
	\item Déduire que toute matrice $M\in\M_n(\K)$ tel que son polynôme caractéristique est scindé est semblable à une matrice diagonale par blocs de Jordan.
	
\end{enumerate}
\subsection{Application}
\begin{enumerate}
	\item Retrouver le résultat de la décomposition de Dunford
\end{enumerate}

\section{\underline{Réduction de Frobenius}}
\label{Réduction de Frobenius}
\textcolor{blue}{\hyperref[Réduction de Frobenius corrigé]{[Corrigé]}}\\
Soient $E$ un espace vectoriel de dimension $n$, et $u\in\mathcal{L}(E)$.
On admettra les résultats sur les polynômes minimaux ponctuels. \\
En raisonnant par récurrence, montrer qu'il existe une décomposition $\displaystyle E=\bigoplus_{i=1}^rE_i$ en sous-espaces stables et $(P_i)_{i\in\crblanc{1}{r}}\in\K[X]^r$ tels que :
\begin{itemize}
	\item $u_{|E_i}$ est un endomorphisme cyclique de polynôme $P_i$.
	\item pour tout $i\in\crblanc{1}{r-1}$, $P_{i+1}|P_i$ et $P_1=\pi_u$
\end{itemize}
De, plus $(P_1,\dots,P_r)$ ne dépend que de $u$.
\subsection{$M\sim M^\top$}
Démontrer que toute matrice de $\M_n(\K)$ est semblable à sa transposée.

\subsection{Matrices semblables à leur inverse}
Déterminer les matrices de GL$_n(\K)$ semblables à leur inverse.

\subsection{Bicommutant}
	

\newpage
\chapter{Divers}
\section{Fonction $\R$-linéaire mais pas $\C$-linéaire \etoile{1}}
\label{Fonction R linéaire mais pas C linéaire}
\textcolor{blue}{\hyperref[Fonction R linéaire mais pas C linéaire corrigé]{[Corrigé]}}\\
Donner un exemple de fonction $\R$-linéaire mais pas $\C$-linéaire.

\section{Relation d'ordre \etoile{2}}
\label{Relation d'ordre}
\textcolor{blue}{\hyperref[Relation d'ordre corrigé]{[Corrigé]}}\\
Soient $D_1$ et $D_2$ deux dés à $6$ faces.\\ On pose $X_1$ et $X_2$ les variables aléatoires qui a un lancé des deux dés associe le nombre exhibé sur la face supérieur du dé $1$ et du dé $2$ respectivement.\\
On définit alors la relation $D_1$ est plus faible ($\preceq$) que $D_2$ par $D_1\preceq D_2\iff \p(X_1>X_2)\geq \p(X_2>X_1)$.\\
$\preceq$ est-elle une relation d'ordre ?

\section{Ordre lexicographique \etoile{1}}
\label{Ordre lexicographique}
\textcolor{blue}{\hyperref[Ordre lexicographique corrigé]{[Corrigé]}}\\
On définit sur $\C$ la relation binaire $\preceq$ par :
$$\forall (z_1,z_2)\in \C^2,\ z_1\preceq z_2\iff [\Re(z_1)<\Re(z_2)\text{ ou } (\Re(z_1)=\Re(z_2)\text{ et }\Im(z_1)\leq\Im(z_2))]$$
Montrer que $\preceq$ est une relation d'ordre sur $\C$ et qu'elle n'est pas totale. Que peut-on modifier pour quelle le soit ?

\section{Sous-groupes de GL$_n(\C)$ d'exposant fini \xens{5}}
\label{Sous-groupes de GL_n(C) d'exposant fini}
\textcolor{blue}{\hyperref[Sous-groupes de Gl_n(C) d'exposant fini corrigé]{[Corrigé]}}\\
Soit $n\in \N^*$.\\
Soit $G$ un sous-groupe de GL$_n(\C)$ tel que $$\exists k\in \N^*,\ \forall M\in G,\ M^k=I_n$$
Démontrer que $G$ est fini.\\
On pourra montrer que l'ensemble $T$ des traces des éléments de $G$ est fini puis construire une injection de $G$ dans $T^d$ pour un certain $d\in \N^*$ bien choisi.

\section{Formule de Burnside \xens{4}}
\label{Formule de Burnside}
\textcolor{blue}{\hyperref[Formule de Burnside corrigé]{[Corrigé]}}\\
Soit $V$ un $\K$-espace vectoriel de dimension $n\in \N^*$.\\
On se donne $G$ un sous-groupe fini de GL$(V)$ et on note :
$$V^G=\{x\in V\ |\ \forall g\in G,\ g(x)=x\}$$
Démontrer que $\displaystyle\frac{1}{|G|}\sum\limits_{g\in G}\Tr(g)=\dim(V^G)$.

\section{Caractères algébriques de GL$_n(\K)$}
\label{Caractères algébriques de GLn(K)}
\textcolor{blue}{\hyperref[Caractères algébriques de GLn(K) corrigé]{[Corrigé]}}\\
Soit $\chi:\text{GL}_n(\K)\to \K^*$ un morphisme de groupe polynomial.\\ Montrer que $\chi$ est une puissance du déterminant.

\section{Théorème de Fermat matriciel \xens{3} (HP)}
\label{Théorème de Fermat matriciel}
\textcolor{blue}{\hyperref[Théorème de Fermat matriciel corrigé]{[Corrigé]}}\\
Soit $p\in\N$ un nombre premier. Soit $M\in\M_n(\Z)$.
\begin{enumerate}
	\item Démontrer que pour tout polynôme $P\in\Z[X]$,
	on a : $P(X^p)-P(X)^p\in p\Z[X]$.
	\item Justifier qu'il existe $A\in\M_n(\Z[X])$ telle que $(XI_n-M)^p-(X^pI_n-M^p)=pA$
	\item Démontrer que $\chi_{M^p}(X^p)-\chi_M(X)^p\in \Z[X]$.
	\item En déduire que $\Tr(M^p)\equiv \Tr(M)[p]$
\end{enumerate}

\section{Développement décimal propre d'un réel \xens{3}}
\label{Développement décimal propre d'un réel}
\textcolor{blue}{\hyperref[Développement décimal propre d'un réel corrigé]{[Corrigé]}}\\
Démontrer que tout réel $x\in [0,1[$ s'écrit de manière unique $x=\displaystyle\sum_{n=1}^{+\infty}\frac{a_n}{10^n}$ avec $(a_n)_{n\in \N^*}$ une suite à valeurs dans $\crblanc{0}{9}$ qui ne stationne pas à $9$.

\subsection{Une caractérisation des rationnels \xens{3}}
Soit $x\in [0,1[$. Montrer que $x\in \Q$ si et seulement si son développement décimal propre est périodique à partir d'un certain rang.

\section{Distribution du premier chiffre des puissances de $2$ \xens{3}}
\label{Distribution du premier chiffre des puissances de 2}
\textcolor{blue}{\hyperref[Distribution du premier chiffre des puissances de 2 corrigé]{[Corrigé]}}\\
Soit $i\in \crblanc{1}{9}$. On note $N_i(n)$ le nombre d'éléments de $\{2,2^2,\dots,2^n\}$ dont le premier chiffre dans l'écriture décimale est $i$. On note pour $x\in \R$, $\{x\}=x-\lfloor x\rfloor$ la partie fractionnaire de $x$.
\begin{enumerate}
	\item Montrer qu'en notant $\theta=\log_{10}(2)$, $N_i(n)$ est exactement le nombre d'entier $k\in \crblanc{1}{n}$ tels que $\{k\theta\}$ est dans $[\log_{10}(i),\log_{10}(i+1)[$.
	\item Montrer que la suite $(\{k\theta\})_{k\in \N^*}$ est dense dans $[0,1]$.
	\item En déduire qu'il existe une infinité de puissances de $2$ dont le premier chiffre est $i$.
\end{enumerate}

\chapter{Correction}

\section{Correction Algèbre linéaire}
	\subsection{Extension de corps \etoile{2}}
	\label{Extension de corps corrigé}
	\textcolor{blue}{\hyperref[Extension de corps]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $\R$ est un $\R$-espace vectoriel de $\C$ qui est un $\C$-espace vectoriel. De plus $\R$ n'est pas un $\C$-espace vectoriel de $\C$ puisque $i\times 1_\C=i\ne \R$ par exemple.
		\item Soit $(v_1,\dots,v_n)$ une base du $\C$-espace vectoriel $V$. Soit $v\in V$.\\
		$\exists (z_1,\dots,z_n)\in \C^n,\ v=\displaystyle\sum_{k=1}^nz_kv_k$.\\
		Posons pour tout $k\in \crblanc{1}{n},\ x_k=\operatorname{Re}(z_k),\ y_k=\operatorname{Im}(z_k)$ et $w_k=iv_k\in V$ car $V$ est un $\C$-espace vectoriel.\\
		On peut alors écrire $v$ comme combinaison linéaire à coefficients réels de $v_1,\dots,v_n,w_1,\dots,w_n$ :
		$$v=\sum_{k=1}^nx_kv_k+y_kw_k$$
		ce qui justifie que $V$ est un $\R$-espace vectoriel et que $\F=(v_1,\dots,v_n,w_1,\dots,w_n)$ en est une partie génératrice.\\
		De plus, si $a_1,\dots,a_n,b_1,\dots,b_n$ sont des réels vérifiant $\displaystyle\sum_{k=1}^na_kv_k+b_kw_k=0_V$ alors $\displaystyle\sum_{k=1}^n(a_k+ib_k)v_k=0_V$.\\
		On en déduit, comme $(v_1,\dots,v_n)$ est une famille libre du $\C$-espace vectoriel $V$, que $\forall k\in \crblanc{1}{n},\ a_k+ib_k=0$.\\
		Puis comme $a_k$ et $b_k$ sont des réels, $a_k=b_k=0$ quel que soit $k\in \crblanc{1}{n}$.\\
		Ainsi $\F$ est une base du $\R$-espace vectoriel $V$ qui est donc de dimension $2n$.
		\item Supposons (i).\\
		Montrons que $(\mathbb M,+,\times)$ est un $\mathbb L$-espace vectoriel de dimension finie : $\forall (\alpha,\beta)\in \mathbb L^2,\ \forall (x,y)\in \mathbb M^2$,
		\begin{itemize}
			\item $(\mathbb M,+)$ est un groupe abélien.
			\item $(\alpha+\beta)\times x=\alpha\times x+\beta\times x$;
			\item $\alpha\times(x+y)=\alpha\times x+\alpha\times y$;
			\item $(\alpha\times\beta)\times x=\alpha\times(\beta\times x)$;
			\item $\mathbbm 1_{\mathbb L}\times x=x$.
		\end{itemize}
		et que $\mathbb L$ est un sous-espace vectoriel de $\mathbb M$ :
		\begin{itemize}
			\item $\mathbb L\subset \mathbb M$;
			\item $0_{\mathbb M}=0_{\mathbb L}\in \mathbb L$;
			\item $\forall (g_1,g_2,\lambda)\in \mathbb L\times \K,\ g_1-\lambda g_2\in \mathbb L$.
		\end{itemize}
		Donc $\mathbb L$ est de dimension finie en tant que $\K$-espace vectoriel.\\
		On se donne une base $(f_1,\dots,f_k)$ de $\mathbb M$ sur $\mathbb L$ et une base $(g_1,\dots,g_p)$ de $\mathbb L$ sur $\K$. Soit $f\in \mathbb M$.\\
		$\exists (\lambda_1,\dots,\lambda_k)\in \mathbb L^k,\ f=\displaystyle\sum_{i=1}^k\lambda_if_i$.\\
		$\forall i\in \crblanc{1}{k},\ \exists (\mu_{i,1},\dots,\mu_{i,p})\in \K^p,\ \lambda_i=\displaystyle\sum_{j=1}^p\mu_{i,j}g_j$.\\
		Posons pour $(i,j)\in \crblanc{1}{k}\times\crblanc{1}{p},\ h_{i,j}=g_jf_i\in \mathbb M$.\\
		On a $f=\displaystyle\sum_{\substack{1\leq i\leq k\\1\leq j\leq p}}\mu_{i,j}h_{i,j}$ donc $\F=(h_{i,j})_{\substack{1\leq i\leq k\\1\leq j\leq p}}$ est une famille génératrice du $\K$-espace vectoriel $\mathbb M$.\\
		De plus, si $(\eta_{i,j})_{\substack{1\leq i\leq k\\1\leq j\leq p}}\in \K^{kp}$ vérifie $\displaystyle\sum_{\substack{1\leq i\leq k\\1\leq j\leq p}}\eta_{i,j}h_{i,j}=0$ alors,
		$$\displaystyle\sum_{i=1}^k\left(\sum_{j=1}^p\eta_{i,j}g_j\right)f_i=0$$
		d'où par liberté de $(f_1,\dots,f_k)$,
		$$\forall i\in \crblanc{1}{k},\ \displaystyle\sum_{j=1}^p\eta_{i,j}g_j=0$$
		Puis par liberté de $(g_1,\dots,g_p)$,
		$$\forall (i,j)\in \crblanc{1}{k}\times\crblanc{1}{p},\ \eta_{i,j}=0$$
		Par conséquent $\F$ est une base du $\K$-espace vectoriel $\mathbb M$ et $m=kp$.\\\\
		Réciproquement supposons (ii).\\
		On note comme précédemment $(f_1,\dots,f_k)$ une base de $\mathbb M$ sur $\mathbb L$ et $(g_1,\dots,g_p)$ une base de $\mathbb L$ sur $\K$. On pose ensuite pour tout $(i,j)\in \crblanc{1}{k}\times \crblanc{1}{p},\ h_{i,j}=f_ig_j$ et $\F=(h_{i,j})_{\substack{1\leq i\leq k\\1\leq j\leq p}}$. Alors comme $\mathbb M$ est un corps, $\Vect_\K(\F)\subset \mathbb M$. Et si on fixe $f\in \mathbb M$ alors,\\
		on sait qu'il existe des éléments $\lambda_1,\dots,\lambda_k\in \mathbb L$ tels que $f=\displaystyle\sum_{i=1}^k\lambda_if_i$.\\
		Puis on sait que pour chaque $i\in \crblanc{1}{k}$, il existe des éléments $\mu_{i,1},\dots,\mu_{i,p}\in \K$ tels que $\lambda_i=\displaystyle\sum_{j=1}^p\mu_{i,j}g_j$.\\
		Par conséquent $f=\displaystyle\sum_{i=1}^k\left(\sum_{j=1}^p\mu_{i,j}g_j\right)f_i=\sum_{\substack{1\leq i\leq k\\1\leq j\leq p}}\mu_{i,j}h_{i,j}$ et $\mathbb M\subset \Vect_\K(\F)$.\\
		On montre comme précédemment que $\F$ est une famille libre et finalement $\mathbb M=\Vect_\K(\F)$ est un $\K$-espace vectoriel de dimension $kp$.
	\end{enumerate}
	
\subsection{Passage du complexe au réel \etoile{2}}
	\textcolor{blue}{\hyperref[Passage du complexe au réel]{[Enoncé]}}\\
	\label{Passage du complexe au réel corrigé}
	$\chi_v\in \C[X]$ est scindé donc $v$ est trigonalisable dans une certaine base $\B=(a_1,\dots,a_n)$ de $V$. On note $\Mat_\B(v)=(z_{ij})_{1\leq i,j\leq n}\in \M_n(\C)$. Pour tout $i,j\in \crblanc{1}{n}$ on note $z_{ij}=x_{ij}+iy_{ij}$ avec $x_{ij},y_{ij}\in \R$. On montre (cf.\ref{Extension de corps}) que $\tilde\B=(a_1,\dots,a_n,b_1,\dots,b_n)$, où $b_k=ia_k$, est une base de $W$.\\
	Alors $\M_{\tilde B}(u)$ est triangulaire par blocs, avec pour blocs diagonaux $R_k:=\begin{pmatrix}
		x_{kk}&-y_{kk}\\
		y_{kk}&x_{kk}
	\end{pmatrix}$.\\
	Ainsi $\det(u)=\displaystyle\prod_{k=1}^n\det(R_k)=\prod_{k=1}^n(x_k^2+y_k^2)=\prod_{k=1}^n|z_k|^2=|\det(v)|^2$.
	
\subsection{Dimension de $\R$ en tant que $\Q$-espace vectoriel \etoile{2}}
\label{Dimenseion de R en tant que Q espace vectoriel corrigé}
\textcolor{blue}{\hyperref[Dimension de R en tant que Q espace vectoriel ]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On sait que $\R$ est un $\R$-espace vectoriel et comme $\Q$ est un sous-corps de $\R$ on vérifie aisément les axiomes (cf. \ref{Extension de corps}).
		\item Soient $p_1,\dots,p_n$ des nombres premiers distincts et soient $\alpha_1,\dots,\alpha_n$ des nombres rationnels tels que $\displaystyle\sum_{i=1}^n\alpha_i\ln(p_i)=0$.\\
		On note $I=\{i\in \crblanc{1}{n},\ \alpha_i\ne 0\}$ et pour tout $i\in I,\ \alpha_i=\displaystyle\frac{a_i}{b_i}$ avec $a_i\in \Z\backslash\{0\}$ et $b_i\in \N^*$. Enfin on note pour tout $i\in I,\ c_i=a_i\displaystyle\prod_{j=1}^nb_j\in \Z$\\
		En multipliant par $\displaystyle\prod_{j=1}^nb_j$ on a $\displaystyle\sum_{i\in I}c_i\ln(p_i)=0$ puis $\displaystyle\sum_{\substack{i\in I\\c_i>0}}c_i\ln(p_i)=\sum_{\substack{i\in I\\c_i<0}}-c_i\ln(p_i)$.\\
		Ou encore $\displaystyle\ln\left(\prod_{\substack{i\in I\\c_i>0}}p_i^{c_i}\right)=\ln\left(\prod_{\substack{i\in I\\c_i<0}}p_i^{-c_i}\right)$ c'est à dire $\displaystyle\prod_{\substack{i\in I\\c_i>0}}p_i^{c_i}=\prod_{\substack{i\in I\\c_i<0}}p_i^{-c_i}$.\\
		Par unicité de la décomposition en produit de facteurs premiers et comme $\{i\in I,\ c_i>0\}\cap\{i\in I,\ c_i<0\}=\emptyset$, $\forall i\in I,\ c_i=0$. Donc $\forall i\in I,\ a_i=0$ et $\forall i\in \crblanc{1}{n},\ \alpha_i=0$.\\
		Ainsi $(\ln(p_1),\dots,\ln(p_n))$ est $\Q$-libre.
		\item On sait qu'il existe une infinité de nombres premiers donc $\R$ n'admet pas de famille $\Q$-libre maximale : $\R$ est un $\Q$-espace vectoriel de dimension infinie.
	\end{enumerate}
	
	\subsection{Fonctions indépendantes \xens{3}}
	\label{Fonctions indépendantes corrigé}
	\textcolor{blue}{\hyperref[Fonctions indépendantes]{[Enoncé]}}\\
	On construit $x_1,\dots,x_n$ par récurrence.\\
	$f_1$ n'est pas nulle donc il existe $x_1$ tel que $f_1(x_1)\ne 0$. La matrice $(f_1(x_1))\in \M_1(\C)$ est inversible.\\
	Supposons disposer de $x_1,\dots,x_{k-1}\in X$ tels que la matrice $(f_i(x_j))_{1\leq i,j\leq k-1}$ soit inversible (on note $\Lambda$ son déterminant). Supposons que quel que soit $x_k\in X$ la matrice $M:=(f_i(x_j))_{1\leq i,j\leq k}$ n'est pas inversible.\\
	Alors en développant le déterminant par rapport à la dernière colonne on a :
	$$\det(M)=\Lambda f_k(x_k)+\sum_{i=1}^k(-1)^{i+k}\Delta_{ik}f_i(x_k)=0$$
	où $\Delta_{ij}$ est le mineur de $M$ correspondant au coefficient $f_i(x_j)$.\\
	Par hypothèse $\Lambda$ est non nul donc $f_k(x_k)=\displaystyle\sum_{i=1}^k\frac{(-1)^{i+k}\Delta_{ik}}{\Lambda}f_i(x_k)$.\\
	Ceci étant vrai pour tout $x_k\in X,\ f_k=\displaystyle\sum_{i=1}^k\frac{(-1)^{i+k}\Delta_{ik}}{\Lambda}f_i$.\\
	Cela contredit la liberté de $(f_1,\dots,f_k)$ donc il existe $x_k\in X$ tel que $M$ est inversible.\\
	Par récurrence, il existe $x_1,\dots,x_n\in X$ tel que la matrice $(f_i(x_j))_{1\leq i,j\leq n}$ soit inversible.
	
	\subsection{L'ordre a son importance \telecom{2}}
	\label{L'ordre a son importance corrigé}
	\textcolor{blue}{\hyperref[L'ordre a son importance]{[Enoncé]}}\\
	Il est clair que si $M$ est une homothétie alors \[\forall A,B\in \M_n(\R), \Tr(MAB)= \Tr(MBA)\]
	Réciproquement, supposons que $M$ vérifie la propriété de l'énoncé.\\
	Alors elle est vérifié en particulierr pour les matrices élémentaires, cela va nous fournir des conditions très restrictives sur les coefficients de $M$.\\ On en déduit que $M$ est une homothétie.
	
	\subsection{Isomorphisme de $\M_n(\K)$ qui respecte les matrices de rang 1}
	\label{Isomorphisme de M_n(K) qui respecte les matrices de rang 1 corrigé}
	\textcolor{blue}{\hyperref[Isomorphisme de M_n(K) qui respecte les matrices de rang 1]{[Enoncé]}}\\
	
	\subsection{Centre de $\M_n(\K)$ \centraleponts{2}}
	\label{Centre de Mn(K) corrigé}
	\textcolor{blue}{\hyperref[Centre de Mn(K)]{[Enoncé]}}\\
	Soit $A\in Z$. On note pour $1\leq i,j\leq n,\ E_{ij}$ la matrice dont tous les coefficients sont nuls sauf celui sur la $i$-ième ligne, $j$-ième colonne qui vaut $1$. Fixons $(i,j)\in \crblanc{1}{n}^2$.\\
	$AE_{ij}=E_{ij}A$ donc pour $(k,l)\in \crblanc{1}{n}^2$ on a :\\
	$$\left(AE_{ij}\right)_{kl}=\displaystyle\sum_{p=1}^nA_{kp}\left(E_{ij}\right)_{pl}=A_{ki}\left(E_{ij}\right)_{il}=\left(E_{ij}\right)_{kj}A_{jl}=\sum_{p=1}^n\left(E_{ij}\right)_{kp}A_{pl}=\left(E_{ij}A\right)_{kl}$$
	En particulier, pour $k=i,l=j$ on obtient $A_{ii}=A_{ii}\left(E_{ij}\right)_{ij}=\left(E_{ij}\right)_{ij}A_{jj}=A_{jj}$.\\
	Et pour $k\ne i,l=j$ on obtient $A_{ki}=A_{ki}\left(E_{ij}\right)_{ij}=\left(E_{ij}\right)_{kj}A_{jj}=0$.\\
	Ainsi $A=A_{11}I_n$ est une matrice scalaire.\\\\
	Réciproquement fixons $\alpha\in \K$.\\
	$\forall B\in \M_n(\K),\ (\alpha I_n)B=\alpha B=B(\alpha I_n)$.\\
	Finalement, les seules matrices qui commutent avec toutes les autres sont les matrices scalaires : $Z=\Vect(I_n)$.
	
	\subsection{$\M_n(\K)$ est violemment non commutatif}
	\label{Mn(K) est violemment non commutatif}
	\textcolor{blue}{\hyperref[Mn(K) est violemment non commutatif corrigé]{[Enoncé]}}\\
	
	\subsection{Racine carrée de la dérivation \xens{3}}
	\label{Racine carrée de la dérivation corrigé}
	\textcolor{blue}{\hyperref[Racine carrée de la dérivation]{[Enoncé]}}\\
	Supposons qu'il existe un tel endomorphisme $\delta$. Alors $\delta\circ\Delta=\delta^3=\Delta\circ\delta$.\\
	Donc $\Ker(\Delta)=\Vect(x\mapsto 1)$ est stable par $\delta$, autrement dit l'image par $\delta$ d'une fonction constante est une fonction constante. Intéressons nous à l'image de la fonction constante égale à $1$. Si $\delta(x\mapsto 1)=x\mapsto \lambda\ne 0_E$ alors par linéarité $\Delta(x\mapsto 1)=\delta^2(x\mapsto 1)=\delta(x\mapsto \lambda)=\lambda\delta(x\mapsto 1)=x\mapsto \lambda^2\ne 0_E$ ce qui est absurde.\\
	Par conséquent $\delta(x\mapsto 1)=0_E$ Mais alors, $\Delta(\delta(\Id_\R))=\delta(\Delta(\Id_\R))=\delta(x\mapsto 1)=0_E$.\\
	Par conséquent $\delta(\Id_\R)$ est une fonction constante d'où $\Delta(\Id_\R)=\delta(\delta(\Id_\R))=0$ ce qui est absurde.\\
	Ainsi il n'existe pas d'endomorphisme $\delta$ de $E$ qui vérifie $\delta^2=\Delta$.
	
	\subsection{Produit de matrices nilpotentes \xens{5}}
	\label{Produit de matrices nilpotentes corrigé}
	\textcolor{blue}{\hyperref[Produit de matrices nilpotentes]{[Enoncé]}}\\
	Montrons le lemme suivant : si $A$ est une matrice non nulle et $B$ est une matrice nilpotente qui commute avec $A$ alors $\rg(AB)<\rg(A)$.\\
	Soient $A\in \M_n(\K)\backslash\{0_n\}$ et $B\in \M_n(\K)$ une matrice nilpotente qui commute avec $A$. Posons $\varphi_A$ et $\varphi_B$ les endomorphismes canoniquement associés à $A$ et $B$ respectivement.\\
	Il est clair que $\text{Im}(AB)\subset \text{Im}(A)$ donc $\rg(AB)\leq \rg(A)$. Supposons par l'absurde que $\rg(AB)=\rg(A)$\\
	Soit $Y\in \text{Im}(A)$. $\exists X\in \M_{n,1}(\K),\ Y=AX$. Donc $BY=BAX=ABX\in \text{Im}(A)$. Ainsi $\varphi_B$ induit un endomorphisme $\tilde \varphi_B$ sur Im($A$).\\
	De plus, si $Y\in \Ker(\tilde \varphi_B)$ alors $BY=0$ et il existe $X\in \M_{n,1}(\K)$ telle que $Y=AX$. Autrement dit $BAX=0$ ou encore $X\in \Ker(BA)=\Ker(AB)$. Or $\Ker(A)\subset \Ker(AB)$ et on a supposé que $\rg(AB)=\rg(A)$. Donc à l'aide du théorème du rang on en déduit que $\Ker(AB)=\Ker(A)$. Mais alors $Y=0$ d'où $\tilde \varphi_B$ est injectif et donc bijectif.\\
	Pourtant, on montre aisément que $\forall p\in \N^*,\forall X\in \M_{n,1}(\K)\ \varphi_B^p(X)=B^pX$. Par conséquent $\tilde \varphi_B$ est nilpotent en tant que restriction d'une application nilpotente; il ne peut pas être bijectif.\\
	En outre, $\rg(AB)<\rg(A)$.\\
	Par récurrence immédiate sur le nombre de matrice dans le produit on en déduit que $\rg\left(\displaystyle\prod_{i=1}^nN_i\right)=0$ c'est à dire que $\displaystyle\prod_{i=1}^nN_i=0$.
	
	\subsection{Suites périodiques \etoile{4}}
	\label{Suites périodiques corrigé}
	\textcolor{blue}{\hyperref[Suites périodiques]{[Enoncé]}}\\
	$(0)_{n\in \N}\in E$ car la suite nulle est $1$-périodique par exemple. Soit $(u_n)_{n\in \N}$ une suite $p$-périodique et $(v_n)_{n\in \N}$ une suite $q$-périodique pour un certain couple $(p,q)\in (\N^*)^2$. Fixons $\lambda\in \C$.\\
	Par récurrence immédiate, si $a$ est un multiple de $p$ et si $b$ est un multiple de $q$ alors $\forall n\in \N,\ u_{n+a}=u_n,v_{n+b}=v_n$.\\
	Notons $m=\ppcm(p,q)$. Comme $m$ est un multiple de $p$ et de $q$, $\forall n\in \N,\ u_{n+m}+\lambda v_{n+m}=u_n+\lambda v_n$.\\
	Autrement dit $(u_n+\lambda v_n)_{n\in \N}$ est $m$-périodique.\\
	Ainsi $E$ est un sous-espace vectoriel de $\C^\N$.\\
	Posons $\fonction{S}{\C^\N}{\C^\N}{(u_n)_{n\in \N}}{(u_{n+1})_{n\in \N}}$. On vérifie que $S$ est un endomorphisme de $\C^\N$.\\
	On remarque que $u\in E\iff \exists r\in \N^*,\ S^r(u)=u$.\\
	De plus pour un entier $r\in \N^*$ fixé, $X^r-1=\displaystyle\prod_{k=1}^r(X-\omega_r^k)$ en notant $\omega_r=e^{\frac{2i\pi}{r}}$.\\
	Enfin pour $1\leq k_1\ne k_2\leq r, (X-\omega_r^{k_1})\wedge (X-\omega_r^{k_2})=1$.\\
	Par conséquent d'après le lemme des noyaux, $\Ker(S^r-\Id_{\C^\N})=\displaystyle\bigoplus_{k=1}^r\Ker(S-\omega_r^k\Id_{\C^\N})$.\\
	On sait que $\Ker(S-\omega_r^k\Id_{\C^n})=\{(u_n)\in \C^\N\ |\ \forall n\in \N,\ u_{n+1}=\omega_r^ku_n\}=\Vect\left((\omega_r^{kn})_{n\in \N}\right)$. On remarque enfin que $\forall (k,r,p)\in (\N^*)^3,\ (\omega_{pr}^{pkn})_{n\in \N}=(\omega_r^{kn})_{n\in \N}$ ce qui permet de se contenter des $(\omega_r^{kn})_{n\in \N}$ pour $r$ et $k$ premiers entre eux.\\
	On a alors montré que $E=\Vect\left((\omega_r^{kn})_{n\in \N},\ r\wedge k=1\right)$. Cette famille est bien libre puisqu'elle est formée de vecteurs propres de $S$ associés à des valeurs propres différentes (ce qui est assuré par le fait que $k$ et $r$ sont pris premiers entre eux).
	
	\subsection{Dimension du commutant (1) \centraleponts{3}}
	\label{Dimension du commutant (1) corrigé}
	\textcolor{blue}{\hyperref[Dimension du commutant (1)]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soient $B,C\in \M_n(\C)$ et soit $\lambda\in \C$.\\
		$f_A(B+\lambda C)=A(B+\lambda C)-(B+\lambda C)A=AB-BA+\lambda(AC-CA)=f_A(B)+\lambda f_A(C)$.\\
		On en déduit que $\mathcal C(A)=\Ker(f_A)$ est un sous-espace vectoriel de $\M_n(\C)$.
		\item $A$ est trigonalisable dans $\M_n(\C)$ donc il existe $P\in \T{GL}_n(\C)$ et $T$ triangulaire supérieure telles que $A=PTP^{-1}$.\\
		Alors $\forall B\in \M_n(\C),\ B\in \mathcal C(A)\iff AB=BA\iff PTP^{-1}B=BPTP^{-1}\iff TP^{-1}BP=P^{-1}BPT\iff P^{-1}BP\in \mathcal C(T)$.\\
		Donc l'application $B\in \M_n(\C)\mapsto P^{-1}BP$ est un isomorphisme de $\mathcal C(A)$ sur $\mathcal C(T)$.\\
		Par conséquent $\dim\mathcal C(A)=\dim\mathcal C(T)$.
		\item On sait que le produit et la différence de deux matrices triangulaires supérieures est triangulaire supérieure. Ainsi $f_A$ stabilise le sous-espace des matrices triangulaires supérieures, il induit donc un endomorphisme sur celui-ci.
		\item On applique le théorème du rang à $\tilde f_A$ : $\dim(\Ker(\tilde f_A))=\dfrac{n(n+1)}{2}-\rg(\tilde f_A)$.\\
		Or on remarque que l'image de $\tilde f_A$ est incluse dans le sous-espace des matrices triangulaires supérieures à diagonale nulle (car les coefficients diagonaux d'un produit de deux matrices triangulaires sont les produits des coefficients diagonaux des matrices en jeux).\\
		On obtient donc la majoration $\rg(\tilde f_A)\leq \dfrac{n(n+1)}{2}-n$.\\
		Ainsi $\dim(\Ker(\tilde f_A))\geq n$. Enfin il est clair que $\Ker(\tilde f_A)\subset \Ker(f_A)=\mathcal C(A)$ d'où :
		$$\dim\mathcal C(A)\geq \dim(\Ker(\tilde f_A))\geq n$$
	\end{enumerate}
	
	\subsection{Dimension du commutant (2) \centraleponts{4}}
	\label{Dimension du commutant (2) corrigé}
	\textcolor{blue}{\hyperref[Dimension du commutant (2)]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $y\in \T{Im}(u)$. Il existe $x\in E$ tel que $y=u(x)$. Donc $v(y)=v\circ u(x)=u(v(x))\in \T{Im}(u)$. Ainsi $v(\T{Im}(u))\subset \T{Im}(u)$ et $v$ induit un endomorphisme sur $\T{Im}(u)$.
		\item Par double inclusion :\\
		Soit $y\in \T{Im}(u)\cap\Ker(u^k)$. Il existe $x\in E$ tel que $y=u(x)$. De plus $0=u^k(y)=u^{k+1}(x)$ donc $x\in \Ker(u^{k+1})$ d'où $y\in u(\Ker(u^{k+1}))$.\\
		Réciproquement, si $y\in u(\Ker(u^{k+1})$ alors il existe $x\in \Ker(u^{k+1})$ tel que $y=u(x)$. Donc $y\in \T{Im}(u)$ et $u^k(y)=u^{k+1}(x)=0$. Donc $y\in \T{Im}(u)\cap\Ker(u^k)$.\\
		Pour avoir la deuxième égalité on remarque que $\T{Im}(u)\cap\Ker(u^k)=\Ker(\tilde u^k)$.\\
		Puis par le théorème du rang, $\dim(u(\Ker(u^{k+1})))=\dim(\Ker(u^{k+1}))-\dim(\Ker(\overline u))$ avec $\overline u$ l'endomorphisme induit par $u$ sur $\Ker(u^{k+1})$. Or $\Ker(\overline u)=\Ker(u)\cap\Ker(u^{k+1})=\Ker(u)$.\\
		On a alors $e_k(\tilde u)=\dim(\Ker(\tilde u^k))-\dim(\Ker(\tilde u^{k-1}))=\dim(u(\Ker(u^{k+1})))-\dim(u(\Ker(u^k)))=\dim(\Ker(u^{k+1}))-\dim(\Ker(u^k))=e_{k+1}(u)$.
		\item Soit $v\in \mathcal C(u)$. $\tilde v$ commute avec $\tilde u$ en tant que restrictions de deux endomorphismes qui commutent. On montre aisément que $\mathcal C(u)$ est un sous-espace vectoriel de $\L(E)$ et que $\Phi$ linéaire.\\
		Enfin, fixons $w\in \mathcal C(\tilde u)$. On définit $v\in \L(E)$ sur $\T{Im}(u)$ par $v(x)=w(x)$ et sur un supplémentaire $S$ de $\T{Im}(u)$ dans $E$, dont on note $(a_1,\dots,a_s)$ une base, par $v(a_i)=b_i$ où $b_i\in E$ tel que $u(b_i)=w(u(a_i))$ ($b_i$ existe car $w\in \L(\T{Im}(u))$).\\
		On a alors $v(u(a_i))=w(u(a_i))=u(v(a_i))$ pour tout $1\leq i\leq s$ d'où $v\in \mathcal C(u)$ et $\Phi(v)=w$.
		\item Tout d'abord remarquons que la somme est bien définie. $\forall k\in \N,\ \Ker(u^k)\subset \Ker(u^{k+1}\subset E$. La suite $(\dim(\Ker(u^k)))_{k\in \N}$ est donc une suite croissante et majorée d'entiers : elle stationne. Ainsi la suite $(e_k(u))_{k\in \N^*}$ stationne à $0$.\\
		Par récurrence forte sur la dimension de $E$ :\\
		$\dim(E)=0$ : $u=0$, $\mathcal C(u)=\{0\}$ et $\forall k\in \N,\ \dim(\Ker(u^k))=0$.\\
		Hérédité : Supposons le résultat vrai pour tout espace de dimension strictement inférieure à $\dim(E)$.\\
		En appliquant le théorème du rang à $\Phi$ on a $\dim(\mathcal C(u))=\dim(\Ker \Phi)+\dim(\mathcal C(\tilde u))$. Or $\rg(u)<\dim(E)$ donc par hypothèse de récurrence, $\dim(\mathcal C(\tilde u))=\displaystyle\sum_{k=1}^{+\infty}e_k(\tilde u)^2$. Puis par la question 2, $\dim(\mathcal C(\tilde u))=\displaystyle\sum_{k=2}^{+\infty}e_k(u)^2$.\\
		Il faut donc montrer que $\Ker\Phi$ est dimension $e_1^2=\dim(\Ker(u))^2$.\\
		Le noyau de $\Phi$ est inclus dans l'ensemble des endomorphisme de $E$ qui sont nul sur l'image de $u$. Cet espace est isomorphe à $\L(S,E)$. Au vu de la dimension que l'on veut, comme le théorème du rang donne $\dim(S)=\dim(\Ker(u))$, on peut essayer de montrer que $\Ker\Phi$ est isomorphe à $\L(S,\Ker(u))$.\\
		Posons $\fonction{f}{\Ker\Phi}{\L(S,\Ker(u))}{v}{\overline v}$ où $\overline v=v_{|S}^{|\Ker(u)}$. Vérifions que $f$ est un isomorphisme.\\
		Soit $v\in \Ker\Phi$. $v$ commute avec $u$ et est nul sur $\T{Im}(u)$ donc $\forall x\in S,\ u(v(x))=v(u(x))=0$. Ainsi $\overline v\in \L(S,\Ker(u))$. $f$ est clairement linéaire.\\
		$f$ est injectif car pour tout $v\in \Ker\Phi$, $v$ est nul sur $S$ et sur $\T{Im}(u)$ et par suite sur $E$.\\
		Enfin $f$ est surjectif : si $w\in \L(S,\Ker(u))$ alors on pose $v\in \L(E)$ définit comme étant nul sur l'image de $u$ et égal à $w$ sur $S$. On a alors
		\begin{itemize}
			\item $\forall x\in \T{Im}(u),\ u\circ v(x)=u(0)=0=v\circ u(x)$;
			\item $\forall x\in S,\ u\circ v(x)=u\circ w(x)=0=v\circ u(x)$.
		\end{itemize}
		Par conséquent $\dim(\Ker\Phi)=\dim(S)\dim(\Ker(u))=e_1^2$ et puis :
		$$\dim\mathcal C(u)=\sum_{k=1}^{+\infty}e_k(u)^2$$
	\end{enumerate}
	
	\subsection{Etude de la comatrice \etoile{3}}
	\label{Etude de la comatrice corrigé}
	\textcolor{blue}{\hyperref[Etude de la comatrice]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Si $r=n$ alors $J_r=I_n$. On sait que $I_n\Com(I_n)^\top=\det(I_n)I_n$ i.e $\Com(I_n)^\top=I_n$ ou encore $\Com(I_n)=I_n$. Dans ce cas $\rg(\Com(J_r))=n$.\\
		Si $r=n-1$ alors $J_r=\left(\begin{array}{ccc|c}
			& & &0\\
			& I_{n-1}& &\vdots\\
			& & &0\\
			\hline
			0&\cdots&0&0
		\end{array}\right)$. Le seul mineur non nul de $J_r$ est celui obtenu en retirant la dernière ligne et la dernière colonne de $J_r$. Le cofacteur associée vaut $(-1)^{2n}\det(I_{n-1})=1$ d'où $\Com(J_{n-1})=J_1$ est de rang $1$.\\
		Enfin si $0\leq r\leq n-2$ alors par définition du rang toute matrice extraite de $J_r$ de taille $n-1$ n'est pas inversible. La comatrice de $J_r$ est donc nulle et est de rang nul.
		\item Soit $(A,B)\in \M_n(\K)^2$.\\
		Si $A$ et $B$ sont inversibles on sait que $AB$ l'est aussi d'où :\\
		$\Com(AB)=\det(AB)\left((AB)^{-1}\right)^\top=\det(A)\det(B)\left(B^{-1}A^{-1}\right)^\top=\det(A)\left(A^{-1}\right)^\top\det(B)\left(B^{-1}\right)^\top=\Com(A)\Com(B)$.\\
		Or on sait (cf. VIII-53 tome Analyse) que GL$_n(\K)$ est dense dans $\M_n(\K)$ donc il existe deux suites de matrices inversibles $(A_k)_{k\in \N},\ (B_k)_{k\in \N}$ telles que $A_k\ukfty\longrightarrow A$ et $B_k\ukfty\longrightarrow B$.\\
		De plus, on sait que la fonction $(M,N)\in \M_n(\K)^2\mapsto MN$ est continue car bilinéaire et que la fonction $M\in \M_n(\K) \mapsto\det(M)=\displaystyle\sum_{\sigma\in \mathcal S_n}\varepsilon(\sigma)\prod_{i=1}^nm_{i,\sigma(i)}$ est continue car polynomiale en les coefficients de $M$, d'où $\varphi:M\in \M_n(\K)\mapsto\Com(M)$ est continue car les coefficients de $\Com(M)$ sont polynomiaux en ceux de $M$ (ce sont des calculs de déterminant à la multiplication par un scalaire près).\\
		Par conséquent, par composition et produit de limites, $\Com(A_kB_k)\ukfty\longrightarrow\Com(AB)$ et $\Com(A_k)\Com(B_k)\ukfty\longrightarrow\Com(A)\Com(B)$.\\
		D'autre part, $\forall k\in \N,\ \Com(A_kB_k)=\Com(A_k)\Com(B_k)$. Ainsi par unicité de la limite, $\Com(AB)=\Com(A)\Com(B)$.
		\item Soit $A\in \M_n(\K)$. On note $r=\rg(A)$. On sait qu'il existe deux matrices inversibles $P,Q$ telles que $A=PJ_rQ$.\\
		D'après la question précédente, $\Com(A)=\Com(P)\Com(J_r)\Com(Q)$. Or d'après la question $1$, $\Com(P)$ et $\Com(Q)$ sont inversibles car $P$ et $Q$ sont inversibles. Ainsi $\Com(A)$ est équivalente à $\Com(J_r)$, elles ont donc le même rang.\\
		On a alors d'après la question $1$ :\\
		$\rg(\Com(A))=
		\begin{cases}
			0&\mbox{si }r<n-1\\
			1&\mbox{si }r=n-1\\
			n&\mbox{si }r=n
		\end{cases}$
		\item Si $n\geq 3$ alors $\varphi$ n'est pas injective puisque $\varphi(0)=0=\varphi(J_1)$. Elle n'est pas non plus surjective car aucune matrice de l'image de $\varphi$ n'est de rang $2$.\\
		Si $n=1$ alors $\varphi$ est la fonction constante égale à la matrice $(1)$. Elle n'est donc ni injective ni surjective encore une fois.\\
		Enfin si $n=2$ alors $\varphi:\begin{pmatrix}a&b\\c&d\end{pmatrix}\mapsto\begin{pmatrix}d&-c\\-b&a\end{pmatrix}$. On remarque alors que $\varphi^2=\Id_{\M_2(\K)}$ donc $\varphi$ est bijective.\\
		Déterminons Im$(\varphi)$ dans le cas $\K=\C$. On vient de voir que si $n=1$ alors Im$(\varphi)=\{I_1\}$ et si $n=2$ alors Im$(\varphi)=\M_2(\C)$. Supposons maintenant $n\geq 3$.\\
		On a vu que Im$(\varphi)\subset\text{GL}_n(\C)\cup\{A\in \M_n(\C),\ \rg(A)=1\}$.\\
		Fixons $A\in \text{GL}_n(\C)$.\\
		$\Com(M)=A\implies \det(\Com(M)^\top)=\det(A^\top)\implies\det(\det(M)M^{-1})=\det(A)\implies\det(M)^{n-1}=\det(A)$.\\
		On sait qu'il existe $z\in \C$ tel que $z^{n-1}=\det(A)$. On pose donc $M=z\left(A^{-1}\right)^\top$.\\
		On vérifie que $\Com(M)^\top=\det(M)M^{-1}=\displaystyle\frac{z^n}{\det(A)}\cdot\frac{A^\top}{z}=\frac{z^{n-1}}{\det(A)}A^\top=A^\top$ d'où $\Com(M)=A$.\\
		Fixons $A\in \M_n(\C)$ de rang $1$. $\exists P,Q\in \text{GL}_n(\C),\ A=PJ_1Q$. On rappelle que $J_1=\Com(J_{n-1})$ et d'après ce qui précède, il existe deux matrices inversibles $P',Q'$ telles que $P=\Com(P')$ et $Q=\Com(Q')$. Donc $A=\Com(P')\Com(J_1)\Com(Q')$ et d'après la question $2$, $A=\Com(P'J_1Q')$.\\
		Par suite, Im$(\varphi)=\text{GL}_n(\C)\cup\{A\in \M_n(\C),\ \rg(A)=1\}$.
	\end{enumerate}
	
	\subsection{Caractérisation des homothéties (1) \ccinp{2}}
	\label{Caractérisation des homothéties (1) corrigé}
	\textcolor{blue}{\hyperref[Caractérisation des homothéties (1)]{[Enoncé]}}\\
	Soient $x_1,x_2 \in E$. D'après l'hypothèse de l'énoncé, il existe $\lambda_1,\lambda_2 \in \K$ tels que $u(x_1)=\lambda_1x_1$ et $u(x_2)=\lambda_2x_2$.
	\begin{itemize}
		\item  Supposons que $(x_1,x_2)$ est une famille libre.\\ 
		Il existe $\lambda\in\K$ tel que $u(x_1+x_2)=\lambda(x_1+x_2).$\\
		On a par linéarité de $u$ : $u(x_1+x_2)=\lambda_1 x_1 +\lambda_2x_2$. Et puisque $(x_1,x_2)$ est une famille libre, $\lambda=\lambda_1=\lambda_2$.
		\item Supposons que $(x_1,x_2)$ est une famille liée. Donc il existe $\alpha\in\K$ tel que $x_1=\alpha x_2.$\\
		$\lambda_1x_1=u(x_1)=u(\alpha x_2)=\alpha u(x_2)=\alpha\lambda_2x_2=\lambda_2x_1$. 
	\end{itemize}
	On en déduit que $u$ est bien une homothétie.
	
	\subsection{Caractérisation des homothéties (2)}
	\label{Caractérisation des homothéties (2) corrigé}
	\textcolor{blue}{\hyperref[Caractérisation des homothéties (2)]{[Enoncé]}}\\
	
	\subsection{Espaces engendrés par les matrices inversibles et orthogonales}
	\label{Espaces engendrés par les matrice inversibles et orthogonales corrigé}
	\textcolor{blue}{\hyperref[Espaces engendrés par les matrices inversibles et orthogonales]{[Enoncé]}}\\
	
	\subsection{Espace engendré par les matrices nilpotentes}
	\label{Espace engendré par les matrices nilpotentes corrigé}
	\textcolor{blue}{\hyperref[Espace engendré par les matrices nilpotentes]{[Enoncé]}}\\
	
	\subsection{Transmission d'information \centraleponts{3}}
	\label{Transmission d'information corrigé}
	\textcolor{blue}{\hyperref[Transmission d'information]{[Enoncé]}}\\
	Supposons que $(A,B)\ne (0,0)$. Alors on peut se donner $x\in \M_{n,1}\backslash\Ker(A)$ et $y\in \text{Im}(B)\backslash\{0\}$. On complète la famille $(y)$ en une base $(y,e_2,\dots,e_n)$ de $\M_{n,1}(\K)$.\\
	On définit la matrice carré $X$ en posant $Xy=x$ et $Xe_i=0$ pour $i\in \crblanc{2}{n}$.\\
	Alors pour $z\in \M_{n,1}(\K)$ telle que $Bz=y$ on a : $AXBz=AXy=Ax\ne 0$. Donc $AXB\ne 0$.
	
	\subsection{Condition nécessaire et suffisante pour que l'image et le noyau soient supplémentaires}
	\label{Condition nécessaire et suffisante pour que l'image et le noyau soient supplémentaires corrigé}
	\textcolor{blue}{\hyperref[Condition nécessaire et suffisante pour que l'image et le noyau soient supplémentaires]{[Enoncé]}}\\
		On montre aisément que : $$\Ker(f)\subset \Ker(f^2) \quad \text{et} \quad \Ima(f^2)\subset\Ima{f}$$
		Montrons que (i)$\implies$(ii)\\
		Soit $y\in\Ima(f)$\\
		IL existe $x\in E$ tel que $f(x)=y$.\\
		D'après (i), il existe $(x_1,x_2)\in\Ima(f)\times\Ker(f)$.\\
		Ainsi $f(x_1)=y$.\\
		Puisque $x_1\in \Ima(f)$, il existe $y'\in E$ tel que $x_1=f(y')$ donc $f^2(y')=y$.\\
		Donc $\Ima(f^2)=\Ima(f)$ par double inclusion.\\
		Montrons que (ii)$\implies$(iii).\\
		D'après le théorème du rang, $\dim(\Ker(f))=n-\dim(\Ima(f))=n-\dim(\Ima(f^2))=\dim(\Ker(f^2))$\\
		Et puisque $\Ker(f)\subset\Ker(f^2)$, on en déduit que : $\Ker(f)=\Ker(f^2)$.\\
		Montrons que (iii)$\implies$(i)\\
		Soit $x\in\Ker(f)\cap\Ima(f)$\\
			$f(x)=0$\\
			$\exists y\in E, f(y)=x$\\
		Ainsi, $f^2(y)=0$. Donc $y\in\Ker(f^2)=\Ker(f)$. Ce qui implique que $f(y)=0$. Donc $x=0$.\\
		Par conséquent, $\Ker(f)$ et $\Ima(f)$ sont en somme directe.\\
		De plus, d'après le théorème du rang : $\dim(\Ima(f))+\dim(\Ker(f))=n$.\\
		Donc $\Ima(f)\oplus\Ker(f)=E$.
	
	\subsection{Equation entre projecteurs}
	\label{Equation entre projecteurs corrigé}
	\textcolor{blue}{\hyperref[Equation entre projecteurs]{[Enoncé]}}\\
	Supposons qu'il existe $p,q,r$ des projecteurs de $E$ vérifiant $\sqrt 2p+\sqrt 3q=r$.\\
	Alors en appliquant la trace $\sqrt 2\Tr(p)+\sqrt 3\Tr(q)=\Tr(r)$. Or on sait que la trace d'un projecteur est son rang (il suffit d'écrire la matrice du projeteur $f$ dans une base adaptée à la décomposition de $E=\ker(f)\bigoplus\ker(f-\Id_E)$). On note $n,m,k$ les rangs respectifs de $p,q,r$.\\
	Si $n=0$ alors $m=0$ (car sinon $\sqrt 3=\dfrac{k}{m}\in \Q$) et par suite $k=0$. Dans ce cas $p,q,r$ sont tous nuls. De même, $m=0\implies p=q=r=0$.\\
	Supposons $n,m$ non nuls et notons $\alpha=\dfrac{m}{n}\ne 0$ et $\beta=\dfrac{k}{n}$. On a\\
	$\sqrt 2+\sqrt 3\alpha=\beta$ d'où $2+2\alpha\sqrt 6+3\alpha^2=\beta^2$ i.e $\sqrt 6=\dfrac{\beta^2-3\alpha^2-2}{2\alpha}\in \Q$ ce qui est absurde.\\
	Ainsi il n'existe pas de triplet $(p,q,r)\ne (0,0,0)$ de projecteurs de $E$ qui vérifient $\sqrt 2p+\sqrt 3q=r$.
	
	\subsection{Théorème de Maschle}
	\label{Théorème de Maschke corrigé}
	\textcolor{blue}{\hyperref[Théorème de Maschke]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $p$ un projecteur.\\
			On sait que $\Ima(p)\bigoplus\Ker(p)=\M_n(\R)$ .\\
			On remarque que la matrice de $p$ dans une base adaptée à $\Ima(p)\bigoplus\Ker(p)$ est $J_r$ où $r=\rg(p)$.\\
			Ainsi, on en déduit ainsi que  : $\rg(p)=\Tr(p)$.
			\item $\varphi_h$ est clairement un application de $G$ dans $G$. Et on remarque que $\varphi_h$ est inversible d'inverse $\varphi_{h^{-1}}$.
		\end{enumerate}
		\item \begin{enumerate}[label=\alph*.]
			\item 
		\begin{align*}
			p^2&=\frac{1}{n^2}\sum_{g\in G}\sum_{h\in G}h\circ g\\
			&=\frac{1}{n^2}\sum_{g\in G}\sum_{h\in G} g &\text{d'après la question 1b}\\
			&=\frac{1}{n^2}n\sum_{g\in G}g\\
			&=p
		\end{align*}
		Donc $p$ est un projecteur.
		\item Par linéarité de la trace, \[\Tr(p)=\frac{1}{n}\sum_{g\in G}\Tr(g)\]
		Montrons que $\Ker(p-Id_E)=\bigcap_{g\in G}\Ker(g-Id_E)$.\\
		On remarque que $\displaystyle p-Id_E=\frac{1}{n}\sum_{g\in G}(g-Id_E)$. Ainsi, \[\bigcap_{g\in G}\Ker(g-Id_E)\subset \Ker(p-Id_E)\]
		Réciproquement soit $x\in \Ima(p)$ et soit $g_0\in G$.
		\[g_0(x)=g_0(p(x))=g_0\left(\frac{1}{n}\sum_{g\in G}g\right)=\frac{1}{n}\sum_{g\in G}g_o\circ g\]
		Or d'après la question 1b, $\varphi_{g_0}$ est une permutation de $G$, alors \[g_0(x)=p(x)=x\]
		Donc \[\Ima(p)=\Ker(p-Id_E)=\bigcap_{g\in G}\Ker(g-Id_E)\]
		Ainsi, d'après la question 1a, on a \[\dim\left(\bigcap_{g\in G}\Ker(g-Id_E)\right)=\frac{1}{n}\sum_{g\in G}\Tr(g)\]
		\end{enumerate}
		\item On pose $\displaystyle p=\frac{1}{n}\sum_{g\in G}{g^{-1}\circ q\circ g}$.\\
		Montrons que $p$ est un projecteur.\\
		Puisque pour tout $g\in G$ stabilise $F$, on en déduit que $p$ est à valeur dans $F$.\\
		Soit $x\in F$. On a $g(x)\in F$ donc $(p(g(x)))=g(x)$.
		Donc \[g^{-1}\circ q\circ g(x)=g^{-1}\circ g(x)=x\]	
		Par conséquent, $p$ agit comme identité sur $F$.\\
		Ainsi, $p$ est un projecteur sur $F$.\\
		Maintenant, montrons que $S=\Ker(p)$ vérifie les propriétés demandées.\\
		D'après les propriétés des projecteurs, \[S\bigoplus F=E\]\\
		Montrons que $S$ est stable par tous les éléments de $G$.\\
		On montre d'abord que pour tout $h\in G$, on a : \[h\circ p\circ h^{-1}=p\]
		\begin{align*}
			h^{-1}\circ p\circ h&=\frac{1}{n}\sum_{g\in G} h^{-1}\circ g^{-1} \circ p\circ g\circ h\\
			&=\frac{1}{n}\sum_{g\in G}g\circ p\circ g^{-1} &\text{d'après la question 1b}\\
			&=p
		\end{align*}
		Ainsi, pour tout $x\in S$ et pour tout $g\in G$ \[p(g(x))=g(p(x))=g(0)=0\]
		Donc $g(x)\in S$.\\
		Donc $S$ est stable par tous les éléments de $G$.
		\end{enumerate}
	
	\newpage
\section{Correction Réduction géométrique}
	\subsection{Réduction des matrices de rang $1$}
	\label{Réduction des matrices de rang 1 corrigé}
	\textcolor{blue}{\hyperref[Réduction des matrices de rang 1]{[Enoncé]}}\\
	\begin{enumerate}
		\item Soit $M\in\M_n(\K)$.\\
		$\begin{array}{ccc}
			& \rg(M)=1\\
			\mbox{ssi} & \exists (x_1,\dots,x_n)\in\K^n\backslash\{0\}, \exists U\in\M_{n,1}(\K)\backslash\{0\}\text{ tels que }M=\left(x_1U|\dots|x_nU\right) \\
			\mbox{ssi} & \exists (U,V)\in(\M_{n,1}(\K)\backslash\{0\})^2\text{ tel que }M=UV^\top
		\end{array}$
		\item D'après la question précédente il existe $(U,V)\in \M_{n,1}(\K)^2$ tel que $M=UV^\top$.\\
		On remarque que $\Tr(M)=V^\top U$. Ainsi on a aisément $M^2=\Tr(M)M$.
		\item Soit $M\in\M_n(\K)$ tq $\rg(M)=1$.\\
		Par la réduction algébrique :\\
		Le polynôme $P=X^2-\Tr(M)X$ annule $M$. Comme $M$ n'est pas une homothétie (qui serait de rang $n$ ou $0$), $P$ est le polynôme minimal de $M$. Il est scindé à racines simples ssi $\Tr(M)\ne 0$.\\
		Ainsi $M$ est diagonalisable ssi $\Tr(M)\ne 0$.\\\\
		Par la réduction géométrique :\\
		D'après le théorème du rang, $\dim(\operatorname{Ker}(M))=n-1$.\\
		On en déduit que la multiplicité algébrique de $0$ est au moins $n-1$. On note $m_0(M)$ cette multiplicité.
		\\Donc $X^{n-1}|\chi_M$ et puisque $\operatorname{deg}(\chi_M)=n$, il existe $\alpha\in\K$ tel que $\chi_M=X^{n-1}(X-\alpha)$.
		\\Notons également que $\alpha=\Tr(M)$ car la trace de $M$ est la somme des valeurs $M$. 
		\begin{itemize}
			\item Si $\Tr(M)=0$, on a $m_0(M)>n-1$ donc $M$ n'est pas diagonalisable.
			\item Si $\Tr(M)\ne 0$, $m_0(M)=n-1$ et $\dim(E_{\Tr(M)}(M))=1=m_{\Tr(M)}(M)$. Donc $M$ est diagonalisable.
		\end{itemize}
		On conclut donc que $M$ est diagonalisable ssi $\Tr(M)\ne0$.
		\item Soit $A\in \M_3(\C)$ telle que $A^3=\begin{pmatrix}
			1 &-2 &1\\
			1 &-2 &1\\
			1 &-2 &1\\
		\end{pmatrix}=:M$
		\\$M$ est de rang $1$ ainsi $M^2=\Tr(M)M=0_3$ car $\Tr(M)=0$.
		\\Donc $A^6=0$.
		\\Ainsi $A$ est nilpotente, donc d'après le théorème de Cayley-Hamilton, $A^3=0_3$.
		\\Finalement il n'y a pas de solution à l'équation : $A^3=M$.
		\begin{enumerate}[label=\alph*.]
			\item Soit $y\in \T{Im}(u)$. Il existe $x\in E$ tel que $y=u(x)$. Alors $u(y)=u^2(x)=\Tr(u)x=0$. Càd $y\in \Ker(u)$.
			\item On en déduit que $\dim(\T{Im}(u)\cap\Ker(u))=\rg(u)=1$. En particulier cet espace n'est pas nul.
			\item Soit alors un vecteur $y\in \T{Im}(u)\cap\Ker(u)$ non nul. Il existe $x\in E$ tel que $y=u(x)$. On complète la famille $(y)$ en une base $(y,e_3,\dots,e_n)$ de $\Ker(u)$. Enfin montrons que la famille $\B=(x,y,e_3,\dots,e_n)$ est une base de $E$. Il suffit de montrer qu'elle est libre.\\
			Si $\alpha,\beta,\gamma_3,\dots,\gamma_n\in \K$ tels que $\alpha x+\beta y+\displaystyle\sum_{k=3}^n\gamma_ke_k=0$ alors en appliquant $u$ on obtient $\alpha u(x)=0$ d'où $\alpha=0$. Puis par liberté de $(y,e_3,\dots,e_n)$, les autres coefficients sont aussi nuls.\\
			La matrice dans la base $\B$ est alors celle recherchée.
		\end{enumerate}
		\item Soient $A,B\in \M_n(\K)$ deux matrices de rang $1$.\\
		Si elles sont semblables elles ont évidemment même trace.\\
		Réciproquement, supposons $\Tr(A)=\Tr(B):=\alpha$.\\
		Si leur trace est nulle alors d'après la question 4 appliquée aux endomorphismes canoniquement associés à $A$ et $B$, $A$ et $B$ sont toutes deux semblables à la matrice de l'énoncé.\\
		Et si leur trace est non nulle alors elles sont semblables à la matrices diagonale $\diag(\alpha,0,\dots,0)$.\\
		Dans les deux cas, $A$ et $B$ sont semblables par transitivité de la relation "être semblable" sur les matrices.
	\end{enumerate}
	
	\subsubsection{Application \ccinp{2}}
	\label{Application corrigé}
	\textcolor{blue}{\hyperref[Application]{[Enoncé]}}\\
	Il est clair que $\varphi$ est linéaire. On remarque que $\T{Im}(\varphi-\Id)\subset\Vect(A)$. De plus comme $A$ n'est pas nulle, $\varphi(I_n)-I_n=A\ne 0$. Donc $\varphi-\Id$ n'est pas nulle et est donc de rang $1$.\\
	On sait alors qu'elle est diagonalisable si et seulement si sa trace est non nulle.\\
	Soient $\lambda\in \K$ et $M\in \M_n(\K)\backslash\{0\}$ telle que $\Tr(M)A=\lambda M$.\\
	En appliquant la trace on obtient $\Tr(M)\Tr(A)=\lambda\Tr(M)$. Donc $\Tr(M)=0$ ou $\lambda=\Tr(A)$.\\
	Si $\Tr(M)=0$ alors $\lambda M=0$ d'où $\lambda=0$ car $M\ne 0$.\\
	Ainsi $\Tr(\varphi-\Id)=\Tr(A)$ et par suite $\varphi-\Id$ est diagonalisable ssi $\Tr(A)\ne 0$.\\
	Pour conclure, $\varphi-\Id$ est diagonalisable ssi $\varphi$ l'est (par la même matrice de passage).
	
	\subsubsection{Dimension du commutant d'une matrice de rang 1 \centraleponts{3}}
	\label{Dimension du commutant d'une matrice de rang 1 corrigé}
	\textcolor{blue}{\hyperref[Dimension du commutant d'une matrice de rang 1]{[Enoncé]}}\\
	Si $n=1$ alors $\mathcal C(A)=\M_1(\K)$. On suppose dans la suite $n\geq 2$.\\
	On distingue deux cas suivant si $\alpha:=\Tr(A)=0$ ou non.\\
	\underline{Si $\Tr(A)\ne 0$ :}\\
	$\exists P\in \T{GL}_n(\C),\ A=PDP^{-1}$ avec $D=\diag(\alpha,0,\dots,0)$.\\
	Alors $\forall B\in \M_n(\K),\ AB=BA\iff PDP^{-1}B=BPDP^{-1}\iff DP^{-1}BP=P^{-1}BPD$.\\
	Or l'application $B\in \M_n(\K)\mapsto PBP^{-1}$ est un isomorphisme donc $\dim\mathcal C(A)=\dim\mathcal C(D)$.\\
	Enfin, si $B$ commute avec $D$, en notant $B=\left(\begin{array}{c|c}
		\beta&L\\
		\hline
		C&B'
	\end{array}\right)$
	avec $\beta\in \K$ et $B'\in \M_{n-1}(\K)$ on obtient :
	$$BD=\left(\begin{array}{c|c}
		\alpha\beta&0\\
		\hline
		\alpha C&0
	\end{array}\right)=\left(\begin{array}{c|c}
	\alpha\beta&\alpha L\\
	\hline
	0&0
	\end{array}\right)=DB$$
	Donc comme $\alpha\ne 0$, $L=0$ et $C=0$.\\
	On en déduit que $\mathcal C(D)$ est isomorphe à $\K\times\M_{n-1}(\K)$, et donc que $\dim\mathcal C(A)=(n-1)^2+1$.\\\\
	\underline{Si $\Tr(A)=0$ :}\\
	Par le même changement de base on se ramène à déterminer la dimension du commutant de $N=\left(\begin{array}{cc|ccc}
		0&0&0&\cdots&0\\
		1&0&0&\cdots&0\\
		\hline
		0&0&&&\\
		\vdots&\vdots&&0_{n-2}&\\
		0&0&&&
	\end{array}\right)$. Notons $N_2=\begin{pmatrix}
	0&0\\
	1&0
	\end{pmatrix}$.\\
	$\forall M=\begin{pmatrix}
		a&b\\
		c&d
	\end{pmatrix}\in \M_2(\K),\ MN_2=N_2M\iff \begin{pmatrix}
	b&0\\
	d&0
	\end{pmatrix}=\begin{pmatrix}
	0&0\\
	a&b
	\end{pmatrix}\iff a=d$ et $b=0$.\\
	Donc $\mathcal C(N)$ est isomorphe à $\left\{\begin{pmatrix}
		a&0\\
		c&a
	\end{pmatrix},\ (a,c)\in \K^2\right\}\times\M_{n-2}(\K)$.\\
	On en déduit $\dim\mathcal C(A)=(n-2)^2+2$.
	
	\subsubsection{Diagonalisabilité d'une matrice \centraleponts{3}}
	\label{Diagonalisabilité d'une matrice corrigé}
	\textcolor{blue}{\hyperref[Diagonalisabilité d'une matrice]{[Enoncé]}}\\
	Pour tout $\lambda\in\R,\quad \chi_M=\det(\diag(\lambda- a_1+b_1,\dots,\lambda-a_n+b_n)-N)$ avec $N=\begin{pmatrix}
		b_1  &b_2   &\dots &b_{n-1}&b_n\\
		b_1   &b_2  &\ddots&\vdots &\vdots\\
		\vdots&b_2   &\ddots&b_{n-1}&b_n\\
		\vdots&\vdots&\ddots&b_{n-1}&b_n\\
		b_1   &b_2   &\dots &b_{n-1}&b_n
	\end{pmatrix}$.
	\\On remarque que $\operatorname{rg}(N)=1$ et que $N=UV^\top=\begin{pmatrix}
		1\\1\\ \vdots \\1
	\end{pmatrix}\begin{pmatrix}
		b_1\\b_2\\\vdots\\b_n
	\end{pmatrix}^\top$. Ainsi, d'après le lemme du déterminant, on a: $$\chi_M(\lambda)=\det(\diag(\lambda-a_1+b_1,\dots,\lambda-a_n+b_n))-V^\top\operatorname{Com}(\diag(\lambda-a_1+b_1,\dots,\lambda-a_n+b_n))U$$
	On choisit $\lambda$ tel que $\diag(\lambda-a_1+b_1,\dots,\lambda-a_n+b_n)$ est inversible. Donc: \begin{align*}
		\chi_M(\lambda)=&\det(\diag(\lambda- a_1+b_1,\dots,\lambda-a_n+b_n))\cdot\left(1-V^\top\diag\left(\left(\frac{1}{\lambda- a_1+b_1},\dots,\frac{1}{\lambda-a_n+b_n}\right)\right)U\right)
		\\&=\prod_{i=1}^n(\lambda-a_i+b_i)\left(1-\sum_{i=1}^n\frac{b_i}{\lambda-a_i+b_i}\right)
		\\&=\prod_{i=1}^n(\lambda-a_i+b_i)-\sum_{i=1}^n b_iP_i(\lambda)
	\end{align*}
	Avec $P_i(\lambda)=\displaystyle\prod_{j\in\crblanc{1}{n}, j\ne i}(\lambda-a_i+b_i)$.
	\\Puisque $a_1\leq\dots\leq a_n$ et $0<b_n<\dots< b_1$, on a $a_1-b_1<\dots<a_n-b_n$.
	\\On a $\chi_M(a_i-b_i)=-b_i\displaystyle\prod_{j\in\crblanc{1}{n}, j\ne i}[(a_i-b_i)-(a_j-b_j)]$.
	\\Ainsi, $\text{sgn}(\chi_M(a_i-b_i))=(-1)^{n+1-i}$ et $\lim\limits_{\lambda\to+\infty}\chi_M(\lambda)=+\infty$.
	\\Par conséquent, d'après le théorème des valeurs intermédiaires, pour tout $i\in\crblanc{1}{n}$, il existe $\mu_i\in]a_i-b_i,a_{i+1}-b_{i+1}[$ tel que $\chi_M(\mu_i)=0$ et il existe $\mu_n\in]a_n-b_n,+\infty[$ tel que $\chi_M(\mu_n)=0$.
	\\On a donc trouvé $n$ racines distinctes de $\chi_M$ qui est de degré $n$. Donc $\chi_M$ est scindé à racines simples et par suite $M$ est diagonalisable.
	
	\subsection{\underline{Matrices réelles semblables} \centraleponts{2}}
	\label{Matrices réelles semblables corrigé}
	\textcolor{blue}{\hyperref[Matrices réelles semblables]{[Enoncé]}}\\
	D'après l'énoncé, il existe $P\in GL_n(\C)$ tel que $A=PBP^{-1}$. On note $P=S+iR$ avec $(S,R)\in\mathcal{M}_n(\R)$.
	Donc $A(S+iR)=(S+iR)B$, et ainsi par unicité de la partie réelle et imaginaire, on a:$\begin{cases}
		AS=SB\\
		AR=BR
	\end{cases}$\\
	De plus, on pose $f:t\mapsto \det(S+tR)$. $f$ est une fonction polynomiale non nulle car $f(i)\ne0$. Donc il existe $\lambda\in\R$ tel que $f(\lambda)\ne0$ et donc $S+\lambda R$ est inversible.
	\\Ainsi comme $AS=SB$ et $AR=RB$, on a $A(S+\lambda R)=(S+\lambda R)B$ d'où le résultat.
	
	\subsection{Complément de Schur \telecom{2}}
	\label{Complément de Schur corrigé}
	\textcolor{blue}{\hyperref[Complément de Schur]{[Enoncé]}}\\
	\begin{enumerate}
		\item On remarque que: $\begin{pmatrix}A &B \\C &D\end{pmatrix}\begin{pmatrix}
			A^{-1}& -A^{-1}B\\ 0 & I_q
		\end{pmatrix}=\begin{pmatrix}
			I_p &0\\CA^{-1} & D-CA^{-1}B
		\end{pmatrix}$.
		Donc $\det(M)\det(A^{-1})=det(S)$ et donc $\det(M)=\det(A)\det(S)$.
		\item Puisque $\begin{pmatrix}
			A^{-1}& -A^{-1}B\\ 0 & I_q \end{pmatrix}$ est inversible. On en déduit que $\operatorname{rg}(M)=\operatorname{rg}\begin{pmatrix}
			I_p &0\\CA^{-1} & D-CA^{-1}B
		\end{pmatrix}$.
		\\Soit $\begin{pmatrix}
			X \\Y 
		\end{pmatrix}\in\text{Ker}\begin{pmatrix}
			I_p &0\\CA^{-1} & D-CA^{-1}B
		\end{pmatrix}$
		On a: $\begin{cases}
			X=0 \\
			CX+SY=0
		\end{cases}$\\
		Par conséquent, $\dim\left(\text{Ker}\begin{pmatrix}
			I_p &0\\CA^{-1} & D-CA^{-1}B
		\end{pmatrix}\right)=\dim(\Ker(S))=q-\operatorname{rg}(S)$ donc $\dim(\Ker(M))=q-\operatorname{rg}(S)$.
		Finalement, $\operatorname{rg}(M)=p+q-\dim(\Ker(M))=p+\operatorname{rg}(S)=\operatorname{rg}(A)+\operatorname{rg}(S)$.
	\end{enumerate}
	
	\subsection{Produit de Kronecker \ccinp{2}}
	\label{Produit de Kronecker corrigé}
	\textcolor{blue}{\hyperref[Produit de Kronecker]{[Enoncé]}}\\
	\begin{enumerate}
		\item On a $A \otimes B = \begin{pmatrix}
			a_{11}B & a_{12}B \\
			a_{21}B & a_{22}B
		\end{pmatrix}$ et $C \otimes D = \begin{pmatrix}
			c_{11}D & c_{12}D \\
			c_{21}D & c_{22}D
		\end{pmatrix}$. Un calcul par blocs donne
		$$(A \otimes B) \cdot (C \otimes D) = \begin{pmatrix}
			(a_{11}c_{11} + a_{12}c_{21})BD & (a_{11}c_{12} + a_{12}c_{22})BD \\
			(a_{21}c_{11} + a_{22}c_{21})BD & (a_{21}c_{12} + a_{22}c_{22})BD
		\end{pmatrix} = \begin{pmatrix}
			ac_{11}BD & ac_{12}BD \\
			ac_{21}BD & ac_{22}BD
		\end{pmatrix} = (AC) \otimes (BD)$$
		en notant $ac_{ij}$ le coefficient en position $(i, j)$ de la matrice $AC$.
		\item $I_2 \otimes B = \begin{pmatrix}
			B & 0_2 \\
			0_2 & B
		\end{pmatrix}$ donc $\det(I_2 \otimes B) = (\det B)^2$.
		
		Soit $u$ l'endomorphisme de $\K^4$ canoniquement associé à $A \otimes I_2$. Notons $(e_1, e_2, e_3, e_4)$ la base canonique de $\K^4$. Alors la matrice de $u$ dans la base $(e_1, e_3, e_2, e_4)$ est $I_2 \otimes A$. On a donc $\det(A \otimes I_2) = \det u = (\det A)^2$ d'après ce qui précède. D'après la première question, $A \otimes B = (A \otimes I_2) \cdot (I_2 \otimes B)$. Ainsi $\det(A \otimes B) = \det(A)^2 \det(B)^2$.
		\item Puisqu'une matrice est inversible si et seulement si son déterminant est non nul, d'après la question précédente, $A\otimes B$ est inversible si et seulement si $A$ et $B$ le sont. Dans ce cas, on a d'après la première question
		$$(A \otimes B) \cdot (A^{-1} \otimes B^{-1}) = (AA^{-1}) \otimes (BB^{-1}) = I_2 \otimes I_2 = I_4$$
		Càd $(A\otimes B)^{-1}=A^{-1}\otimes B^{-1}$.
		\item D'après l'énoncé, il existe $P,Q \in \T{GL}_n(\K)$ et $D_A , D_B \in \M_n(\K)$ diagonales telles que $A=PD_AP^{-1}$ et $B=QD_BQ^{-1}$.\\
		On montre aisément que : $$A\otimes B=(P\otimes Q)(D_A\otimes D_B)(P\otimes Q)^{-1}$$
		Ce qui justifie que $A\otimes B$ est diagonalisable.	 
	\end{enumerate}

	\subsection{\underline{Disques de Gershgorin} \telecom{2}}
	\label{Disques de Gershgorin corrigé}
	\textcolor{blue}{\hyperref[Disques de Gershgorin]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
			\item Soit $\lambda\in\text{Sp}(A)$ et $X=\begin{pmatrix}
			x_1\\ \vdots \\ x_n
				\end{pmatrix}$ un vecteur propre de $A$ associé à $\lambda$.
			On a pour tout $i\in\crblanc{1}{n}$, \[(\lambda-a_{i,i})x_i=\sum_{j\ne i}a_{i,j}x_j\]
			On choisit un $i_0$ pour lequel $|x_{i_0}|$ est maximal ($|x_{i_0}|>0$ car $X$ est non nul). Ainsi, 
			\begin{align*}
				|\lambda-a_{i_0,i_0}|&=\left|\sum_{j\ne i_0}a_{i_0,j}\frac{x_j}{x_{i_0}}\right|\\
				&\leq\sum_{j\ne i_0}|a_{i_0,j}|\frac{|x_j|}{|x_{i_0}|}\\
				&\leq\sum_{j\ne i_0}|a_{i_0,j}|&\mbox{car $|x_{i_0}|$ est maximal}\\
				&=R_{i_0}
			\end{align*}
			Par conséquent, on a $\lambda \in D(a_{i_0,i_0},R_{i_0})$.
			\item Soit $i\in\crblanc{1}{n}$.\\
			\[|0-a_{i,i}|=|a_{i,i}|>R_i\]
			donc $0\not\in D(a_{i,i},R_i)$.\\
			Par conséquent, $0$ ne peut être une valeur propre de $A$, donc $A$ est  inversible.
	\end{enumerate}
	
	\subsection{Spectre de $u\circ v$ et $v\circ u$ \ccinp{1}}
	\label{Spectre de uov et vou corrigé}
	\textcolor{blue}{\hyperref[Spectre de uov et vou]{[Enoncé]}}\\
	Montrons que $\text{Sp}(u\circ v)\subset \text{Sp}(v\circ u)$. (L'autre inclusion se montrera de la même façon par inversion des rôles de $u$ et $v$).\\
	Soit $\lambda\in\text{Sp}(u\circ v)$ non nul.\\
	On prend $x$ un vecteur propre de $u\circ v$ associé à $\lambda$.\\
	On a : \[u\circ v(x)=\lambda x\]
	Par conséquent \[v\circ u\circ v (x)=\lambda v(x)\]
	Montrons que $v(x)$ ne peut être nul.\\
	Raisonnons par l'absurde que $v(x)=0$.\\
	\[0=u(0)=u\circ v(x)=\lambda x \]
	Puisque $\lambda\ne0$, $x=0$ ce qui est absurde car $x$ est un vecteur propre.\\
	Donc $v(x)$ est un vecteur propre de $v\circ u$ associé à $\lambda$.\\
	Il reste à montrer le cas quand $0$ est valeur propre de $u\circ v$.\\
	\begin{align*}
		0\in \text{Sp}(u\circ v)&\Leftrightarrow \det(u\circ v)=0\\
		&\Leftrightarrow \det(v\circ u)=0\\
		&\Leftrightarrow 0\in \text{Sp}(v\circ u)
	\end{align*}
	
	\subsection{\underline{Endomorphismes qui commutent} \ccinp{2}}
	\label{Endomorphismes qui commutent corrigé}
	\textcolor{blue}{\hyperref[Endomorphismes qui commutent]{[Enoncé]}}\\
	Supposons que $u$ et $v$ commutent.\\
	Alors $E_\lambda(u)$ est stable par $v$.\\
	Donc $v$ induit un endomorphisme $v_{E_\lambda(u)}$ sur $E_\lambda(u)$.\\
	$v_{E_\lambda(u)}$ admet un vecteur propre car $E_\lambda(u)$ est un $\C$-espace vectoriel $(\chi_{v_{E\lambda(u)}})$ est scindé).\\
	Ce vecteur propre est un vecteur propre de $v$ et un vecteur propre de $u$ par définition de $E_\lambda(u)$, donc $u$ et $v$ admettent un vecteur propre commun.
	
	\subsection{Vecteur propre commun \centraleponts{3}}
	\label{Vecteur propre commun corrigé}
	\textcolor{blue}{\hyperref[Vecteur propre commun]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $\Ima(v)\subset \Ker(u)$ donc $v(\Ker(u))\subset\Ima(v)\subset\Ker(u)$. $v$ induit donc un endomorphisme $\tilde v$ sur $\Ker(u)$. $\Ker(u)$ étant un $\C$-espace vectoriel, $\chi_{\tilde v}$ est scindé et donc $\tilde v$ admet un vecteur propre. Ce vecteur propre est vecteur propre de $v$ et vecteur propre de $u$ en tant qu'élément non nul de son noyau.
		\item On a $u\circ(v-a\Id_E)=0$ donc d'après la question 1 $u$ et $v-a\Id_E$ admettent un vecteur propre commun que l'on note $x$. Notons $\lambda\in \C$ tel que $(v-a\Id_E)(x)=\lambda x$. Alors $x$ est vecteur propre de $v$ pour la valeur propre $\lambda+a$.
		\item De même $(u-b\Id_E)\circ v=0$ donne le résultat.
		\item $u$ est l'inverse de $v$. En particulier ils commutent et admettent donc un vecteur propre commun (cf.\ref{Vecteur propre commun corrigé}).
		\item On a $(u-b\Id_E)\circ (v-a\Id_E)=ab\Id_E$.\\
		Si $ab=0$ alors on est dans le cas de la question 1 et sinon on est dans le cas de la question 4. Dans tous les cas $u-b\Id_E$ et $v-a\Id_E$ ont un vecteur propre commun $x$. On montre aisément que $x$ est un vecteur propre commun à $u$ et $v$.
	\end{enumerate}
	
	\subsection{Eléments propres d'un endomorphisme (1)}
	\label{Eléments propres d'un endomorphismes (1) corrigé}
	\textcolor{blue}{\hyperref[Eléments propres d'un endomorphismes (1)]{[Enoncé]}}\\
	
	
	\subsection{Eléments propres d'un endomorphisme (2)}
	\label{Eléments propres d'un endomorphisme (2) corrigé}
	\textcolor{blue}{\hyperref[Eléments propres d'un endomorphismes (2)]{[Enoncé]}}\\
	
	\subsection{Eléments propres d'un endomorphisme (3)}
	\label{Eléments propres d'un endomorphisme (3) corrigé}
	\textcolor{blue}{\hyperref[Eléments propres d'un endomorphismes (3)]{[Enoncé]}}\\
	
	\subsection{Eléments propres d'un endomorphisme (4)}
	\label{Eléments propres d'un endomorphisme (4) corrigé}
	\textcolor{blue}{\hyperref[Eléments propres d'un endomorphismes (4)]{[Enoncé]}}\\
	
	\subsection{Loi de Hooke}
	\label{Loi de Hooke corrigé}
	\textcolor{blue}{\hyperref[Loi de Hooke]{[Enoncé]}}\\
	
	\subsection{Existence d'une valeur propre double}
	\label{Existence d'une valeur propre double corrigé}
	\textcolor{blue}{\hyperref[Existence d'une valeur propre double]{[Enoncé]}}\\
	
	\subsection{Détermination de spectre}
	\label{Détermination de spectre corrigé}
	\textcolor{blue}{\hyperref[Détermination de spectre]{[Enoncé]}}\\
	
	\subsection{Sommes et produits de valeurs propres}
	\label{Sommes et produits de valeurs propres corrigé}
	\textcolor{blue}{\hyperref[Sommes et produits de valeurs propres]{[Enoncé]}}\\
	
	\subsection{Matrice compagnon (1)}
	\label{Matrice compagnon (1) corrigé}
	\textcolor{blue}{\hyperref[Matrice compagnon (1)]{[Enoncé]}}\\
	Vous pouvez espérer que dire : "On développe par rapport à la dernière colonne pour en déduire le résultat" sans justifier les calculs suffira à convaincre le correcteur, mais il y a de fortes chances qu'il n'apprécie pas.\\ On va donc montrer de façon rigoureuse que : \[C_P=P\]
	Pour visualiser un peu, écrivons le polynôme caractéristique sous forme de déterminant : 
	\[C_P(X)=\begin{vmatrix}
		X&0&\dots&\dots& \dots&0&a_0\\
		-1&X&0 &\dots&\dots &0&a_1\\
		0&\ddots&\ddots&\ddots&&\vdots&a_2\\
		\vdots&\ddots&\ddots&\ddots&\ddots&\vdots&\vdots\\
		\vdots&&\ddots&\ddots&\ddots&0&\vdots\\
		\vdots&&&\ddots&\ddots&X&a_{n-2}\\
		0&\dots&\dots&\dots&0&-1&X-a_{n-1}
	\end{vmatrix}\]
	On procède aux opérations suivantes  : \[L_1\leftarrow L_1+X^iL_i\]
	Ainsi, on a : \[C_P(X)=\begin{vmatrix}
		0&0&\dots&\dots& \dots&0&P(X)\\
		-1&X&0 &\dots&\dots &0&a_1\\
		0&\ddots&\ddots&\ddots&&\vdots&a_2\\
		\vdots&\ddots&\ddots&\ddots&\ddots&\vdots&\vdots\\
		\vdots&&\ddots&\ddots&\ddots&0&\vdots\\
		\vdots&&&\ddots&\ddots&X&a_{n-2}\\
		0&\dots&\dots&\dots&0&-1&X-a_{n-1}
	\end{vmatrix}\]
	En développant par rapport à la première ligne, on a : 
	\[C_P(X)=P(X)(-1)^{n+1}\begin{vmatrix}
		-1&X&0 &\dots&\dots &0\\
		0&\ddots&\ddots&\ddots&&\vdots\\
		\vdots&\ddots&\ddots&\ddots&\ddots&\vdots\\
		\vdots&&\ddots&\ddots&\ddots&0\\
		\vdots&&&\ddots&\ddots&X\\
		0&\dots&\dots&\dots&0&-1
	\end{vmatrix}\]
	Ainsi, on a montré que $C_P=P$.
	\subsection{Réduction de la transposée d'une matrice}
	\label{Réduction de la transposée d'une matrice corrigé}
	\textcolor{blue}{\hyperref[Réduction de la transposée d'une matrice]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On sait que pour tout $M\in\M_n(\K), \det(M)=\det(M^\top)$.\\
		Ainsi, \[\chi_A=\det(XI_n-A)=\det((XI_n-A)^\top)=\det(XI_n-A^\top)=\chi_{A^\top}\]
		\item On sait que pour tout $M\in\M_{n}(\K), \rg(M)=\rg(M^\top)$.\\
		Ainsi, d'après le théorème du rang : \[\dim(E_\lambda(A))=\dim(\Ker(A-\lambda I_n))=\dim(\Ker(A^\top-\lambda I_n))=\dim(E_\lambda I_n)\]
		\item Supposons que $A$ est diagonalisable. Alors \[\displaystyle n=\sum_{\lambda\in\text{Sp}(A)}\dim(E_\lambda(A))\].\\
		Ainsi, à l'aide de la question précédente, on montre aisément que \[\displaystyle n=\sum_{\lambda\in\text{Sp}(A^\top)}\dim(E_\lambda(A^\top))\].\\
		Donc $A^\top$ est diagonalisable.\\
		Puisque $(A^\top)^\top=A$, la réciproque est immédiate.
	\end{enumerate}
	\subsection{Algorithme de Faddeev}
	\label{Algorithme de Faddeev corrigé}
	\textcolor{blue}{\hyperref[Algorithme de Faddeev]{[Enoncé]}}\\
	
	\subsection{Endomorphisme de transposition}
	\label{Endomorphisme de transposition corrigé}
	\textcolor{blue}{\hyperref[Endomorphisme de transposition]{[Enoncé]}}\\
	On prend la base canonique $(E_{i,j})$ de $\M_{n}(\R)$.\\
	On remarque que : \[\forall (i,j)\in(\crblanc{1}{n})^2, \Phi(E_{i,j})=E_{j,i}\]
	Ainsi, \begin{itemize}
		\item les matrices $E_{i,i}$ sont les vecteurs propres de $\Phi$,
		\item pour chaque paire $(i,j)$ avec $i<j$, l'espace engendré par $E_{i,j},E_{j,i}$ est stable par $\Phi$.\\
		Dans cette base, $\Phi$ a pour matrice 
		\[\begin{pmatrix}
			0&1\\
			1&0
		\end{pmatrix}\]
		dont les valeurs propres sont $-1$ et $1$.
	\end{itemize}
	Par conséquent, puisque $\displaystyle\det(\Phi)=\prod_{\lambda\in\text{Sp}(\Phi)}\lambda$ et $\displaystyle\Tr(\Phi)=\sum_{\lambda\in\text{Sp}(\Phi)}\lambda$, on a :
	\[\det(\Phi)=(-1)^{\frac{n(n-1)}{2}} \text{ et } \Tr(\Phi)=\frac{n(n+1)}{2}+\frac{n(n-1)}{2}=n\]
	
	\subsection{Exemple de matrice non diagonalisable}
	\label{Exemple de matrice non diagonalisable corrigé}
	\textcolor{blue}{\hyperref[Exemple de matrice non diagonalisable]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item La matrice $\begin{pmatrix}
			1&1\\0&1
		\end{pmatrix}$ n'est pas diagonalisable sur $\C$.
		\item La matrice $\begin{pmatrix}
			0&1\\-1&0
		\end{pmatrix}$ est diagonalisable sur $\C$ mais pas sur $\R$.
		\item On pose $A=\begin{pmatrix}
			1&1\\1&1
		\end{pmatrix}$ et $B=\begin{pmatrix}
		0&1\\-1&0
		\end{pmatrix}$.\\ Ces deux matrices sont diagonalisables et pourtant : \[A+B=\begin{pmatrix}
		1&2\\0&1
		\end{pmatrix}\]
		n'est pas diagonalisable.
		\item \[AB=\begin{pmatrix}
			-1&1\\
			-1&1
		\end{pmatrix}\]
		n'est pas diagonalisable.
	\end{enumerate}
	\subsection{Diagonalisabilité d'une matrice (1)}
	\label{Diagonalisabilité d'une matrice (1) corrigé}
	\textcolor{blue}{\hyperref[Diagonalisabité d'une matrice (1)]{[Enoncé]}}\\
	Il est vrai que la matrice est symétrique mais pas forcément à coefficients réelles.\\
	On calcule le polynôme caractéristique de $A$.\\
	En développant par rapport à la première ligne, on obtient :
	\[\chi_A=X^{n-2}(X^2-a_nX-S)\]
	où $S=\sum_{k=0}^{n-1}a_k^2$.\\
	On pose $Q=X^2-a_nX-S$.
	$A$ est diagonalisable si et seulement si son polynôme minimal est scindé à racine simples.\\
	Ici, $A$ est diagonalisable si et seulement si :
	\begin{itemize}
		\item $a_1=a_2=\dots=a_{n-1}=0$, dans ce cas $S=0$ et la matrice est diagonale donc a fortiori diagonalisable.
		\item Au moins un $a_i\ne 0$ pour $i<n$. Dans ce cas, le rang de $A$ est au moins $1$ et la dimension du noyau égale à $n-2$. Pour que $A$ soit diagonalisable, il faut que :
		\begin{itemize}
			\item $S\ne 0$ pour que la valeur propre $0$ ne soit pas racine de $Q$.
			\item Le discriminant de $Q$ soit non nulle. Pour que les deux racines de $Q$ soit distinctes.
		\end{itemize}
	\end{itemize}
	Ainsi, $A$ est diagonalisable si et seulement si :
	\begin{itemize}
		\item $a_1=\dots=a_{n-1}=0$,
		\item ou $S\ne 0$ et $a_n^2+4S\ne 0$
	\end{itemize}
	\subsection{Diagonalisabilité d'une matrice (2)}
	\label{Diagonalisabilité d'une matrice (2) corrigé}
	\textcolor{blue}{\hyperref[Diagonalisabité d'une matrice (2)]{[Enoncé]}}\\
	
	\subsection{Diagonalisibilité d'une matrice (3)}
	\label{Diagonalisabilité d'une matrice (3) corrigé}
	\textcolor{blue}{\hyperref[Diagonalisabité d'une matrice (3)]{[Enoncé]}}\\
	
	
	\subsection{Cotrigonalisation (1)}
	\label{Cotrigonalisation (1) corrigé}
	\textcolor{blue}{\hyperref[Cotrigonalisation (1)]{[Enoncé]}}\\
	
	\subsection{Cotrigonalisation (2)}
	\label{Cotrigonalisation (2) corrigé}
	\textcolor{blue}{\hyperref[Cotrigonalisation (2)]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soient $\Phi_1,\Phi_2\in E^*$ et $\lambda\in \C$.\\
		\[T_u(\Phi_1+\lambda\Phi_2)=(\Phi_1+\lambda\Phi_2)\circ u=\Phi_1\circ u +\lambda\Phi_2\circ u=T_u(\Phi_1)+\lambda T_u(\Phi_2)\]
		Donc $T_u\in\mathcal{L}(E^*)$.
		\item Montrons que si $u$ et $v$ commutent alors $T_u$ et $T_v$ commutent.
		Supposons que $u$ et $v$ commutent.
		Pour tout $\Phi\in E^*$, \[T_u\circ T_v(\Phi)=T_u(\Phi\circ v)=(\Phi\circ v)\circ u=(\Phi\circ u)\circ v=T_v(\Phi\circ u)=T_v\circ T_u(\Phi)\]
		Donc $T_u$ et $T_v$ commutent.
		\item Raisonnons par récurrence. \\
		Le résultat est immédiat quand $n=1$.\\
		Supposons que le résultat est vrai pour un certain $n\in \N$.
		Soit $E$ un $\C$-espace vectoriel de dimension $n+1$.\\
		Puisque $u$ et $v$ commutent, $T_u$ et $T_v$ commutent d'après la question précédente.\\
		Puisque $T_u$ et $T_v$ commutent, ils admettent un vecteur propre commun.\\
		En effet, puisque $T_u$ et $T_v$ commutent, les sous-espaces vectoriels $T_u$ sont stables par $T_v$, donc pour $\lambda\in \text{Sp}(T_u)$, $T_v$ induit un endomorphisme sur $E_\lambda(T_u)$ donc $T_v$ admet un vecteur propre sur $E_\lambda(T_u)$.\\
		On note ce vecteur propre $\Phi$.\\
		Il existe alors $\lambda,\mu\in \C$ tel que: \[\begin{cases}
			T_u(\Phi)=\lambda\Phi\\
			T_v(\Phi)=\mu\Phi
		\end{cases}\]
		On pose $H=\Ker(\Phi)$.\\
		On sait que $H$ est un hyperplan de $E$, donc $\dim(H)=n$.
		Montrons que $H$ est stable par $u$ et par $v$.\\
		Soit $x\in H$.\\
		On a alors $T_u(\Phi)(x)=\lambda\Phi(x)=0$ i.e. $\Phi\circ u(x)=0$.\\
		Donc $u(x)\in H$.\\
		De même, on a $v(x)\in H$.\\
		Donc $u$ et $v$ induisent un endomorphisme sur $H$ ; $u_{|H}$ et $v_{|H}$ sont des endomorphismes qui commutent.\\
		Ainsi, il existe une base $\B$ de $H$ telle que $\Mat_\B(u_{|H})$ et $\Mat_\B(v_{|H})$ sont triangulaires.\\
		Ainsi, il suffit de compléter $\B$ en une base de $E$ pour avoir le résultat souhaité.
	\end{enumerate}
	\subsection{Cotrigonalisation (3)}
	\label{Cotrigonalisation (3) corrigé}
	\textcolor{blue}{\hyperref[Cotrigonalisation (3)]{[Enoncé]}}\\
	
	\subsection{Cotrigonalisation (4)}
	\label{Cotrigonalisation (4) corrigé}
	\textcolor{blue}{\hyperref[Cotrigonalisation (4)]{[Enoncé]}}\\
	
	\subsection{Cotrigonalisation (5)}
	\label{Cotrigonalisation (5) corrigé}
	\textcolor{blue}{\hyperref[Cotrigonalisation (5)]{[Enoncé]}}\\
	
	\subsection{Caractérisation des matrices nilpotentes par la trace}
	\label{Caractérisation des matrices nilpotente par la trace corrigé}
	\textcolor{blue}{\hyperref[Caractérisation des matrices nilpotente par la trace]{[Enoncé]}}\\
	Il est clair que si $A$ est nilpotente alors $\forall k\in\N^*, \Tr(A^k)=0$.\\
	Supposons que $\forall k\in \N^* \Tr(A^k)=0$.\\
	On note $\lambda_1,\dots,\lambda_r$ les valeurs propres non nuls et distinctes de $A$ de multiplicités respectives $m_1,\dots, m_r$.\\
	On a $A$ est semblable à une matrice triangulaire de coefficients diagonaux $$(0,\dots,0,\lambda_1,\dots, \lambda_1,\dots,\lambda_r,\dots,\lambda_r)$$,\\
	et on a également $A^k$ est semblable à une matrice triangulaire de coefficients diagonaux $$(0,\dots,0,\lambda_1^k,\dots, \lambda_1^k,\dots,\lambda_r^k,\dots,\lambda_r^k)$$
	Par conséquent pour tout $k\in \N^*$, 
	et donc : \[\Tr(A^k)=\sum_{i=1}^rm_i\lambda_i^k=0\]
	On peut réécrire ses équations par un produit matriciel : 
	\[\begin{pmatrix}
		\lambda_1&\lambda_1^2&\dots&\lambda_1^r\\
		\lambda_2&\lambda_2^2&\dots&\lambda_2^r\\
		\vdots&\vdots&&\vdots\\
		\lambda_r&\lambda_r^2&\dots&\lambda_r^r\\
	\end{pmatrix}\begin{pmatrix}
	m_1\\m_2\\\vdots\\m_r
	\end{pmatrix}=\begin{pmatrix}
	0\\0\\ \vdots \\0
	\end{pmatrix}\]
	On remarque que : \[[\begin{vmatrix}
		\lambda_1&\lambda_1^2&\dots&\lambda_1^r\\
		\lambda_2&\lambda_2^2&\dots&\lambda2^r\\
		\vdots&\vdots&&\vdots\\
		\lambda_r&\lambda_r^2&\dots&\lambda_r^r\\
	\end{vmatrix}= V(\lambda_1,\lambda_2,\dots,\lambda_r)\prod_{i=1}^{r}\lambda_i\]
	où $V(\lambda_1,\lambda_2,\dots, \lambda_r)$ est le déterminant de Vandermonde des $\lambda_1,\lambda_2,\dots,\lambda_r$.\\
	Puisque les $\lambda_i$ sont distincts et non nuls, on en déduit que \[\begin{pmatrix}
		\lambda_1&\lambda_1^2&\dots&\lambda_1^r\\
		\lambda_2&\lambda_2^2&\dots&\lambda2^r\\
		\vdots&\vdots&&\vdots\\
		\lambda_r&\lambda_r^2&\dots&\lambda_r^r\\
	\end{pmatrix}\] est inversible \ref{Déterminant de Vandermonde} par conséquent, on a : \[\begin{pmatrix}
	m_1\\m_2\\\vdots\\m_r
	\end{pmatrix}=\begin{pmatrix}
	0\\0\\ \vdots \\0
	\end{pmatrix}\]
	Donc $A$ admet une unique valeur propre qui est $0$.\\
	A fortiori, $A$ est nilpotente.
	
	\subsection{Facteur commun dans le polynôme caractéristique}
	\label{Facteur commun dans le polynôme caractéristique corrigé}
	\textcolor{blue}{\hyperref[Facteur commun dans le polynôme caractéristique]{[Enoncé]}}\\
	
	\subsection{$\chi_{AB}=\chi_{BA}$}
	\label{chiAB=chiBA corrigé}
	\textcolor{blue}{\hyperref[chiAB=chiBA]{[Enoncé}}\\
	On remarque que : 
	\[\begin{pmatrix}
		\lambda I_n& -A\\
		-B&I_p
	\end{pmatrix}=\begin{pmatrix}
	I_n&0\\B&I_p
	\end{pmatrix}=\begin{pmatrix}
	\lambda I_n-AB&-A\\0&I_p-BA
	\end{pmatrix}\]
	Ainsi, en regardant les déterminants, on a : 
	\[\begin{vmatrix}
		\lambda I_n& -A\\
		-B&I_p
	\end{vmatrix}=\chi_{AB}(\lambda)\]
	Remarquons maintenant que : 
	\[\begin{pmatrix}
		I_n&0\\B&\lambda I_p
	\end{pmatrix}\begin{pmatrix}
	\lambda I_n&-A\\-B&I_p\end{pmatrix}=\begin{pmatrix}
	\lambda I_n&-A\\0&I_p-BA
	\end{pmatrix}\]
	De même, en considérant les déterminants, on a :
	\[\lambda^p\begin{vmatrix}
		\lambda I_n&-A\\-B&I_p
	\end{vmatrix}=\lambda^n\chi_{BA}(\lambda)\]
	Finalement, on obtient :
	\[\lambda^p\chi_{AB}(\lambda)=\lambda^n\chi_{BA}(\lambda)\]
	Ceci étant vrai pour tout $\lambda\in\K$, \[X^p\chi_{AB}=X^n\chi_{BA}\]
	Et par intégrité de $\K[X]$, quand $n=p$, on a \[\chi_{AB}=\chi_{BA}\] 
	\subsection{Polynôme caractéristique de l'inverse}
	\label{Polynôme caractéristique de l'inverse corrigé}
	\textcolor{blue}{\hyperref[Polynôme caractérisque de l'inverse]{[Enoncé]}}\\
	Pour tout $x\ne 0$,
	\[\chi_{A^{-1}}(x)=\det(xI_n-A^{-1})=\det(A^{-1})\det(xA-I_n)=\frac{(-x)^n}{\det(A)}\det(\frac{1}{x}I_n-A)\]
	Or $\det(A)=(-1)^n\chi_A(0)$, donc \[\chi_{A^{-1}}(x)=\frac{x^n}{\chi_A(0)}\chi_A\left(\frac{1}{x}\right)\]
	Cette expression défini bien un polynôme unitaire (donc on montre qu'elle est bien défini en 0) car 
	\begin{align*}
		x^n\chi_A(\frac{1}{x})&=x^n\sum_{i=0}^{n}a_i\frac{1}{x^i}\\
		&=\sum_{i=0}^{n}a_{n-i}x^i
	\end{align*}
	Donc $\chi_{A^{-1}}$ est un polynôme de coefficient dominant $\displaystyle\frac{a_0}{\chi_A(0)}=1$.\\
	
	\subsection{Commutativité et stabilité}
	\label{Communtativité et stabilité corrigé}
	\textcolor{blue}{\hyperref[Commutativité et stabilité]{[Enoncé]}}\\
	
	\subsection{Dimension du commutant d'une matrice diagonalisable}
	\label{Dimension du commutant d'une matrice diagonalisable corrigé}
	\textcolor{blue}{\hyperref[Dimension du commutant d'une matrice diagonalisable]{[Enoncé]}}\\
	
	\subsection{Sous-espaces stables d'un endomorphisme diagonalisable}
	\label{Sous-espaces stables d'un endomorphisme diagonalisable corrigé}
	\label{dz stable}
	\textcolor{blue}{\hyperref[Sous-espaces stables d'un endomorphisme diagonalisable]{[Enoncé]}}\\
	
	\subsection{Caractérisation de la diagonalisabilité par les hyperplans}
	\label{Caractérisation de la diagonalisabilité par les hyperplans corrigé}
	\textcolor{blue}{\hyperref[Caractérisation de la diagonalisabilité par les hyperplans]{[Enoncé]}}\\
	Supposons que $f$ est diagonalisable. Alors il existe une base $(e_1,\dots,e_n)$ de $E$ formée de vecteurs propres de $f$. On note pour $i\in \crblanc{1}{n},\ H_i=\Vect(e_1,\dots,e_{i-1},e_{i+1},\dots,e_n)$. Ce sont des hyperplans stables par $f$.\\
	De plus, si $x\in \displaystyle\bigcap_{i=1}^nH_i$ alors en écrivant $x=\displaystyle\sum_{k=1}^na_ke_k$, $x\in H_i\implies a_i=0$. D'où $x=0$.\\
	Réciproquement, supposons qu'il existe des hyperplans $H_1,\dots,H_n$ stables par $f$ et d'intersection nulle.\\
	On sait d'après le cours de MPSI que la dimension de l'intersection d'un sev $F$ et d'un hyperplan $H$ de $E$ est de dimension $\dim(F)$ ou $\dim(F)-1$, suivant si $F\subset H$ ou pas (cela découle du fait que $F+H=H$ ou $F+H=E$).\\
	Donc $\displaystyle\dim\left(\bigcap_{i=1}^nH_i\right)=0\implies \forall i\in \crblanc{1}{n},\ D_i:=\bigcap_{\substack{1\leq j\leq n\\j\ne i}}H_j$ est de dimension $1$.\\
	On se donne pour chaque $i$ un vecteur non nul $e_i\in D_i$.\\
	Comme les hyperplans sont stables par $f$, $D_i$ aussi càd $e_i$ est un vecteur propre de $f$.\\
	Soit $(a_1,\dots,a_n)\in \K^n$ tels que $\displaystyle\sum_{i=1}^na_ie_i=0$.\\
	Alors $\forall i\in \crblanc{1}{n},\ \lambda_ie_i=-\displaystyle\sum_{\substack{1\leq j\leq n\\j\ne i}}a_je_j\in D_i\cap H_i=\{0\}$.\\
	Ainsi $(e_1,\dots,e_n)$ est une base de $E$ formée de vecteurs propres de $f$ et par suite $f$ est diagonalisable.
	
	\subsection{Hyperplans stables}
	\label{Hyperplans stables corrigé}
	\textcolor{blue}{\hyperref[Hyperplans stables]{[Enoncé]}}\\
	
	
	\subsection{Sous-espaces stables d'un endomorphisme à spectre simple \centraleponts{3}}
	\label{Sous-espaces stables d'un endomorphisme à spectre simple corrigé}
	\textcolor{blue}{\hyperref[Sous-espaces stables d'un endomorphisme à spectre simple]{[Enoncé]}}\\
	On pose $\text{Sp}(u)=\{\lambda_1,\dots,\lambda_n\}$.\\
	Et on note $E_1,\dots,E_n$ les sous-espaces propres de $u$ respectivement associés à $\lambda_1,\dots,\lambda_n$. 
	On remarque que pour toute partie $I$ de $\crblanc{1}{n}$, $\displaystyle\bigoplus_{i\in I}E_i$ est stable par $u$.
	\\Ainsi, il existe au moins $2^n$ sous-espaces vectoriels stables par $u$.
	D'après l'exercice \ref{dz stable}, pour tout sous-espace vectoriel $F$ stable par $u$, on a: $F=\displaystyle\bigoplus_{i=1}^n{F\cap E_i}$. Or pour tout $i\in \crblanc{1}{n},\ F\cap E_i$ est un sous-espace vectoriel de $E_i$.
	\\Donc puisque $\dim(E_i)=1$, on a $F\cap E_i=\{0\}$ ou $F\cap E_i=E_i$ pour tout $i\in \crblanc{1}{n}$.
	\\Par conséquent, $F$ est une somme de sous-espaces propres.
	\\Ainsi, les seuls espaces vectoriels stables par $u$ sont de la forme $\displaystyle\bigoplus_{i\in I}E_i$ avec $I$ une partie de $\crblanc{1}{n}$. Il y a donc $2^n$ espaces vectoriels stables par $u$.
	
	\subsection{Sous-espaces stables d'un endomorphisme nilpotent maximal \centraleponts{3}}
	\label{Sous-espaces stables d'un endomorphisme nilpotent maximal corrigé}
	\textcolor{blue}{\hyperref[Sous-espaces stables d'un endomorphsime nilpotent maximal]{[Enoncé]}}\\
	On montre classiquement qu'il existe $x\in E$ tel que $(x,u(x),\dots,u^{n-1}(x))$ est une base de $E$.
	\\On remarque que $\{0\}\subset \ker(u)\subset\dots\subset\ker(u^{n-1})\subset E$.
	\\Montrons que: $\forall p\in\crblanc{1}{n}, \dim(\ker(u^p))=p$.
	Puisque $\B=(x,u(x),\dots,u^{n-1}(x))$ est une base de $E$, la matrice de $u$ dans cette base est: $$\operatorname{Mat}_\B(u)=\begin{pmatrix}
		0 &0 &\cdots &\cdots &0\\
		1 & \ddots & \ddots &&\vdots\\
		0 &\ddots & \ddots &\ddots&\vdots\\
		\vdots &\ddots&\ddots&\ddots&0\\
		0 &\cdots&0&1&0
	\end{pmatrix}$$
	Donc en calculant les puissances de cette matrice :
	$$\operatorname{Mat}(u^p)=\operatorname{Mat}_\B(u)^p=\begin{pmatrix}
		0 &\cdots &\cdots &0 &0 &\cdots&0\\
		\vdots & & &\vdots &\vdots & & \vdots\\
		0 &\cdots &\cdots &0 &\vdots & &\vdots\\
		1 &0 &\cdots &0 &\vdots & &\vdots\\
		0 &\ddots &\ddots &\vdots &\vdots & &\vdots\\
		\vdots &\ddots&\ddots&0 &\vdots & &\vdots\\
		0 &\cdots&0&1&0&\cdots&0
	\end{pmatrix}$$
	on en déduit que la dimension de $\ker(u^p)$ est égale à $p$ pour tout $p\in\crblanc{1}{n-1}$.
	\\Soit $F$ un espace stable par $u$ de dimension $p\in\crblanc{1}{n-1}$.
	\\En étudiant l'endomorphisme induit $u_F$ par $u$ sur $F$, on a que $u_F$ est nilpotent puisque $u_F^n=0$ et donc $u_F^p=0$ puisque $u_F\in \L(F)$.
	\\Donc $F\subset \ker(u^p)$.
	\\Et donc avec $\dim(F)=\dim(\ker(u^p))$, on a $F=\ker(u^p)$.
	\\Finalement, les espaces vectoriels stables par $u$ sont les $\ker(u^p)$ pour tout $p\in\crblanc{0}{n}$ ce qui donne bien qu'il y a exactement $n+1$ espaces vectoriels stables par $u$.
	
	\subsection{Endomorphismes de permutation \xens{4}}
	\label{Endomorphismes de permutation corrigé}
	\textcolor{blue}{\hyperref[Endomorphismes de permutation]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Notons $\B=(e_1,\dots,e_n)$ la base canonique de $\C^n$. On note aussi $\sigma=\sigma_1\circ\dots\circ\sigma_r$ la décomposition en cycles à supports disjoints de $\sigma$ ainsi que $l_i$ la longueur de $\sigma_i$ pour $i\in \crblanc{1}{r}$.\\
		On remarque que $\forall \tau,\tau'\in \mathcal S_n,\ u_{\tau\circ\tau'}=u_{\tau'}\circ u_\tau$ et que $u_\tau$ laisse laisse invariant $e_i$ lorsque $i$ n'est pas dans son support. Ainsi il suffit de déterminer la matrice de $u_{\sigma_1},\dots,u_{\sigma_r}$ pour avoir celle de $u_\sigma$.\\
		Traitons d'abord le cas simple : $\sigma=(1,\dots,n)$. Dans ce cas, la matrice de $u_\sigma$ dans la base $\B$ est
		$$P_n:=\begin{pmatrix}
			0&1&0&\cdots&0\\
			\vdots&\ddots&\ddots&\ddots&\vdots\\
			\vdots&&\ddots&\ddots&0\\
			0&\cdots&\cdots&0&1\\
			1&0&\cdots&0&0
		\end{pmatrix}$$
		On remarque que $P_n$ est une matrice compagnon (cf. \ref{Matrice compagnon (1)}) donc $\chi_{u_\sigma}=X^n-1$.\\
		Ensuite, pour un cycle quelconque $\sigma=(i_1,\dots,i_p)$ de $\mathcal S_n$, de manière similaire la matrice de $u_\sigma$ dans la base $\B'=(e_{j_1},\dots,e_{j_{n-p}},e_{i_1},\dots,e_{i_p})$ avec $j_1,\dots,j_{n-p}$ les points fixes de $\sigma$ est :
		$$M=\left(\begin{array}{c|c}
			I_{n-p}&0  \\
			\hline
			0&P_p 
		\end{array}\right)$$
		Où $P_p\in \M_p(\C)$ est de la même forme que $P_n$. Donc $\chi_{u_\sigma}=(X-1)^{n-p}(X^p-1)$.\\
		Enfin, dans le cas général on note $j_1,\dots,j_k$ les points fixes de $\sigma$ ainsi que $\sigma_m=(i_{m,1},\dots,i_{m,l_m})$ pour $i\in \crblanc{1}{r}$. Si $s\in \crblanc{1}{n}$ est dans le support de $\sigma$ alors il est dans un, et seulement un des supports de $\sigma_1,\dots,\sigma_r$. Disons que $s=i_{m,t}$. Donc $u_\sigma(e_s)=u_{\sigma_m}(e_{i_{m,t}})=\begin{cases}
			e_{i_{m,t+1}}&\mbox{si }t<l_m\\
			e_{i_{m,1}}&\mbox{si }t=l_m
		\end{cases}$.\\
		Autrement dit, la matrice de $u_\sigma$ dans la base $B'$, concaténation des bases $B_0=(e_{j_1},\dots,e_{j_k})$ et $B_m=(e_{i_{m,1}},\dots,e_{i_{m,l_m}})$ pour $1\leq m\leq r$ est :
		$$M=\left(\begin{array}{c|c|c|c|c}
			I_k&0&\cdots&\cdots&0\\
			\hline
			0&P_{l_1}&0&\cdots&0\\
			\hline
			\vdots&0&\ddots&\ddots&\vdots\\
			\hline
			\vdots&\vdots&\ddots&\ddots&\vdots\\
			\hline
			0&0&\cdots&0&P_{l_r}
		\end{array}\right)$$
		Par conséquent $\chi_{u_\sigma}=(X-1)^k\displaystyle\prod_{i=1}^r(X^{l_i}-1)$ et $\text{Sp}(u_\sigma)=\displaystyle\bigcup_{i=1}^r\U_{l_i}$.\\\\
		\textit{Remarque : Vu la forme de la matrice de $u_{\sigma}$, on peut montrer que $\pi_{u_\sigma}=\ppcm(X^{l_1}-1,\dots,X^{l_r}-1)=X^{\ppcm(l_1,\dots,l_r)}-1$.}
		\item On note encore $(e_1,\dots,e_n)$ la base canonique de $\C^n$ et on note pour $(i,j)\in \crblanc{1}{n}^2$ avec $i\ne j$, $\tau_{i,j}$ la transposition $(i,j)$. Pour comprendre l'idée du raisonnement on va d'abord traiter le cas $n=3$.\\
		Soit $V$ un sev non nul de $\C^3$ stables par tous les $u_\sigma,\ \sigma\in \mathcal S_3$.\\
		Fixons $x=(x_1,x_2,x_3)\in V$. On sait que $u_{\tau_{1,2}}(x)=x_2e_1+x_1e_2+x_3e_3\in V$. Il est alors naturel de considérer la différence $u_{\tau_{1,2}}(x)-x=(x_2-x_1)e_1+(x_1-x_2)e_2=(x_2-x_1)(e_1-e_2)\in V$. Si $x_2-x_1\ne 0$ cela donne $e_1-e_2\in V$. Mais en considérant $u_{\tau_{1,2}\circ\tau_{2,3}}(x)=u_{\tau_{2,3}}(u_{\tau_{1,2}}(x))=x_2e_1+x_3e_2+x_1e_3$ on obtient de la manière que $e_1-e_3\in V$. Alors pour des raisons de dimensions $V=\Vect(e_1-e_2,e_1-e_3)$ ou $V=\C^3$. De plus le raisonnement s'adapte tant qu'il existe un vecteur $x\in V$ dont deux composantes au moins diffèrent. C'est cela que l'on va chercher à généraliser.\\
		Soit $V$ un sev non nul de $\C^n$ stables par tous les $u_\sigma$ pour $\sigma\in \mathcal S_n$. Supposons qu'il existe un vecteur $x=(x_1,\dots,x_n)\in V$ dont deux composantes au moins diffèrent. On note $i<j$ tel que $x_i\ne x_j$.\\
		On pose pour $k\ne j$, $y_k=u_{\tau_{k,j}}(x)\in V$. En notant $y_k=(y_{k,1},\dots,y_{k,n})$ on sait que $y_{k,i}=x_i\ne x_j=y_{k,k}$. On sait alors que $u_{\tau_{i,k}}(y)-y=(x_j-x_i)(e_i-e_k)\in V$ ce qui impose $e_i-e_k\in V$. Ceci étant vrai pour tout $k\ne i$, $V$ contient l'hyperplan $H=\Vect(e_i-e_k,\ k\ne i)$. Donc $V$ est soit de dimension $n-1$ et égal à $H$, soit de dimension $n$ et égal à $\C^n$.\\
		Réciproquement on vérifie aisément que $\{0\},\Vect((1,\dots,1)),H=\left\{(x_1,\dots,x_n)\in \C^n,\ \displaystyle\sum_{i=1}^nx_i=0\right\}$ et $\C^n$ sont stables par tous les $u_\sigma,\ \sigma\in \mathcal S_n$.
	\end{enumerate}
	
	\subsection{Semi-simplicité}
	\label{Semi-simplicité corrigé}
	\textcolor{blue}{\hyperref[Semi-simplicité]{[Enoncé]}}\\
	
	\subsection{Endomorphismes diagonalisables d'un $\R$-espace vectoriel \telecom{3}}
	\label{Endomorphismes diagonalisables d'un R-espace vectoriel corrigé}
	\textcolor{blue}{\hyperref[Endomorphismes diagonalisables d'un R-espace vectoriel]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $f$ est diagonalisable donc $E$ admet une base $(x_1,\dots,x_n)$ formée de vecteurs propres de $f$. On note $f(x_j)=\lambda_j$ pour tout $j\in \crblanc{1}{n}$.\\
		Soient $x$ un vecteur propre de $f^k$ et $\lambda$ la valeur propre associée. On écrit $x=\displaystyle\sum_{j=1}^na_jx_j$.\\
		On a $f^k(x)=\lambda x=\displaystyle\sum_{j=1}^na_j\lambda_j^kx_j$.\\
		Donc comme $(x_1,\dots,x_n)$ est libre, $\forall j\in \crblanc{1}{n},\ a_j\lambda=a_j\lambda_j^k$.\\
		Donc comme $k$ est impair, si $a_j\ne 0$, $\lambda_j=\sqrt[k]\lambda$. Ainsi $x=\displaystyle\sum_{j=1}^na_j\sqrt[k]\lambda x_j$ est un vecteur propre de $f$ (associé à la valeur propre $\sqrt[k]\lambda$).
		\item Soit $(e_1,\dots,e_n)$ une base de $E$ formée de vecteurs propres de $f^k=g^k$. D'après la question précédente c'est aussi une base formée de vecteurs propres de $f$ et de $g$.\\
		Si l'on note $\lambda_i$ les valeurs propres de $f$ et $\mu_i$ celles de $g$ l'égalité $f^k=g^k$ donne $\forall i\in \crblanc{1}{n},\ \lambda_i^k=\mu_i^k$ d'où $\lambda_i=\mu_i$ car $k$ est impair.\\
		Ainsi $\forall i\in \crblanc{1}{n},\ f(e_i)=\lambda_ie_i=\mu_ie_i=g(e_i)$. $f$ et $g$ coïncident sur une base, ils sont donc égaux.
	\end{enumerate}
	
	\subsection{Matrices à spectres disjoints}
	\label{Matrices à spectres disjoints corrigé}
	\textcolor{blue}{\hyperref[Matrices à spectres disjoints]{[Enoncé]}}\\
	\textbullet (i)$\Longrightarrow$(ii) :\\
	Pour tout $\lambda \in \text{Sp}(A)$, on a $B-\lambda I_n \in \text{GL}_n(\K)$ car $\lambda_\not\in \text{Sp}(B)$.\\
	Ainsi par produit, $\chi_A(B)$ est inversible.	\\
	\textbullet (ii)$\Longrightarrow$ (iii) :\\
	Soit $X\in \M_n(\K)$ tel que $AX=XB$.\\
	On montre par récurrence que \[\forall k\in\N, A^kX=XB^k\]
	Ainsi pour tout $P\in\K[X]$, \[P(A)X=XP(B)\]
	En particulier, pour $P=\chi_A$, on a d'après Cayley-Hamilton : \[0=\chi_A(A)X=X\chi_A(B)\]
	Et puisque que $\chi_A(B)$ est inversible, on en déduit que $X=0$.\\
	\textbullet (iii)$\Longrightarrow$ (iv) :\\
	On pose l'application \[\fonction{\varphi}{\M_n(K)}{\M_n(\K)}{X}{AX-XB}\]
	On montre aisément que $\varphi$ est linéaire. Et puisque $\M_n(\K)$ est de dimension finie et que l'on a supposé que $\varphi$ est injectif. On en déduit que $\varphi$ est bijectif.\\
	En particulier, $\varphi$ est surjective d'où le résultat.\\
	\textbullet (iv)$\Longrightarrow$(i) :\\ 
	Raisonnons par l'absurde que $A$ et $B$ possède une valeur propre commune $\lambda$.\\
	Soit $u,v\in \M_{n,1}(\K)$ tel que $u$ est un vecteur propre de $A^\top$ et $v$ est un vecteur propre de $B$ pour la valeur propre $\lambda$.
	Ainsi pour tout $X\in \M_n(\K)$, \[u^\top AXv-u^\top XBv=0\]
	Ainsi, il suffit de trouver une matrice $M$ tel que $u^\top M v$ est non nulle. On peut prendre $M=uv^\top$.\\
	Ainsi, on obtient une contradiction, donc $A$ et $B$ sont à spectres disjoints. 
	\newpage

\section{Correction Réduction algébrique}
	\subsection{Equation matricielle polynomiale (1)}
	\label{Equation matricielle polynomiale (1) corrigé}
	\textcolor{blue}{\hyperref[Equation matricielle polynomiale (1)]{[Enoncé]}}\\
	On raisonne par analyse synthèse. Soit $M\in \M_n(\C)$ telle que $M^5=M^2$ et $\Tr(M)=n$.\\
	Le polynôme $X^5-X^2=X^2(X^3-1)$ annule $M$ donc $\text{Sp}(M)\subset\{0,1,j,\overline j\}$ en notant $j=e^{2i\pi/3}$. On note $m_\lambda$ la multiplicité de $\lambda\in \C$ en tant que valeur propre de $M$ ($m_\lambda=0$ si $\lambda$ n'est pas valeur propre de $M$).\\
	$\Tr(M)=n\iff jm_j+\overline jm_{\overline j}+m_1=n$. En comparant la partie réelle et la partie imaginaire on obtient $\begin{cases}
		m_1-\dfrac{m_j+m_{\overline j}}{2}=n\\
		\dfrac{\sqrt 3}{2}(m_j-m_{\overline j})=0
	\end{cases}$ Donc $m_j=m_{\overline j}$ et par suite $m_1-m_j=n$.\\
	Or $\forall \lambda\in \C,\ m_\lambda\in \crblanc{0}{n}$ et de plus $m_0+m_1+m_j+m_{\overline j}=n$. Donc $m_1=n$ et $m_j=m_{\overline j}=m_0=0$. Autrement dit $1$ est la seule valeur propre de $M$. Mais alors en particulier $M$ est inversible d'où $M^3=I_n$.\\
	Le polynôme $X^3-1$ annule $M$ donc $M$ est diagonalisable. N'ayant que $1$ pour valeur propre, on conclut que $M=I_n$.\\
	Enfin $I_n$ vérifie évidemment $I_n^5=I_n^2$.
	
	\subsection{Equation matricielle polynomiale (2)}
	\label{Equation matricielle polynomiale (2) corrigé}
	\textcolor{blue}{\hyperref[Equation matricielle polynomiale (2)]{[Enoncé]}}\\
		Soit $A\in \M_n(\R)$ telle que $A^3+A^2+A=0$. Le polynôme $X^3+X^2+X=X(X^2+X+1)$ annule $A$ donc $\text{Sp}(A)\subset\{0,j,\overline j\}$ en notant $j=e^{2i\pi/3}$.\\
	Pour $\lambda\in \C$ on note $m_\lambda$ la multiplicité algébrique de $\lambda$ en tant que valeur propre de $A$, càd en tant que racine de son polynôme caractéristique ($m_\lambda=0$ si $\lambda$ n'est pas valeur propre de $A$).\\
	Comme $A$ est à coefficients réels, $\chi_A\in \R[X]$. Donc $m_j=m_{\overline j}$. De plus d'après le théorème du rang $\rg(A)=n-m_0$.\\
	Enfin on sait que $m_0+m_j+m_{\overline j}=n$ càd $2m_j=n-m_0=\rg(A)$. $\rg(A)$ est donc pair.
	
	\subsection{Equation matricielle avec la comatrice}
	\label{Equation matricielle avec la comatrice corrigé}
	\textcolor{blue}{\hyperref[Equation matricielle avec la comatrice]{[Enoncé]}}\\
	\begin{enumerate}
		\item On sait que si $M\in \T{GL}_n(\C)$ on a $\Com(M)^\top=\det(M)M^{-1}$.\\
		Soit $A,B\in \T{GL}_n(\C)$. $\det(AB)=\det(A)\det(B)\ne 0$ donc $AB\in \text{GL}_n(\C)$.\\
		Ainsi $\Com(AB)^\top=\det(AB)(AB)^{-1}=\det(B)B^{-1}\det(A)A^{-1}=\Com(B)^\top\Com(A)^\top$ i.e $\Com(AB)=\Com(A)\Com(B)$.\\
		Soit maintenant $A,B\in \M_n(\C)$. On sait que GL$_n(\C)$ est dense dans $\M_n(\C)$ cf.[l'exo qui va bien] donc il existe deux suites de matrices inversibles $(A_k)_{k\in \N}$ et $(B_k)_{k\in \N}$ de limites respectives $A$ et $B$.\\
		D'après ce qui vient d'être fait, $\forall k\in \N,\ \Com(A_kB_k)=\Com(A_k)\Com(B_k)$.\\
		Or les coefficients de $\Com(M)$ sont polynomiaux en ceux de $M$, donc l'application $\Com:M\in \M_n(\C)\mapsto \Com(M)$ est continue. D'autre part le produit matriciel $(M,N)\in \M_n(\C)^2\mapsto MN$ est bilinéaire sur $\M_n(\C)$ qui est un espace vectoriel de dimension $n^2<\infty$. C'est donc aussi une application continue.\\
		On en déduit par passage à la limite que $\Com(AB)=\Com(A)\Com(B)$.
		\item Soit $\lambda\in \C$ et soit $A\in E_\lambda$. Soit enfin $P\in \text{GL}_n(\C)$. On pose $B=PAP^{-1}$.\\
		D'après la question précédente $\Com(B)=\Com(P)\Com(A)\Com(P^{-1})$.\\
		Or toujours d'après la question précédente $I_n=\Com(I_n)=\Com(PP^{-1})=\Com(P)\Com(P^{-1})$. Donc $\Com(P^{-1})=\Com(P)^{-1}$. Et comme $P$ est inversible, $\Com(P)^\top=\det(P)P^{-1}$.\\
		Ainsi, $\Com(B)^\top=P\Com(A)^\top P^{-1}$ et par suite $B+\Com(B)^\top=P(A+\Com(A)^\top)P^{-1}=\lambda I_n$.\\
		C'est-à-dire $B\in E_\lambda$.
		\item On note $\tilde A=\Com(A)^\top$. $\rg(\tilde A)=\rg(\Com(A))$.\\
		Pour $n=1$ on déduit de la formule $A\tilde A=\det(A)I_n$ que $\tilde A=1$ si $A\ne 0$. $\tilde A=0$ si $A=0$. On suppose donc maintenant $n\geq 2$.\\
		Si $\rg(A)=n$ i.e si $A$ est inversible, alors $\tilde A$ est inversible puisque $A\tilde A=\det(A)I_n$ avec $\det(A)\ne 0$.\\
		Si $\rg(A)\leq n-2$ alors tous les mineurs de $A$ sont nuls, comme déterminants de matrices extraites de $A$ de tailles strictement supérieures à $\rg(A)$ donc $\tilde A=0$ i.e $\rg(\tilde A)=0$.\\
		Enfin, supposons que $\rg(A)=n-1$. On a $A\tilde A=\det(A)I_n=0$. Donc $\Ima(\tilde A)\subset \Ker(A)$. D'après le théorème du rang $\Ker(A)$ est de dimension $1$, donc $\tilde A$ est de rang $1$ ou $0$. $\Com(A)$ n'est pas nulle puisque $A$, de rang $n-1$, a au moins un mineur non nul. On en déduit que $\rg(\tilde A)=1$.\\
		En résumé, $\rg(\Com(A))=\begin{cases}
			n&\mbox{si }\rg(A)=n\\
			1&\mbox{si }0<\rg(A)=n-1\\
			0&\mbox{sinon}
		\end{cases}$
		\item On note toujours $\tilde A=\Com(A)^\top$.\\
		Pour $n=1$, toute matrice $A$ appartient à $E_{A+\tilde A}$. Supposons dans la suite $n\geq 2$.\\
		Soient $\lambda\in \C$ et $A\in \M_n(\C)$ tels que $A+\tilde A=\lambda I_n$. Observons qu'en multipliant l'égalité par $A$ il vient $A^2-\lambda A-\det(A)I_n=0$. On va discuter selon le rang de $A$.\\
		Si $\rg(A)\leq n-2$ alors $\tilde A=0$ et par suite $A=\lambda I_n$. Comme $A$ n'est pas inversible on a $\lambda=0$ d'où $A=0$. La matrice nulle est bien dans $E_0$.\\
		Supposons que $\rg(A)=n-1$.\\
		Supposons dans un premier temps que $\lambda\ne 0$. Alors $A(A-\lambda I_n)=0$ et le polynôme $X(X-\lambda)$ est scindé à racines simples. Donc $A$ est diagonalisable et puisque $\rg(A)=n-1$, $A$ est semblable à la matrice $\lambda J_{n-1}=\diag(\lambda,\dots,\lambda,0)$. On calcule $\Com(\lambda J_{n-1})=\left(\begin{array}{ccc|c}
			&&&0\\
			&0&&\vdots\\
			&&&0\\
			\hline
			0&\cdots&0&\lambda^{n-1}
		\end{array}\right)$. La matrice $A$ appartient donc à 	$E_\lambda$ ssi $\lambda^{n-1}=\lambda$, autrement dit ssi $\lambda$ est une racine $(n-2)$-ième de l'unité.\\
		Maintenant si $\lambda=0$ alors $A^2=0$. Ceci impose $\Ima(A)\subset \Ker(A)$ et par le théorème du rang $n-1=\rg(A)\leq \dim\Ker(A)=1$ i.e $n\leq 2$. Donc $E_0$ est vide lorsque $n>2$.\\
		Pour le cas $n=2$, on écrit $A=\begin{pmatrix}a&b\\c&d\end{pmatrix}$. On calcule $\Com(A)=\begin{pmatrix}d&-c\\-b&a\end{pmatrix}$ puis $A+\tilde A=\begin{pmatrix}
			a+d&0\\
			0&a+d\end{pmatrix}=\Tr(A)I_n$. Ainsi $\forall A\in \M_2(\C),\ A\in E_{\Tr(A)}$.\\
		On s'intéressera dans la suite au cas $n\geq 3$.\\
		Supposons enfin $\rg(A)=n$. On note $\Delta$ le discriminant de $P(X)=X^2-\lambda X-\det(A)$.\\
		Si $\Delta\ne0$ alors $P$ est scindé à racines simples donc $A$ est diagonalisable. On note $\lambda_1,\lambda_2$ les deux racines de $P$ et $m$ la multiplicité de $\lambda_1$ en tant que valeur propre de $A$. On peut remarquer qu'avec ses notations, $\lambda=\lambda_1+\lambda_2$ et $\det(A)=\lambda_1\lambda_2$.\\
		$A$ est semblable à la matrice diagonale $D=\diag(\lambda_1,\dots,\lambda_1,\lambda_2,\dots,\lambda_2)$ où $\lambda_1$ apparaît $m$ fois et $\lambda_2$ apparaît $n-m$ fois.\\
		Alors $\lambda_1\lambda_2=\det(A)=\det(D)=\lambda_1^m\lambda_2^{n-m}$. Comme $A$ est inversible $\lambda_1\ne 0$ et $\lambda_2\ne 0$. Ainsi $\lambda_1,\lambda_2$ vérifient la relation $\lambda_1^{m-1}=\lambda_2^{m+1-n}$.\\
		Etudions la réciproque. Si $D=\diag(\lambda_1,\dots,\lambda_1,\lambda_2,\dots,\lambda_2)\in \text{GL}_n(\C)$, où $\lambda_1$ apparaît $m\in \crblanc{1}{n-1}$ fois et $\lambda_1^{m-1}=\lambda_2^{m+1-n}$ alors on calcule $\tilde D=\det(D)D^{-1}=\lambda_1\lambda_2\diag(\lambda_1^{-1},\dots,\lambda_1^{-1},\lambda_2^{-1},\dots,\lambda_2^{-1})=\diag(\lambda_2,\dots,\lambda_2,\lambda_1,\dots,\lambda_1)$ où $\lambda_2$ apparaît $m$ fois.\\
		Donc $D+\tilde D=(\lambda_1+\lambda_2)I_n$.\\
		Reste à traiter le cas $\Delta=0$ càd $\lambda^2=4\det(A)$. On note $\mu=\dfrac{\lambda}{2}\ne 0$ la racine double de $P$. On a $\det(A)=\mu^n=\mu^2$ donc $\mu$ est une racine $(n-2)$-ième de l'unité.\\
		réciproquement fixons $\mu\in \U_{n-2}$ et $A\in \M_n(\C)$ telle que $B=A-\mu I_n$ vérifie $B^2=0$.\\
		Alors $\mu$ est l'unique valeur propre de $A$ et $\det(A)=\mu^n=\mu^2$.
		$B^2=0\implies A^2-2\mu A+\mu^2I_n=0\implies \displaystyle-\frac{1}{\mu^2}A^2+\frac{2}{\mu}A=I_n\implies A^{-1}=\frac{2}{\mu}I_n-\frac{1}{\mu^2}A$.
		On calcule alors $A+\tilde A=A+\det(A)A^{-1}=A+\displaystyle\mu^2\left(\frac{2}{\mu}I_n-\frac{1}{\mu^2}A\right)=2\mu I_n$.\\\\
		En outre on peut résumer la réponse en :
		\begin{itemize}
			\item $\forall A\in \M_1(\C),\ A\in E_{A+\tilde A}$;
			\item $\forall A\in \M_2(\C),\ A\in E_{\Tr(A)}$;
			\item Si $n\geq 3$ et si $A\in E_\lambda$ alors,
			\begin{itemize}
				\item si $\rg(A)<n-2$ alors $\lambda=0$ et $A=0$;
				\item si $\rg(A)=n-1$ alors $\lambda\in \U_{n-2}$ et $A\sim\lambda J_{n-1}=\diag(\lambda,\dots,\lambda,0)$;
				\item si $\rg(A)=n$ alors,
				\begin{itemize}
					\item si $\lambda^2\ne 4\det(A)$ alors $A$ a deux valeurs propres distinctes $\lambda_1$ et $\lambda_2$, $\lambda=\lambda_1+\lambda_2$ et $A$ est semblable à une matrice de la forme $\diag(\lambda_1,\dots,\lambda_1,\lambda_2,\dots,\lambda_2)$ où $\lambda_1$ apparaît $m\in \crblanc{1}{n-1}$ fois. De plus $\lambda_1,\lambda_2$ vérifient la relation $\lambda_1^{m-1}=\lambda_2^{m+1-n}$;
					\item si $\lambda^2=4\det(A)$ alors $\dfrac{\lambda}{2}\in \U_{n-2}$ et $\left(A-\dfrac{\lambda}{2}I_n\right)^2=0$.
				\end{itemize}
			\end{itemize}
		\end{itemize}
	\end{enumerate}
	
	\subsection{Rang et spectre de la comatrice}
	\label{Rang et spectre de la comatrice corrigé}
	\textcolor{blue}{\hyperref[Rang et spectre de la comatrice]{[Enoncé]}}\\
	Pour $n=1$ on déduit de la formule $A\tilde A=\det(A)I_n$ que $\tilde A=1$ si $A\ne 0$. $\tilde A=0$ si $A=0$. On suppose donc maintenant $n\geq 2$.\\
	Si $\rg(A)=n$ i.e si $A$ est inversible, alors $\tilde A$ est inversible puisque $A\tilde A=\det(A)I_n$ avec $\det(A)\ne 0$.\\
	Si $\rg(A)\leq n-2$ alors tous les mineurs de $A$ sont nuls, comme déterminants de matrices extraites de $A$ de tailles strictement supérieures à $\rg(A)$ donc $\tilde A=0$ i.e $\rg(\tilde A)=0$.\\
	Enfin, supposons que $\rg(A)=n-1$. On a $A\tilde A=\det(A)I_n=0$. Donc $\T{Im}(\tilde A)\subset \Ker(A)$. D'après le théorème du rang $\Ker(A)$ est de dimension $1$, donc $\tilde A$ est de rang $1$ ou $0$. $\tilde A$ n'est pas nulle puisque $A$, de rang $n-1$, a au moins un mineur non nul. On en déduit que $\rg(\tilde A)=1$.\\
	En résumé, $\rg(\tilde A)=\begin{cases}
		n&\mbox{si }\rg(A)=n\\
		1&\mbox{si }0<\rg(A)=n-1\\
		0&\mbox{sinon}
	\end{cases}$
	
	Pour ce qui est du spectre, l'étude du rang nous permet déjà de dire que $\T{Sp}(\tilde A)=\{0\}$ si $\rg(A)<n-1$. Si $\rg(A)=n-1$ alors $\tilde A$ est de rang $1$. On en déduit qu'il existe $\lambda\in \C$ tel que $\chi_{\tilde A}(X)=X^{n-1}(X-\lambda)$. Comme on sait que le coefficient en $X^{n-1}$ de $\chi_{\tilde A}$ est $-\Tr(\tilde A)$ on en déduit que $\lambda=\Tr(\tilde A)$. Donc $\T{Sp}(\tilde A)=\{0,\Tr(\tilde A)\}$ (On a potentiellement $\Tr(\tilde A)=0$).\\
	Enfin supposons que $\tilde A$ est inversible. Fixons $\lambda\in \C^*$.
	\begin{align*}
		\chi_{\tilde A}(\lambda)&=\det(\lambda I_n-\tilde A)\\
		&=\det(A^{-1})\det(\lambda A-A\tilde A)\\
		&=\frac{1}{\det(A)}\det(\lambda A-\det(A)I_n)\\
		&=\frac{(-\lambda)^n}{\det(A)}\det\left(\frac{\det(A)}{\lambda}I_n-A\right)\\
		&=\frac{\lambda^n}{(-1)^n\det(A)}\chi_A\left(\frac{\det(A)}{\lambda}\right)
	\end{align*}
	$\C^*$ étant infini on a l'égalité entre polynômes $\chi_{\tilde A}(X)=\dfrac{X^n}{(-1)^n\det(A)}\chi_A\left(\dfrac{\det(A)}{X}\right)$.\\
	On sait que $\tilde A$ est inversible donc $\lambda=0$ n'est pas racine de $\chi_{\tilde A}$.\\
	Ainsi $\forall \lambda\in \C,\ \chi_{\tilde A}(\lambda)=0\iff \chi_A\left(\dfrac{\det(A)}{\lambda}\right)=0\iff \dfrac{\det(A)}{\lambda}\in \T{Sp}(A)$.\\
	Finalement $\T{Sp}(\tilde A)=\{\det(A)\mu^{-1},\ \mu\in \T{Sp}(A)\}$.

	\subsection{Racine $p$-ième d'une matrice}
	\label{Racine pieme d'une matrice corrigé}
	\textcolor{blue}{\hyperref[Racine pieme d'une matrice]{[Enoncé]}}\\
	\underline{$1^{\T{ère}}$ méthode :}\\
	$A$ admet un polynôme annulateur scindé à racines simples $P$. On pose $Q(X)=P(X^p)$. $Q$ est un polynôme annulateur de $B$. Comme $A$ est inversible, $Q'(X)=pX^{p-1}P'(X^p)$ n'a pas de racine commune avec $Q$. Donc $Q$ est scindé à racines simples dans $\C[X]$. Ainsi $Q$ est scindé à racines simples et $B$ est diagonalisable.\\
	\underline{$2^{\T{nd}}$ méthode :}\\
	$\M_{n,1}(\C)=\displaystyle\bigoplus_{\lambda\in \T{Sp}(B^p)}\ker(B^p-\lambda\Id)$.\\
	Posons pour $\lambda\in \T{Sp}(A)$ et $k\in \crblanc{0}{p-1},\ \mu_{\lambda,k}=\sqrt[p]{|\lambda|}e^{i(2k\pi+\arg(\lambda))/p}$.\\
	Dans ce cas les polynômes $X-\mu_{\lambda,0},\dots,X-\mu_{\lambda,p-1}$ sont deux à deux premiers entre eux et de produit $X^p-\lambda$. Et donc d'après le lemme des noyaux :\\
	$\M_{n,1}(\C)=\displaystyle\bigoplus_{\lambda\in \T{Sp}(B^p)}\left(\bigoplus_{k=0}^{p-1}\ker(B-\mu_{\lambda,k}\Id)\right)=\bigoplus_{\mu\in \T{Sp}(B)}\ker(B-\mu\Id)$.\\
	càd $B$ est diagonalisable (il n'y a pas trop de termes, certains des noyaux en jeu sont nuls).\\\\
	Le résultat n'est plus vrai pour $n,p\geq 2$ si on ne suppose plus $A$ inversible. Par exemple pour $A=0$ il suffit de trouver une matrice non nulle nilpotente d'indice inférieur à $p$. $B=\left(\begin{array}{ccc|c}
		0&\cdots&0&1\\
		\hline
		&&&0\\
		&0&&\vdots\\
		&&&0\\
	\end{array}\right)$ convient car $B^2=0$.
	
	\subsection{Diagonalisabilité de $f$ dans le cas $f^2$ diagonalisable}
	\label{Diagonalisabilité de f dans le cas f^2 diagonalisable corrigé}
	\textcolor{blue}{\hyperref[Diagonalisabilité de f dans le cas f^2 diagonalisable]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Remarquons que l'on a toujours $\ker(f)\subset \ker(f^2)$.
		Supposons que $f$ est diagonalisable.\\
		Il existe une base $\B$ de $\C^n$ telle que $\Mat_\B(f)=\diag(0,\dots,0,\lambda_1,\dots,\lambda_r)$ avec $r=\rg(f)$. Alors $\Mat_\B(f^2)=\Mat_\B(f)^2=\diag(0,\dots,0,\lambda_1^2,\dots,\lambda_r^2)$. Donc $\rg(f^2)=r=\rg(f)$ d'où d'après le théorème du rang $\dim(\ker f)=\dim(\ker f^2)$. Donc $\ker(f)=\ker(f^2)$.\\
		Réciproquement supposons que $f^2$ est diagonalisable et que $\ker(f)=\ker(f^2)$.\\
		Alors $\C^n=\ker(f^2)\oplus\displaystyle\bigoplus_{\lambda\in \T{Sp}(f^2)\backslash\{0\}}\ker(f^2-\lambda\Id_{\C^n})$.\\
		Or $\forall \lambda\in \T{Sp}(f^2)\backslash\{0\}$, $\exists \mu\in \C^*,\ \lambda=\mu_\lambda^2$.\\
		Dans ce cas les polynômes $X-\mu_\lambda$ et $X+\mu_\lambda$ sont premiers entre eux et de produit $X^2-\lambda$. Et donc d'après le lemme des noyaux :\\
		$\C^n=\ker(f)\oplus\displaystyle\bigoplus_{\lambda\in \T{Sp}(f^2)\backslash\{0\}}\ker(f-\mu_\lambda\Id_{\C^n})\oplus\ker(f+\mu_\lambda\Id_{\C^n})=\bigoplus_{\mu\in \T{Sp}(f)}\ker(f-\mu\Id_{\C^n})$.\\
		càd $f$ est diagonalisable (il n'y a pas trop de termes, certains des noyaux en jeu sont nuls).\\\\
		\textit{Remarque : la même preuve montre que pour tout $p\in \N^*$ $f$ est diagonalisable si et seulement si $f^p$ est diagonalisable et $\ker f=\ker f^p$.}
		\item Si $\T{Sp}(f^2)\subset \R_+$ alors on peut faire la même preuve pour montrer que $\ker f=\ker f^2\implies f$ diagonalisable. Montrons que cette condition est nécessaire.\\
		Supposons que $f$ est diagonalisable. Alors dans une base $\B$ de diagonalisation de $f$, $\Mat_\B(f^2)=\diag(\lambda_1^2,\dots,\lambda_n^2)$ avec $\lambda_1,\dots,\lambda_n$ des valeurs propres de $f$. Comme $f$ est un endomorphisme d'un $\R$-espace vectoriel, ses valeurs propres sont toutes réelles. Ainsi $\forall i\in \crblanc{1}{n},\ \lambda_i^2\geq 0$ et par suite $\T{Sp}(f^2)\subset \R_+$.
		\item Quitte à considérer l'endomorphisme $f_A\in \K^n$ induit par $A$, les questions précédentes montrent que si $A^2$ est diagonalisable dans $\M_n(\K)$ alors $A$ est diagonalisable dans $\M_n(\C)$ ssi $\ker A=\ker A^2$ et $A$ est diagonalisable dans $\M_n(\R)$ ssi $\ker A=\ker A^2$ et $\T{Sp}(A^2)\subset \R_+$. On note $(e_1,\dots,e_n)$ la base canonique de $\K^n$.\\
		On calcule $Ae_i=a_ie_{n+1-i}$ donc $A^2e_i=a_ia_{n+1-i}e_i$. Donc $A^2$ est diagonalisable dans $\M_n(\K)$.\\
		De plus $\ker A=\Vect(e_i\ |\ a_i=0)$ et $\ker A^2=\Vect(e_i\ |\ a_ia_{n-i+1}=0)$. Ainsi $\ker A=\ker A^2\iff (\forall i\in \crblanc{1}{n},\ a_i=0\iff a_ia_{n-1+i}=0)\iff (\forall i\in \crblanc{1}{n},\ a_i=0\iff a_{n+1-i}=0)$.\\\\
		Pour $\K=\C$ :\\
		$A$ est diagonalisable ssi $\forall i\in \crblanc{1}{n},\ a_i=0\iff a_{n+1-i}=0$.\\\\
		Pour $\K=\R$ :\\
		Il faut et il suffit de plus que les produits $a_ia_{n+1-i}$ soient positifs, autrement dit que $a_i$ et $a_{n+1-i}$ soient de même signes lorsqu'ils sont non nuls.
	\end{enumerate}
	
	\subsection{Endomorphismes diagonalisables non bijectifs}
	\label{Endomorphismes diagonalisables non bijectifs corrigé}
	\textcolor{blue}{\hyperref[Endomorphismes diagonalisables non bijectifs]{[Enoncé]}}\\
	\begin{enumerate}
		\item On écrit $P(X)=XQ(X)$. Comme $0$ est racine simple de $P$ on sait que $X\wedge Q(X)=1$. Par conséquent d'après le lemme des noyaux $E=\Ker P(f)=\Ker f\bigoplus\Ker Q(f)$. Montrons que $\Ker Q(f)=\T{Im}f$. On note $Q(X)=\displaystyle\sum_{k=0}^da_kX^k$. On sait que $a_0\ne 0$.\\
		Soit $y\in \T{Im}f$. $\exists x\in E,\ y=f(x)$. Donc $Q(f)(y)=\displaystyle\sum_{k=0}^da_kf^{k+1}(x)=(f\circ Q(f))(x)=P(f)(x)=0$. Donc $y\in \Ker Q(f)$.\\
		Réciproquement, fixons $x\in \Ker Q(f)$. $Q(f)(x)=0\iff -a_0x=\displaystyle\sum_{k=1}^da_kf^k(x)\iff x=f\left(-\frac{1}{a_0}\sum_{k=0}^{d-1}a_{k+1}f^k(x)\right)$. Donc $x\in \T{Im}f$.\\
		Ainsi $\Ker Q(f)=\T{Im}f$ et par suite $E=\Ker f\bigoplus\T{Im}f$.\\\\
		\textit{Remarque : si $E$ est de dimension finie le théorème du rang donne $\dim(\Ker Q(f))=\rg(f)$ donc il suffit de vérifier une seule inclusion.}
		\item Comme $f$ n'est pas injectif, $0$ est valeur propre de $f$ càd $\pi_f(0)=0$. De plus $f$ est diagonalisable donc $\pi_f$ est scindé à racines simples. Donc $\pi_f'(0)\ne 0$. Enfin $\pi_f$ est un polynôme annulateur de $f$ donc d'après la question précédente $E=\Ker f\bigoplus \T{Im}f$.
	\end{enumerate}
	
	\subsection{Valuation du polynôme minimal}
	\label{Valuation du polynôme minimal corrigé}
	\textcolor{blue}{\hyperref[Valuation du polynôme minimal]{[Enoncé]}}\\
	\begin{enumerate}
		\item Si $p=0$ alors $\pi_f(0)\ne 0$ càd $0$ n'est pas valeur propre de $f$ càd, comme $E$ est dimension finie, $f$ est bijectif.
		\item C'est la même preuve que dans l'exercice précédant.\\
		On écrit $\pi_f(X)=X^pQ(X)$. Comme $p$ est la valuation de $\pi_f$ on sait que $X^p\wedge Q(X)=1$. Par conséquent d'après le lemme des noyaux $E=\Ker \pi_f(f)=\Ker(f^p)\bigoplus\Ker Q(f)$. Montrons que $\Ker Q(f)=\T{Im}(f^p)$. On note $Q(X)=\displaystyle\sum_{k=0}^da_kX^k$. On sait que $a_0\ne 0$.\\
		Soit $y\in \T{Im}(f^p)$. $\exists x\in E,\ y=f^p(x)$. Donc $Q(f)(y)=\displaystyle\sum_{k=0}^da_kf^{k+p}(x)=(f^p\circ Q(f))(x)=\pi_f(f)(x)=0$. Donc $y\in \Ker Q(f)$.\\
		De plus d'après le théorème du rang, $\rg(f^p)=\dim(E)-\dim\Ker(f^p)=\dim\Ker(Q(f))$.\\
		Ainsi $\Ker Q(f)=\T{Im}(f^p)$ et par suite $E=\Ker(f^p)\bigoplus\T{Im}(f^p)$.
		\item Soit $q\in \N^*$ tel que $E=\Ker(f^q)\bigoplus\T{Im}(f^q)$. Notons $R$ le polynôme minimal de l'endomorphisme $\tilde f$ induit par $f$ sur $\T{Im}(f^q)$. Alors $X^qR(X)$ annule $f$ sur $\Ker(f^q)$ et sur $\T{Im}(f^q)$ donc sur $E$. Ainsi $\pi_f|X^qR(X)$ et donc $X^p|X^k\pi_f$.\\
		Or comme $\Ker(f^q)$ et $\T{Im}(f^q)$ sont en somme directe,
		$$\Ker(\tilde f)=\Ker(f)\cap\T{Im}(f^q)\subset \Ker(f^q)\cap\T{Im}(f^q)=\{0\}$$
		Autrement dit $0$ n'est pas racine de $R$ d'où $X^p|X^q$ i.e $p\leq q$.
	\end{enumerate}
	
	\subsection{Sous-espaces stables}
	\label{Sous-espaces stables corrigé}
	\textcolor{blue}{\hyperref[Sous-espaces stables]{[Enoncé]}}\\
	Considérons l'endomorphisme $u_F$ induit par $u$ sur $F$.\\
	Pour tout polynôme $Q\in \K[X]$, $Q(u_F)$ est exactement l'endomorphisme induit par $Q(u)$ sur $F$. Donc $\forall Q\in \K[X],\ \ker Q(u_F)=F\cap \ker Q(u)$.\\
	$P(u)=0\implies P(u_F)=0$.\\
	Donc d'après le lemme des noyaux $F=\ker P(u_F)=\displaystyle\bigoplus_{i=1}^r\ker(P_i^{\alpha_i}(u_F))=\bigoplus_{i=1}^r F\cap N_i$.
	
	\subsection{Une formule sur les polynômes}
	\label{Une formule sur les polynômes corrigé}
	\textcolor{blue}{\hyperref[Une formule sur les polynômes]{[Enoncé]}}\\
	On considère l'endomorphisme de $S\in \L(\C_{n-1}[X])$ défini par $\forall P\in \C_{n-1}[X],\ S(P)=P(X+1)$.\\
	Alors l'égalité se réécrit
	$$\sum_{k=0}^n(-1)^{n-k}\binom{n}{k}S^k=0$$
	Ou encore
	$$(S-\Id)^n=0$$
	Il suffit donc de montrer que $S-\Id$ est nilpotent (on dit que $S$ est unipotent) et le théorème de Cayley-Hamilton permettra de conclure.\\
	Soit $\lambda$ une valeur propre de $S-\Id$ et $P$ un vecteur propre associé (que l'on peut prendre unitaire). On note $P(X)=X^d+Q(X)$ avec $d=\deg(P)$ (et $\deg(Q)<d$).\\
	Alors $(S-\Id)(P)=(X+1)^d-X^d+Q(X+1)-Q(X)=\displaystyle\sum_{k=0}^{d-1}\binom{d}{k}X^k+Q(X+1)-Q(X)$.\\
	On en déduit que $\deg((S-\Id)(P))\leq d-1<\deg(P)$. Or $(S-\Id)(P)=\lambda P$ donc si $\lambda\ne 0$, $\deg((S-\Id)(P))=\deg(P)$. Par conséquent $\lambda=0$.\\
	Ainsi $S-\Id$ est un endomorphisme nilpotent de $\C_{n-1}[X]$ qui est un $\C$-espace vectoriel de dimension $n$, son polynôme caractéristique est donc $X^n$.\\
	Finalement, d'après le théorème de Cayley-Hamilton $(S-\Id)^n=0$.
	
	\subsection{Ordre de matrice}
	\label{Ordre de matrice corrigé}
	\textcolor{blue}{\hyperref[Ordre de matrice]{[Enoncé]}}\\
	Soit $M\in GL_2(\Z)$ d'ordre fini que l'on note $k$. On note $\lambda$ et $\mu$ ses valeurs propres. Puisque $M$ est d'ordre fini, on en déduit que $\lambda$ et $\mu$ sont des racines $k$-ème de l'unité, en particulier $|\lambda|=|\mu|=1$.
	Par conséquent, on en déduit que 
	\begin{align*}
		&|d|=|\lambda||\mu|\\
		&|t|=|\lambda +\mu|\leq |\lambda|+|\mu|=2
	\end{align*}
	Donc \begin{align*}
	&d=\pm 1\\
	&t\in\crblanc{-2}{2}
	\end{align*}
	Si $d=1$, on en déduit que $\mu=\frac{1}{\lambda}=\overline{\lambda}$. Par conséquent, $t=2\text{Re}(\lambda)$. \\Ainsi $\text{Re}(\lambda)\in\{0 , \pm \frac{1}{2},\pm 1 \}$.
	On en déduit donc les couples suivants (à l'ordre près entre $\lambda$ et $\mu$) :\[(i,-i); (j,j^2); (-j, -j^2); (1,1) \text{ et } (-1,-1)\]
	Dans les trois premiers cas, $M$ est diagonalisable et l'on obtient respectivement les ordres 4, 3 et 6.\\
	Dans les cas, où $\lambda=\mu=\pm$, M est semblable à une matrice de la forme \[\begin{pmatrix}
		\pm1 & a\\
		0&\pm1
	\end{pmatrix}\]
	On remarque que $A$ est d'ordre fini si et seulement si $a=0$.\\
	Pour $\lambda=\mu=1$, $A$ est d'ordre 1, et pour $\lambda=\mu=-1$, $A$ est d'ordre 2.\\
	Si $d=-1$, on en déduit $\mu=\frac{-1}{\lambda}=-\overline{\lambda}$. Par conséquent, $t=2i\Ima(\lambda)\in \Z$.\\
	On en déduit que $\Ima(\lambda)=0$
	Ainsi, $\lambda=1$ et $\mu=-1$ ou $\lambda=-1$ et $\mu=1$, dans les cas, on obtient des matrices diagonalisables d'ordre 2.
	Ainsi, les ordres possibles sont 1,2,3,4 et 6.
	\begin{center}
		
	\begin{tabular}{c|c}
		Ordre & Matrice\\
		\hline
		1& $\begin{pmatrix}
			1& 0\\
			0& 1
		\end{pmatrix}$\\
		2&$\begin{pmatrix}
			-1& 0\\
			0& -1
		\end{pmatrix}$\\
		3&$\begin{pmatrix}
			0&-1 \\
			1& -1
		\end{pmatrix}$\\
		4&$\begin{pmatrix}
			0& -1\\
			1& 0
		\end{pmatrix}$\\
		6&$\begin{pmatrix}
			0&-1 \\
			1& 1
		\end{pmatrix}$\\
	\end{tabular}
	\end{center}
	
	\subsection{Sous-groupes finis de GL$_2(\Z)$}
	\label{Sous-groupes finis de GL2(Z) corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes finis de GL2(Z)]{[Enoncé]}}\\
	\begin{enumerate}
		\item On montre que $\M_2(\Z)$ est un sous-anneau de $\M_2(\C)$, ce qui découle directement du fait que $\Z$ est un anneau.\\
		GL$_2(\Z)$ est l'ensemble des inversibles d'un anneau, c'est donc un groupe.\\
		Il sera utile pour la suite de remarquer que si $M\in \T{GL}_2(\Z)$ alors $\det(M)\in \{-1,1\}$. En effet le déterminant est polynomial en les coefficients de la matrices et $\Z$ est un anneau donc $M\in \T{GL}_2(\Z)\implies \det(M)\in \Z$ et $\det(M^{-1})=\det(M)^{-1}\in \Z$. $\det(M)$ est donc un inversible de $\Z$, d'où $\det(M)\in \{-1,1\}$.
		\item Notons $n=|G|$. Fixons $M\in G$. On sait que $M^n=I_2$ et le polynôme $X^n-1$ est scindé à racines simples sur $\C$. Donc $M$ est diagonalisable dans $\M_n(\C)$ et de plus, $\T{Sp}(M)\subset \U_n$.
		On note $M=P\begin{pmatrix}\lambda&0\\0&\mu\end{pmatrix}P^{-1}$ avec $P\in \T{GL}_2(\C)$.\\
		Supposons que $\lambda$ est réel i.e $\lambda\in \{-1,1\}$. Alors $\det(M)=\lambda\mu\in \{-1,1\}$ donne $\mu\in \{-1,1\}$. De même $\mu\in \R\implies (\mu,\lambda)\in \{-1,1\}^2$. Dans ce cas, $M^2=I_2$.\\
		On suppose dans la suite que $\lambda$ et $\mu$ ne sont pas réels. Les coefficients du polynôme caractéristique de $M$ sont polynomiaux en ceux de $M$, ce sont donc des entiers et a fortiori $\chi_M\in \R[X]$. Ainsi $\mu=\overline \lambda$. On note aussi $\lambda=e^{i\theta}$ avec $\theta\in ]-\pi,\pi]$.\\
		On en déduit que $\Tr(M)=\lambda+\overline\lambda=2\cos(\theta)$. Or $\Tr(M)\in \Z$ puisque $M\in \M_n(\Z)$.\\
		Par conséquent $2\cos(\theta)\in [-2,2]\cap \Z$ donc $\cos(\theta)\in \displaystyle\left\{-1,-\frac{1}{2},0,\frac{1}{2},1\right\}$.\\
		$\cos(\theta)=1\implies \theta=0\implies \lambda=1$ et $\cos(\theta)=-1\implies \theta=\pi\implies \lambda=-1$. Ces deux cas sont exclus.\\
		$\cos(\theta)=0\implies \theta\in \left\{-\dfrac{\pi}{2},\dfrac{\pi}{2}\right\}\implies \lambda\in \{i,-i\}\implies M^4=I_2$.\\
		$\cos(\theta)=\displaystyle\frac{1}{2}\implies \theta\in \left\{-\dfrac{\pi}{3},\dfrac{\pi}{3}\right\}\implies \lambda\in \{e^{i\pi}/3,e^{-i\pi/3}\}\implies M^6=I_2$.\\
		$\cos(\theta)=\displaystyle-\frac{1}{2}\implies \theta\in \left\{-\dfrac{2\pi}{3},\dfrac{2\pi}{3}\right\}\implies \lambda\in \{e^{2i\pi/3},e^{-2i\pi/3}\}\implies M^3=I_2$.\\
		Ainsi dans tous les cas $M^{\ppcm(2,3,4,6)}=M^{12}=I_2$.
	\end{enumerate}
	\subsection{Sous-groupes finis de GL$_n(\Z)$}
	\label{Sous-groupes finis de GLn(Z) corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes finis de GLn(Z)]{[Enoncé]}}\\
	\begin{enumerate}
		\item $\det M$ est polynomial en les coefficients de $M$. Comme $\Z$ est un anneau, $\det M\in \Z$. Ainsi si $|\det M|=1$ alors $\det M\in \{-1,1\}$ et $M\in \T{GL}_n(\C)$. De plus, $M^{-1}=\dfrac{1}{\det(M)}\Com(M)^\top=\pm\Com(M)^\top$. Or les coefficients de $\Com(M)$ sont polynomiaux en ceux de $M$ puisque ce sont des déterminants de matrices extraites de $M$. Les coefficients de $\Com(M)^\top$ sont ceux de $\Com(M)$ donc $M^{-1}\in \M_n(\Z)$.\\
		Réciproquement, si $M\in \T{GL}_n(\Z)$ alors $\det M$ et $\det M^{-1}=\dfrac{1}{\det M}$ sont des entiers. Ainsi $\det M\in \{-1,1\}$ et a fortiori $|\det M|=1$.\\
		On a vu que $\T{GL}_n(\Z)=\det^{-1}(\{-1,1\})$. Or $\det:\T{GL}_n(\C)\to \C^*$ est un morphisme de groupes. $\{-1,1\}$ est un sous-groupe de $\C^*$, $\T{GL}_n(\Z)$ est donc un sous-groupe de $\T{GL}_n(\C)$.
		\item \begin{enumerate}[label=\alph*.]
			\item $\forall k\in \N,\ N^k=\displaystyle\frac{1}{3^k}\sum_{j=0}^k(-1)^{k-j}\binom{k}{j}M^j$.\\
			\underline{$1^{\T{ère}}$ méthode :}\\
			Comme $G$ est un groupe fini $M^{|G|}=I_n$. Par conséquent l'ensemble $\langle M\rangle=\{M^j,\ j\in \Z\}$ est fini. On pose $R=\max\{\norme X,\ X\in \langle M\rangle\}$ pour une norme quelconque de $\M_n(\C)$.\\
			On peut majorer $\forall k\in \N,\ \norme{N^k}\leq \displaystyle\frac{R}{3^k}\sum_{j=0}^k\binom{k}{j}=R\left(\frac{2}{3}\right)^k\ukfty\longrightarrow0$.\\
			Ainsi la suite $(N^k)_{k\in \N}$ converge vers la matrice nulle.\\
			\underline{$2^{\T{ème}}$ méthode :}\\
			Comme $G$ est un groupe fini, $M^{|G|}=I_n$. le polynôme $X^{|G|}-1$ est scindé à racines simples sur $\C$ donc $M$ est diagonalisable sur $\C$ et de plus $\T{Sp}(M)\subset \U_{|G|}$.\\
			On écrit $M=PDP^{-1}$ avec $P\in \T{GL}_n(\C)$ et $D=\diag(\lambda_1,\dots,\lambda_n)$.\\
			Alors $N^k=P\diag\left(\left(\dfrac{\lambda_1-1}{3}\right)^k,\dots,\left(\dfrac{\lambda_n-1}{3}\right)^k\right)P^{-1}$.\\
			Or $\forall \lambda\in \U,\ \left|\dfrac{\lambda-1}{3}\right|\leq \dfrac{2}{3}$. Donc $D^k\ukfty\longrightarrow0$. Enfin l'application $A\mapsto PAP^{-1}$ est linéaire sur $\M_n(\C)$, qui est un espace-vectoriel de dimension finie, donc continue. On en déduit que $N^k\ukfty\longrightarrow0$.
			\item On va montrer que l'application $\pi_G$ est injective. Comme $\M_n(\Z/3\Z)$ est un ensemble fini, on obtiendra que GL$_n(\Z)$ est fini.\\
			On montre aisément que $\pi$ est un morphisme d'anneau, il induit donc un morphisme de groupes $\tilde \pi$ de $G$ dans $\pi(G)$. Fixons $A\in \ker\tilde\pi$.\\
			$\tilde\pi(A)=\overline I_n\iff \exists N\in \M_n(\Z),\ A-I_n=3N$.\\
			D'après la question précédente $(N^k)_{k\in \N}$ converge vers $0$. Donc en notant $M_{ij}$ le coefficient $(i,j)$ d'une matrice $M$, chacune des suites $(N^k_{ij})_{k\in \N}$ converge vers $0$. Or ce sont des suites d'entiers, elles stationnent donc à $0$. On en déduit qu'il existe $d\in \N^*$ tel que $N^d=0$ i.e $(A-I_n)^d=0$. Montrons que cela impose $A=I_n$.\\
			On a vu que $A$ est diagonalisable. Notons $A=QD'Q^{-1}$ avec $Q\in \T{GL}_n(\C)$ et $D'=\diag(\mu_1,\dots,\mu_n)$. Alors $(A-I_n)^d=0\iff (D'-I_n)^d=0\iff \forall i\in \crblanc{1}{n},\ (\mu_i-1)^d=0,\ \iff \forall i\in \crblanc{1}{n},\ \mu_i=1\iff A=I_n$.\\
			Ainsi $\tilde\pi$ est injectif et donc $|G|\leq \left|\M_n(\Z/3\Z)\right|=3^{n^2}$.
		\end{enumerate}
	\end{enumerate}
	
	\subsection{Isomorphisme entre GL$_n(\C)$ et GL$_m(\C)$}
	\label{Isomorphisme entre GLn(C) et GLm(C) corrigé}
	\textcolor{blue}{\hyperref[Isomorphisme entre GLn(C) et GLm(C)]{[Enoncé]}}\\
	 On va travailler avec les endomorphismes $\varphi_i$ canoniquement associés aux matrices $A_i$.
	 \begin{enumerate}[leftmargin=*]
	 \item	\begin{enumerate}[label=\alph*.]
		\item $\varphi_1$ est diagonalisable, il a donc une valeur propre $\lambda$. On note $E_\lambda$ le sous-espace propre associé. Comme $\varphi_1$ et $\varphi_2$ commutent, $E_\lambda$ est stable par $\varphi_2$ et $\varphi_2$ induit donc un endomorphisme $\tilde \varphi_2$ sur $E_\lambda$.\\
		Comme $\varphi_2$ est diagonalisable, $\tilde \varphi_2$ l'est aussi. Il existe donc une base $\B_\lambda$ de $E_\lambda$ formée de vecteurs propres de $\tilde\varphi_2$, donc de $\varphi_2$. Mais par définition de $E_\lambda$, les éléments de $\B$ sont des vecteurs propres de $\varphi_1$ (associés à la valeur propre $\lambda$). Enfin $\varphi_1$ est diagonalisable donc $\M_{n,1}(\C)=\displaystyle\bigoplus_{\lambda\in\T{Sp}(\varphi_1)}E_\lambda$. Ainsi la base $\B$ de $\M_{n,1}(\C)$ obtenue par concaténation des bases $\B_\lambda$ est une base de diagonalisation de $\varphi_1$ et $\varphi_2$.
		\item Par récurrence sur le nombre d'endomorphisme : Supposons qu'il existe un entier $k\in \crblanc{1}{p}$ tel que si $u_1,\dots,u_{k-1}$ sont des endomorphismes diagonalisables d'un $\C$-espace vectoriel de dimension inférieure à $n$ et qui commutent deux à deux alors ils sont codiagonalisables.\\
		$\varphi_k$ est diagonalisable, il admet donc une valeur propre $\mu$. On note $E_\mu$ le sous-espace propre associé. Chacun des $\varphi_i$ commute avec $\varphi_k$, ils induisent des endomorphismes $\tilde \varphi_i$ sur $E_\mu$.\\
		Les endomorphismes $\tilde \varphi_1,\dots,\tilde \varphi_{k-1}$ sont diagonalisables et commutent deux à deux (car c'est le cas de $\varphi_1,\dots,\varphi_{k-1}$). Ainsi par hypothèse de récurrence il existe une base $\B_\mu$ de $E_\mu$ formée de vecteurs propres communs à $\tilde\varphi_1,\dots,\tilde\varphi_{k-1}$, donc à $\varphi_1,\dots,\varphi_{k-1}$. Par définition de $E_\mu$, les éléments de cette base sont des vecteurs propres de $\varphi_k$. Enfin $\varphi_k$ est diagonalisable donc $\M_{n,1}(\C)=\displaystyle\bigoplus_{\mu\in\T{Sp}(\varphi_k)}E_\mu$. Ainsi la base $\B$ de $\M_{n,1}(\C)$ obtenue par concaténation des bases $\B_\mu$ est une base de diagonalisation commune à $\varphi_1,\dots,\varphi_k$.
	\end{enumerate}
	\item Soit $A\in G$. Le polynôme $X^2-1=(X-1)(X+1)$ annule $A$ donc $A$ est diagonalisable et ses valeurs propres valent $-1$ ou $1$. Il n'y a qu'un nombre fini de choix possibles\\
	On montre cf.[l'exo qui va bien] que comme $G$ est un groupe dont tous les éléments sont d'ordre au plus $2$, $G$ est un groupe commutatif.\\
	Considérons $V=\Vect(G)$. $V$ est un sous-espace vectoriel de $\M_n(\C)$, il est donc de dimension finie $r$. Donnons nous $A_1,\dots,A_r$ une base de $\Vect(G)$. Comme $A_1,\dots,A_r$ commutent deux à deux, la question 1 assure l'existence d'une matrice inversible $P$ telle que pour tout $i\in \crblanc{1}{r}$, la matrice $D_i=P^{-1}A_iP$ est diagonale.\\
	Donc si $A\in G$, on peut écrire $A=\displaystyle\sum_{i=1}^ra_iA_i$ d'où $P^{-1}AP=\displaystyle\sum_{i=1}^ra_iD_i$ est diagonale. On en déduit que $G$ s'injecte par $\iota:A\mapsto P^{-1}AP$ dans le groupe $D_n=\{\diag(\varepsilon_1,\dots,\varepsilon_n),\ (\varepsilon_1,\dots,\varepsilon_n)\in \{-1,1\}^n\}$ qui est un ensemble fini. $G$ est donc fini. De plus l'injection donne $|G|\leq 2^n$.\\\\
	\textit{Remarque : Ce ne sera pas utile pour la suite mais on peut préciser la réponse. L'injection $\iota$ est un morphisme de groupes, $G$ est donc isomorphe à un sous-groupe de $D_n$. Par le théorème de Lagrange le cardinal de $G$ divise $2^n$, c'est donc une puissance de $2$ inférieure à $2^n$.}
	\item Supposons que l'on dispose d'un isomorphisme $\varphi:\T{GL}_n(\C)\to\T{GL}_m(\C)$. Notons pour $k\in \N^*$, $G_k$ la partie de GL$_k(\C)$ formée des matrices $A$ telles que $A^2=I_k$.\\
	$\forall A\in G_n,\ \varphi(A)^2=\varphi(A^2)=\varphi(I_n)=I_m$. Donc $\varphi(G_n)\subset G_m$. Et de même, $\varphi^{-1}(G_m)\subset G_n$ d'où $G_m\subset \varphi(G_n)$. Ainsi $G_m=\varphi(G_n)$. En particulier, $\varphi(D_n)$ est un sous-groupe de $\T{GL}_m(\C)$ inclus dans $G_m$, d'après la question précédente $2^n=|D_n|=|\varphi(D_n)|\leq 2^m$. De même, $\varphi^{-1}(D_m)$ est un sous-groupe de $\T{GL}_n(\C)$ inclus dans $G_n$, donc $2^m=|D_m|=|\varphi^{-1}(D_m)|\leq 2^n$. Ainsi $2^n=2^m$ et par suite $n=m$.
	\end{enumerate}
	
	\subsection{Inverse et conjugaison}
	\label{Inverse et conjugaison corrigé}
	\textcolor{blue}{\hyperref[Inverse et conjugaison]{[Enoncé]}}\\
	Supposons que $\exists S\in \text{GL}_n(\C),\ A=S{\overline S}^{-1}$. On montre que $\forall A,B\in \M_n(\C),\ \overline{AB}=\overline A\times\overline B$.\\
	$SS^{-1}=I_n$ donc en passant au conjugué $\overline S\ \overline{S^{-1}}=I_n$ i.e ${\overline S}^{-1}=\overline{S^{-1}}$.\\
	Donc $A\overline A=S{\overline S}^{-1}\overline SS^{-1}=I_n$.\\\\
	Réciproquement supposons que $A\overline A=I_n$. On note $X,Y\in \M_n(\R)$ telles que $A=X+iY$. Alors $A=X-iY$.\\
	On a donc $(X+iY)(X-iY)=I_n$ c'est à dire $X^2+Y^2+i(YX-XY)=I_n$. En identifiant partie réelle et partie imaginaire on obtient $X^2+Y^2=I_n$ et $XY=YX$.\\
	$X$ et $Y$ sont trigonalisables dans $\M_n(\C)$ et comme elles commutent, on montre classiquement qu'elles trigonalisent dans une même base. On note alors $P\in \text{GL}_n(\C)$ ainsi que $T_X$ et $T_Y$ triangulaires supérieures telles que $X=PT_XP^{-1}$ et $Y=PT_YP^{-1}$.\\
	Donc d'après l'autre égalité $P(T_X^2+T_Y^2)P^{-1}=I_n$ d'où $T_X^2+T_Y^2=I_n$.\\
	Montrons que les valeurs propres de $A$ sont toutes de modules $1$. Soit $\lambda\in \text{Sp}(A)$ de module maximal et $x$ un vecteur propre associé. $Ax=\lambda x\implies \overline A\ \overline x=\overline \lambda\ \overline x\implies \overline \lambda\in \text{Sp}(\overline A)$. Or $\overline A=A^{-1}$ et $\text{Sp}(A^{-1})=\displaystyle\left\{\frac{1}{\mu},\ \mu\in \text{Sp}(A)\right\}$.\\
	Donc $\displaystyle\frac{1}{\overline \lambda}\in \text{Sp}(A)$. Or $\displaystyle\left|\frac{1}{\overline \lambda}\right|=\frac{1}{|\lambda|}$.
	
	\subsection{Matrice compagnon (2)}
	\label{Matrice compagnon (2) corrigé}
	\textcolor{blue}{\hyperref[Matrice compagnon (2)]{[Enoncé]}}\\
	\begin{enumerate}
		\item Il y a plusieurs méthodes plus ou moins rapides et astucieuses qui sont détaillées dans l'exercice [Matrice compagnon (1)] \textbf{(Je propose de faire les trois méthodes, récurrence, développer une grosse formule et la spéciale Gastaud que je mets là, dans le premier exo seulement)}. On se contentera ici de la plus rapide.\\
		\begin{align*}
			\chi_A(X)&=\begin{vmatrix}
				X&0&\cdots&0&a_0\\
				-1&\ddots&\ddots&\vdots&a_1\\
				0&\ddots&\ddots&0&\vdots\\
				\vdots&\ddots&\ddots&X&a_{n-2}\\
				0&\cdots&0&-1&X+a_{n-1}
			\end{vmatrix}_n\\
			\left(L_1\leftarrow\displaystyle\sum_{k=1}^nX^{k-1}L_k\right)\quad&=\begin{vmatrix}
				0&0&\cdots&0&P(X)\\
				-1&X&\ddots&\vdots&a_1\\
				0&\ddots&\ddots&0&\vdots\\
				\vdots&\ddots&\ddots&X&a_{n-2}\\
				0&\cdots&0&-1&X+a_{n-1}
			\end{vmatrix}_n\\
			&=(-1)^{n+1}P(X)\begin{vmatrix}
				-1&X&0&\cdots&0\\
				0&\ddots&\ddots&\ddots&\vdots\\
				\vdots&\ddots&\ddots&\ddots&0\\
				\vdots&&\ddots&\ddots&X\\
				0&\cdots&\cdots&0&-1
			\end{vmatrix}_{n-1}\\
			&=P(X)
		\end{align*}
		\item D'après le théorème de Cayley-Hamilton $\chi_A(A)=0$ donc $\pi_A|\chi_A$. $\pi_A$ et $\chi_A$ étant unitaires, il suffit de montrer $\deg(\chi_A)=n\leq \deg(\pi_A)$.
		Notons $(e_1,\dots,e_n)$ la base canonique de $\K^n$. On remarque que $\forall i\in \crblanc{1}{n-1},\ Ae_i=e_{i+1}$ i.e $\forall i\in \crblanc{0}{n-1},\ A^ie_1=e_i$.\\
		Donc la famille $(A^ie_1)_{0\leq i\leq n-1}$ est libre, et par suite c'est une base de $\K^n$. Montrons alors que la famille $(A^i)_{0\leq i\leq n-1}$ est libre dans $\K[A]$.\\
		Soit $(\alpha_0,\dots,\alpha_{n-1})\in \K^n,\ \displaystyle\sum_{i=0}^{n-1}\alpha_iA^i=0$.\\
		Alors $\displaystyle\sum_{i=0}^{n-1}\alpha_iA^ie_1=0$ d'où $\alpha_0=\cdots=\alpha_{n-1}=0$. On en déduit que $\dim(\K[A])\geq n$. Or d'après le cours $\dim(\K[A])=\deg(\pi_A)$. Ainsi $\deg(\pi_A)\geq \deg(\chi_A)$ et on conclut que $\pi_A=\chi_A$.
		\item Soit $\lambda$ une valeur propre de $A^\top$ et $X=\begin{pmatrix}x_0\\\vdots\\x_{n-1}\end{pmatrix}$ un vecteur propre associé.\\
		\begin{align*}
			A^\top X&=\lambda X\iff\begin{cases}
				\forall k\in \crblanc{0}{n-2},\ x_{k+1}=\lambda x_k\\
				\displaystyle\sum_{k=0}^{n-1}-a_kx_k=\lambda x_{n-1}
			\end{cases}\\
			&\iff\begin{cases}
				\forall k\in \crblanc{0}{n-1},\ x_k=\lambda^kx_0\\
				\left(\lambda^n+\displaystyle\sum_{k=0}^{n-1}\lambda^k\right)x_0=0
			\end{cases}
		\end{align*}
		On en déduit que le sous-espace propre associé à $\lambda$ est $\Vect\left(\begin{pmatrix}1\\\lambda\\\vdots\\\lambda^{n-1}\end{pmatrix}\right)$. On peut d'ailleurs remarquer que la première ligne impose $x_0\ne 0$ (puisque sinon $X=0$), et donc la deuxième ligne montre d'une nouvelle manière que $\chi_A=X^n+\displaystyle\sum_{k=0}^{n-1}X^k$.
	\end{enumerate}
	
	\subsection{Polynôme minimal ponctuel}
	\label{Polynôme minimal ponctuel corrigé}
	\textcolor{blue}{\hyperref[Polynôme minimal ponctuel]{[Enoncé]}}\\
	
	\subsection{Endomorphismes cycliques}
	\label{Endomorphismes cycliques corrigé}
	\textcolor{blue}{\hyperref[Endomorphismes cycliques]{[Enoncé]}}\\
	
	\subsection{Commutant d'un endomorphisme cyclique}
	\label{Commutant d'un endomorphisme cyclique corrigé}
	\textcolor{blue}{\hyperref[Commutant d'un endomorphisme cyclique]{[Enoncé]}}\\
	
	\subsection{Une démonstration de Cayley-Hamilton}
	\label{Une démonstration de Cayley-Hamilton corrigé}
	\textcolor{blue}{\hyperref[Une démonstration de Cayley_Hamilton]{[Enoncé]}}\\
	
	\subsection{Indépendance de corps du polynôme minimal}
	\label{Indépendance de corps du polynôme minimal corrigé}
	\textcolor{blue}{\hyperref[Indépendance de corps du polynôme minimal]{[Enoncé]}}\\
	
	\subsection{Polynôme minimal de l'inverse}
	\label{Polynôme minimal de l'inverse corrigé}
	\textcolor{blue}{\hyperref[Polynôme minimal de l'inverse]{[Enoncé]}}\\
	
	\subsection{Polynôme minimal de la transposée}
	\label{Polynôme minmal de la transposée corrigé}
	\textcolor{blue}{\hyperref[Polynôme minimal de la transposée]{[Enoncé]}}\\
	
	\subsection{Polynôme minimal imposé}
	\label{Polynôme minimal imposé corrigé}
	\textcolor{blue}{\hyperref[Polynôme minimal imposé]{[Enoncé]}}\\
	
	\subsection{Matrice de Gram}
	\label{Matrice de Gram corrigé}
	\textcolor{blue}{\hyperref[Matrice de Gram]{[Enoncé]}}\\
	
	\subsection{Matrice circulante}
	\label{Matrice circulante corrigé}
	\textcolor{blue}{\hyperref[Matrice circulante]{[Enoncé]}}\\
	
	\subsection{Diagonalisabilité du produit de deux matrices}
	\label{Diagonalisabilité du produit de deux matrices corrigé}
	\textcolor{blue}{\hyperref[Diagonalisabilité du produit de deux matrices]{[Enoncé]}}\\
	
	\subsection{Diagonalisation d'une matrice par bloc}
	\label{Diagonalisation d'une matrice par bloc corrigé}
	\textcolor{blue}{\hyperref[Diagonalisation d'une matrice par bloc]{[Enoncé]}}\\
	
	\subsection{Exponentielle matricielle}
	\label{Exponentielle matricielle corrigé}
	\textcolor{blue}{\hyperref[Exponentielle matricielle]{[Enoncé]}}\\
	
	\subsection{Exponentiel d'un endomorphisme nilpotent}
	\label{Exponentiel d'un endomorphisme nilpotent corrigé}
	\textcolor{blue}{\hyperref[Exponentiel d'un endomorphisme nilpotent]{[Enoncé]}}\\
	
	\subsection{Endomorphismes anticommutants}
	\label{Endomorphimes anticommutants corrigé}
	\textcolor{blue}{\hyperref[Endomorphismes anticommutants]{[Enoncé]}}\\
	
	\subsection{Trace entière}
	\label{Trace entière corrigé}
	\textcolor{blue}{\hyperref[Trace entière]{[Enoncé]}}\\
	
	\subsection{$P(A)$ nilpotente}
	\label{P(A) nilpotente corrigé}
	\textcolor{blue}{\hyperref[P(A) nilpotente]{[Enoncé]}}\\
	On sait que $A$ est trigonalisable dans $\M_n(\C)$.\\
	On note $\lambda_1,\dots,\lambda_n$ les valeurs propres de $A$ sans se soucier de leurs multiplicités.\\
	On remarque que pour tout $P\in\C[X]$, on a : 
	\[P(A)\sim\begin{pmatrix}
		P(\lambda_1) & & & \\
		0&P(\lambda_2)& *& \\
		\vdots& \ddots& \ddots & \\
		0&\dots &0 & P(\lambda_n)
	\end{pmatrix}\]
	Ainsi, les valeurs propres de $P(A)$ sont les $P(\lambda_i)$.
	Pour que $P(A)$ soit nilpotent, il est donc nécessaire et suffisant que \[\text{Sp}(A)\subset \{x\in \C, P(x)=0\}\] autrement dit que les valeurs propres de $A$ soient racines de $P$.
	
	\newpage
\section{Correction Déterminant}
	\subsection{Dimension de l'espace des formes multilinéaires alternées}
	\label{Dimension de l'espace des formes multilinéaires alternées corrigé}
	\textcolor{blue}{\hyperref[Dimension de l'espace des formes multilinéaires alternées]{[Enoncé]}}\\
	
	\subsection{Théorème de Bézout matriciel \etoile{2}}
	\label{Théorème de Bézout matriciel corrigé}
	\textcolor{blue}{\hyperref[Théorème de Bézout mattriciel]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On sait que $\det(A)=\displaystyle\sum_{\sigma\in \mathcal S_n}\varepsilon(\sigma)\prod_{i=1}^na_{i,\sigma(i)}$. Donc $\det(A)$ est polynomial en les coefficients de $A$.\\
		$\Z$ étant un anneau on en déduit que $\det(A)\in \Z$. De même $\det(B)\in \Z$.
		\item D'après le théorème de Bézout, $\exists (u,v)\in \Z^2,\ u\det(A)+v\det(B)=1$.\\
		On sait que $A\Com(A)^\top=\det(A)I_n$ et $B\Com(B)^\top=\det(B)I_n$.\\
		Ainsi $uA\Com(A)^\top+vB\Com(B)^\top=(u\det(A)+v\det(B))I_n=I_n$. On pose alors $U=u\Com(A)^\top$ et $V=v\Com(B)^\top$.\\
		$U$ et $V$ sont des matrices carrées d'ordre $n$ à coefficients entiers car leurs coefficients sont, au signe près, un calcul de déterminant d'une matrice extraite de $A/B$ (donc à coefficients entiers) que multiplie $u/v$. Elles vérifient $AU+BV=I_n$.
	\end{enumerate}
	
	\subsection{Déterminant tridiagonal}
	\label{Déterminant tridiagonal corrigé}
	\textcolor{blue}{\hyperref[Déterminant tridiagonal]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Développons $\Delta_n$ par rapport à la première colonne :
		$$\Delta_n=a\Delta_{n-1}-b\begin{vmatrix}
			c&0&0&\cdots&0\\
			b&a&c&\ddots&\vdots\\
			0&\ddots&\ddots&\ddots&0\\
			\vdots&\ddots&\ddots&\ddots&c\\
			0&\cdots&0&b&a
		\end{vmatrix}_{n-1}$$
		Puis développons le deuxième terme par rapport à la première ligne :
		$$\Delta_n=a\Delta_{n-1}-bc\Delta_{n-2}$$
		Remarquons que cette relation est vérifiée pour $n=2$ en posant $\delta_0=1$. En effet, $\Delta_1=|a|=a$ et $\Delta_2=\begin{vmatrix}
			a&c\\
			b&a
		\end{vmatrix}=a^2-bc=a\Delta_1-bc\Delta_0$.\\
		L'équation caractéristique de la relation est $r^2-ar+bc=0$ de solutions $r_1=\dfrac{a+\delta}{2}$ et $r_2=\dfrac{a-\delta}{2}$ avec $\delta^2=\Delta=a^2-4bc$. Il faut maintenant distinguer deux cas :
		\begin{itemize}
			\item Si $\Delta\ne 0$ alors il existe des constantes $A,B\in \C$ telles que $\forall n\in \N,\ \Delta_n=Ar_1^n+Br_2^n$.\\
			On a $A+B=1$ et $Ar_1+Br_2=a$.\\
			Donc $B=1-A$ et $A=\dfrac{r_1}{\delta}$ i.e
			$$\forall n\in \N,\ \Delta_n=\dfrac{1}{\delta}\left(r_1^{n+1}-r_2^{n+1}\right)$$
			\item Si $\Delta=0$ alors il existe des constantes $C,D\in \C$ telles que $$\forall n\in \N^,\ \Delta_n=(C+Dn)\left(\dfrac{a}{2}\right)^n$$
			Les conditions initiales donnent dans ce cas $C=D=1$ i.e
			$$\forall n\in \N,\ \Delta_n=(1+n)\left(\dfrac{a}{2}\right)^n$$
		\end{itemize}
		\item On a $\Delta=4(\cos^2\theta-1)=-4\sin^2\theta$. Donc $\Delta=0\iff \theta\in \pi\Z$.
		\begin{itemize}
			\item Si $\theta\in \pi\Z$ alors :\\
			Si $\theta\in2\pi\Z$, alors $\cos\theta=1$ et $\forall n\in \N^*,\ \Delta_n=2(2-n)$.\\
			Et si $\theta\in\pi+2\pi\Z$ alors $\cos\theta=-1$ et $\forall n\in \N^*,\ \Delta_n=2(2-n)(-1)^n$.
			\item Si $\theta\notin \pi\Z$, on pose $\delta=2i\sin\theta$. On a $r_1=\cos\theta+i\sin\theta=e^{i\theta}$ et $r_2=\cos\theta-i\sin\theta=e^{-i\theta}$. Alors
			$$\forall n\in \N,\ \Delta_n=\frac{1}{2i\sin\theta}\left(e^{i(n+1)\theta}-e^{-i(n+1)\theta}\right)=\frac{\sin((n+1)\theta)}{\sin(\theta)}$$
		\end{itemize}
	\end{enumerate}
	
	\subsection{Déterminant bitriangulaire\etoile{3}}
	\label{Déterminant bitriangulaire corrigé}
	\textcolor{blue}{\hyperref[Déterminant bitriangulaire]{[Enoncé]}}\\
	On pose $U=\begin{pmatrix}
		1&\dots&\dots&1\\
		\vdots&\ddots&\adots&\vdots\\
		\vdots&\adots&\ddots&\vdots\\
		1&\dots&\dots&1
	\end{pmatrix}$ la matrice Attila.
	\\ $\forall x\in\C$,\begin{align*}
		\det(M+xU)=&\begin{vmatrix}
			x&a+x&\dots&a+x\\
			b+x&x&\ddots&\vdots\\
			\vdots&\ddots&\ddots&a+x\\
			b+x&\dots&b+x&x
		\end{vmatrix}\\
		\underset{C_i\leftarrow C_i-C_1}{=}&\begin{vmatrix}
			x &a&\dots&\dots &a\\
			b+x&-b&a-b&\dots&a-b\\
			\vdots &b &\ddots&\ddots&\vdots\\
			\vdots &\vdots&\ddots&\ddots&a-b\\
			b+x&b&\dots&b&-b
		\end{vmatrix}
	\end{align*}
	Donc, en développant par rapport à la première colonne, $x\mapsto\det(M+xU)$ est une fonction polynomiale de degré au plus 1.
	\\Donc, il existe $(\lambda,\mu)\in\C^2$ tel que pour tout $x\in\C$, $\det(M+xU)=\lambda x+\mu$.
	\\Or, $\begin{cases}
		\det(M-aU)=(-a)^n\\
		\det(M-bU)=(-b)^n
	\end{cases}$
	Ainsi, on en déduit que $\det(M)=\mu=\displaystyle\frac{(-a)^nb-(-b)^na}{b-a}$.
	
	\subsection{Déterminant de Vandermonde\etoile{1}}
	\label{Déterminant de Vandermonde corrigé}
	\textcolor{blue}{\hyperref[Déterminant de Vandermonde]{[Enoncé]}}\\
	Soit $(x_1,\dots,x_{n-1})\in\K^{n-1}$.
	On pose $f:x\mapsto V(x_1,\dots,x_{n-1},x)$.
	\\En développant par rapport à la dernière ligne, on remarque que $f$ est une fonction polynomiale de degré $n-1$ de coefficient dominant $V(x_1,\dots,x_{n-1})$.
	\\De plus, on remarque que $f$ s'annule pour chaque $x_k$.
	\\Ainsi, on en déduit que pour tout $x\in\K$, $f(x)=V(x_1,\dots,x_{n-1})\displaystyle\prod_{k=0}^{n-1}(x-x_k)$.
	\\Puis, en évaluant $f$ en $x_n$, on a $V(x_1,\dots,x_n)=V(x_1,\dots,x_{n-1})\displaystyle\prod_{k=0}^{n-1}(x_n-x_k)$.
	\\Finalement, par récurrence, on montre que: $\forall n\in\N^*,V(x_1,\dots,x_n)=\displaystyle\prod_{1\leq i<j\leq n}(x_j-x_i)$.
	
	\subsection{Déterminant de Hilbert \etoile{3}}
	\label{Déterminant de Hilbert corrigé}
	\textcolor{blue}{\hyperref[Déterminant de Hilbert]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item 
		\begin{enumerate}[label=\alph*.]
			\item En développant par rapport à la dernière ligne, on a: $\Delta_n(x)=\displaystyle\sum_{k=0}^{n-1}\frac{a_k}{x+k}$.
			\\Donc en réduisant au même dénominateur, on obtient: 
			\[\Delta_n(x)=\displaystyle\frac{\displaystyle\sum_{k=0}^{n-1} a_k\displaystyle\prod\limits_{\substack{0\leq i\leq n-1\\i\ne k}}(x+i)}{x(x+1)(x+2)\dots(x+n-1)}\]
			\\D'où l'existence d'un $Q_n=\displaystyle\sum_{k=0}^{n-1} a_k\displaystyle\prod\limits_{\substack{0\leq i\leq n-1\\i\ne k}}(x+i)\in\R_{n-1}[X]$ qui vérifie la condition.
			\item On remarque que $\Delta_n$ s'annule pour $x\in\crblanc{1}{n-1}$ donc $(X-1)\dots(X-n+1)|Q_n$.
			\\Ainsi comme $\deg((X-1)\dots(X-n+1))=n-1\geq \deg(Q_n)$, on sait qu'il existe $\lambda_n\in\R$ tel que $Q_n=\lambda_n(X-1)\dots(X-n+1)$.
		\end{enumerate}
		\item Dans un premier temps, trouvons une expression de $\lambda_n$.
		\\En multipliant $\Delta_n$ par $(x-n+1)$, on a: $(x-n+1)\Delta_n(x)$=
		$\begin{vmatrix}
			1            &\frac{1}{2}  &\frac{1}{3}  &\dots &\frac{1}{n}\\
			\frac{1}{2}  &\frac{1}{3}  &\frac{1}{4}  &\dots &\frac{1}{n+1}\\
			\vdots       &\vdots       &\vdots       &\ddots&\vdots\\
			\frac{1}{n-1}&\frac{1}{n}  &\frac{1}{n+1}&\dots &\frac{1}{2n-2}\\
			\frac{x-n+1}{x}  &\frac{x-n+1}{x+1}&\frac{x-n+1}{x+1}&\dots &1
		\end{vmatrix}$
		Et donc en évaluant en $n-1$, on obtient, $\lambda_n\displaystyle\frac{(-n)(-n-1)\dots(-2n-2)}{(1-n)(2-n)\dots(-1)}=H_{n-1}$
		\\Ainsi, $\lambda_n=\displaystyle\frac{(n-1)!}{n(n+1)\dots(2n-2)(2n-1)}H_{n-1}$.
		\\Enfin, \begin{align*}
			H_n=&\displaystyle\frac{((n-1)!)^2}{(n\dots(2n-2))^2(2n-1)}H_{n-1}\\
			=&\displaystyle\frac{((n-1)!)^4}{((2n-2)!)^2(2n-1)}H_{n-1}
		\end{align*}
		Par récurrence immédiate, on a: \begin{align*}
			H_n=&\displaystyle\prod_{k=1}^{n}\frac{((k-1)!)^4}{((2k-2)!)^2(2k-1)}\\
			=&\prod_{k=0}^{n-1}\frac{(k!)^4}{((2k)!)^2(2k+1)!}\\
			=&\frac{2^nn!}{(2n)!}\prod_{k=0}^{n-1}\frac{(k!)^4}{((2k)!)^2}
		\end{align*}
		
	\end{enumerate}
	\subsection{Déterminant de Gram\etoile{3}}
	\label{Déterminant de Gram corrigé}
	\textcolor{blue}{\hyperref[Déterminant de Gram]{[Enoncé]}}\\
	On remarquera bien que $p$ n'est pas forcément égal à $n$. Et qu'ici $n$ n'est pas la dimension de $E$.
	\begin{enumerate}
		\item On rappelle que pour tout $i\in\crblanc{1}{n}$, $x_i=\displaystyle\sum_{k=1}^n\langle x_i,e_k\rangle e_k$.
		Ainsi pour tout $(i,j)\in\crblanc{1}{p}$\begin{align*}
			(A^\top A)_{i,j}=&\sum_{k=0}^n\langle x_i,e_k\rangle\langle x_j,e_k\rangle\\
			=&\langle x_i,\sum_{k=0}^n\langle x_j,e_k\rangle e_k\rangle &\text{par bilinéarité du produit scalaire}\\
			=&\langle x_i,x_j\rangle
		\end{align*}
		Ainsi $G(x_1,\dots,x_p)=A^\top A$
		\item Montrons que $\Ker(A^\top A)=\Ker(A)$.
		\\Il est clair que $\Ker(A)\subset\Ker(A^\top A)$
		\\Soit $X\in\Ker(A^\top A)$
		\\Donc \begin{align*}
			&A^\top AX=0\\
			\text{donc}\quad&X^\top A^\top AX=0\\
			\text{donc}\quad&\norme{AX}^2=0\\
			\text{ainsi}\quad&AX=0
		\end{align*}
		Donc, on a bien $\Ker(A^\top A)=\Ker(A)$
		Finalement, $\dim(\Ker(G(x_1,\dots,x_p))=\dim(\Ker(A))$
		Et donc, $(x_1,\dots,x_p)$ est liée ssi $\det(G(x_1,\dots,x_p))=0$
		\\Soit $x\in E$
		\\On note $p_\mathcal{F}(x)$ la projection orthogonale de x sur $\F$.
		\\Puisque $x-p_\F(x)$ appartient à l'orthogonal de $\F$, on remarque que: $$G(x_1,\dots,x_p,x-p_\F(x))=\left(\begin{array}{c|c}
			G(x_1,\dots,x_p)&0_{n,1}\\
			\hline
			0_{1,n}&\langle x-p_\F(x),x-p_\F(x)\rangle
		\end{array}\right)$$
		Ainsi, puisque $\langle x-p_\F(x),x-p_\F(x)\rangle=\norme{x-p_\F(x)}^2=\text{d}(x,\F)^2$, on a $G(x_1,\dots,x_p,x-p_\F(x))=\text{d}(x,\F)^2G(x_1,\dots,x_p)$
		\\De plus, $p_\F(x)\in\F$, donc $\det(G(x_1,\dots,x_p,p_\F(x)))=0$, d'après le résultat précédent.
		\\Finalement, par multilinéarité du déterminant, on a: $$\det(G(x_1,\dots,x_p,x))=\text{d}(x,\F)^2G(x_1,\dots,x_p)$$
	\end{enumerate}
	
	\subsection{Déterminant de Cauchy \etoile{3}}
	\label{Déterminant de Cauchy corrigé}
	\textcolor{blue}{\hyperref[Déterminant de Cauchy]{[Enoncé]}}\\
	\begin{enumerate}
		\item En développant par rapport à la dernière ligne, on a:
		\begin{align*}
			F(X)=&\displaystyle\sum_{i=1}^n\frac{(-1)^{n+i}}{X+b_i}C(a_1,\dots,a_{i-1},a_{i+1},\dots,a_n,b_1,\dots,b_n)\\
			=&\frac{P(X)}{\displaystyle\prod_{i=1}^n(X+b_i)}\\
		\end{align*}
		avec $P(X)\in\C_{n-1}[X]$.
		\item \begin{enumerate}[label=\alph*.]
			\item On remarque que pour tout $i\in\crblanc{1}{n-1}$, $F(a_i)=0$.
			\\Donc il existe $\lambda\in\C$, tel que $P=\lambda\displaystyle\prod_{i=1}^{n-1}(X-a_i)$
			\item En multipliant par $(X+b_n)$ la relation obtenue à la question 1, on a:
			\\$\displaystyle\frac{P(X)}{\displaystyle\prod_{i=1}^{n-1}(X+b_i)}=\begin{vmatrix}
				\frac{1}{a_1+b_1} &\dots& \frac{1}{a_1+b_{n-1}}&\frac{1}{a_1+b_n}\\
				\vdots&&\vdots&\vdots\\
				\frac{1}{a_{n-1}+b_1}& \dots & \frac{1}{a_{n-1}+b_{n-1}}& \frac{1}{a_{n-1}+b_n}\\
				\frac{X+b_n}{a_n+b_1}& \dots& \frac{X+b_n}{a_n+b_{n-1}}& 1
			\end{vmatrix}$
			\\Donc $\displaystyle\frac{P(-b_n)}{\prod_{i=1}^{n-1}(-b_n+b_i)}=C_{n-1}$.
			\\Et $\lambda=\displaystyle\frac{\displaystyle\prod_{i=1}^{n-1}(b_i-b_n)}{\displaystyle\prod_{i=1}^{n-1}(-b_n-a_i)}C_{n-1}$
		\end{enumerate}
		\item Ainsi, on a: $C_n=C_{n-1}\displaystyle\frac{\displaystyle\prod_{i=1}^{n-1}(a_n-a_i)\prod_{i=1}^{n-1}(b_n-b_i)}{\displaystyle\prod_{i=1}^{n-1}(b_n+a_i)\prod_{i=1}^{n-1}(a_n+b_i)}$
		\\Finalement, par récurrence, on obtient: $$C_n=\frac{\displaystyle\prod_{1\leq i<j\leq n}(a_j-a_i)\prod_{1\leq i<j\leq n}(b_j-b_i)}{\displaystyle\prod_{1\leq i,j\leq n}(a_i+b_j)}$$
	\end{enumerate}
	\subsection{Déterminant de Smith \etoile{3}}
	\label{Déterminant de Smith corrigé}
	\textcolor{blue}{\hyperref[Déterminant de Smith]{[Enoncé]}}\\
	\begin{enumerate}
		\item $\forall j>i,\ a_{i,j}=0$.
		\\Donc $A$ est triangulaire inférieure donc $\det(A)=\displaystyle\prod_{i=1}^na_{i,i}=1$
		\item On remarque pour tout $(i,j)\in\crblanc{1}{n}^2$, $d_{i,j}=\displaystyle\sum_{\substack{1\leq k\leq n\\k|i,k|j}}1=\sum_{k=1}^na_{i,k}a_{j,k}=(A^\top A)_{i,j}$.
		Donc $D=A^\top A$ et donc $\det(D)=\det(A)^2=1$
		\item On sait que $k|i\wedge j$ ssi $(k|i \;\text{et}\; k|j)$.
		\\Donc d'après l'énoncé, $i\wedge j=\displaystyle\sum_{k|i\wedge j}\varphi(k)=\sum_{(k|i \;\text{et}\; k|j)}\varphi(k)$.
		\\Posons $P\in\M_n(\R)$ tel que $p_{i,j}=\varphi(j)$ si $j|i$ et $p_{i,j}=0$ sinon de sorte que pour tout $(i,j)\in\crblanc{1}{n}^2$, $i\wedge j=\displaystyle\sum_{k=1}^np_{i,k}a_{j,k}$.
		\\Ainsi, $S=PA^\top$.
		\\Par conséquent, $\det(S)=\displaystyle\prod_{k=1}^n\varphi(k)$
	\end{enumerate}
	
	\subsection{Déterminant de Cayley-Menger}
	\label{Déterminant de Cayley-Menger corrigé}
	\textcolor{blue}{\hyperref[Déterminant de Cayley-Menger]{[Enoncé]}}\\
	\newpage
\section{Correction Groupes}
	\subsection{Existence d'un idempotent \centraleponts{3}}
	\label{Existence d'un idempotent corrigé}
	\textcolor{blue}{\hyperref[Existence d'un idempotent]{[Enoncé]}}\\
	Soit $x\in E$. $\forall n\in \N,\ x^{2^n}\in E$. Or $E$ est fini, donc il existe $n<m$ deux entiers naturels tels que $x^{2^n}=x^{2^m}$.\\
	Si $E$ était un groupe de neutres $e$ on aurait $x^{2^m-2^n}=e$, et donc $s^2=s$ en posant $s=x^{2^m-2^n}$.\\
	En s'inspirant de cela, on pose $s=x^{2^m-2^n}$ :\\
	Si $m=n+1$ alors $s=x^{2^n}=x^{2^m}=x^{2^{n+1}}=\left(x^{2^n}\right)^2=s^2$.\\
	Et si $m>n+1$, alors $s^2=x^{2^m-2^n}\cdot x^{2^m-2^n}=x^{2^m}\cdot x^{2^m-2^n-2^n}=x^{2^n}\cdot x^{2^m-2^n-2^n}=x^{2^m-2^n}=s$ ($x^{2^m-2^n-2^n}$ a bien un sens puisque $2^m-2^n-2^n=2^m-2^{n+1}>0$).
	
	\subsection{Sous-semi-groupes finis de $\M_n(\K)$ \centraleponts{3}}
	\label{Sous-semi-groupes finis de Mn(K) corrigé}
	\textcolor{blue}{\hyperref[Sous-semi-groupes finis de Mn(K)]{[Enoncé]}}\\
	On utilise le résultat de l'exercice précédent, ainsi on sait que $U$ admet un idempotent que l'on note $A$
	\\ Donc on en déduit un polynôme annulateur de $A$ qui est $X(X-1)$. 
	\\Par conséquent, $\Tr(A)=\rg(A)\in\crblanc{0}{n}$
	\\Ainsi, il existe une matrice $A$ de trace appartenant à $\crblanc{0}{n}$. 
	
	\subsection{\underline{Groupe composé d'involutions} \ccinp{1}}
	\label{Groupe composé d'involutions corrigé}
	\textcolor{blue}{\hyperref[Groupe composé d'involutions]{[Enoncé]}}\\
	On remarque que tout élément $x\in G$ est une involution : $x^{-1}=x$. Fixons $(x,y)\in G^2$.\\
	$e=(xy)^2=xyxy=x^{-1}yxy^{-1}$ donc en multipliant par $x$ à gauche et par $y$ à droite on obtient $xy=yx$.
	
	\subsection{\underline{Centre et Commutant} \ccinp{1}}
	\label{Centre et Commutant corrigé}
	\textcolor{blue}{\hyperref[Centre et Commutant]{[Enoncé]}}\\
	On note $e$ le neutre de $G$.
	\begin{enumerate}[leftmargin=*]
		\item $Z(G)\subset G$.\\
		$(\forall x\in G,\ ex=x=xe)\implies e\in G$.\\
		Soit $(a,b)\in Z(G)^2$. Soit $x\in G$.\\
		$abx=axb=xab$ donc $ab\in Z(G)$ et $ax=xa\implies axa^{-1}=x\implies xa^{-1}=a^{-1}x$ donc $a^{-1}\in Z(G)$.\\
		Ainsi $Z(G)$ est un sous-groupe de $G$.
		\item Soit $x\in G$. $\mathcal{C}(x)\subset G$.\\
		$ex=x=xe\implies e\in \mathcal{C}(G)$.\\
		Soit $(a,b)\in \mathcal{C}(G)^2$.\\
		$abx=axb=xab$ donc $ab\in \mathcal{C}(G)$ et $ax=xa\implies axa^{-1}=x\implies xa^{-1}=a^{-1}x$ donc $a^{-1}\in \mathcal{C}(G)$.\\
		Ainsi $\mathcal{C}(G)$ est un sous-groupe de $G$.
	\end{enumerate}
	
	\subsection{\underline{Théorème de Dixon} \xens{4}}
	\label{Probabilité que deux éléments d'un groupe commutent corrigé}
	\textcolor{blue}{\hyperref[Probabilité que deux élémebts d'un groupe commutent]{[Enoncé]}}\\
	Notons $n=|G|$. On munit $(G^2,\mathcal{P}(G^2))$ de la probabilité $\p$ uniforme.\\
	On cherche à majorer la probabilité de l'évènement $A=\{(x,y)\in G^2,\ xy=yx\}$.\\
	$\p(A)=\displaystyle\frac{|A|}{n^2}=\frac{1}{n^2}\sum\limits_{(x,y)\in A^2}1=\frac{1}{n^2}\sum\limits_{x\in G,y\in \mathcal{C}(x)}1=\frac{1}{n^2}\sum\limits_{x\in G}\sum\limits_{y\in \mathcal{C}(x)}1=\frac{1}{n^2}\sum\limits_{x\in G}|\mathcal{C}(x)|$.\\
	Fixons $x\in G\backslash Z(G)$. Cet ensemble est non vide puisque $G$ n'est pas commutatif.\\
	On sait alors que $Z(G)$ est un sous-groupe strict de $\mathcal{C}(x)$ ($x\in \mathcal{C}(x)\backslash Z(G)$) et donc que $|Z(G)|<|\mathcal{C}(x)|$.\\
	De plus, d'après le théorème de Lagrange, $|Z(G)|$ divise $|\mathcal{C}(x)|$.\\
	On peut donc affiner la majoration en $2|Z(G)|\leq |\mathcal{C}(x)|$. En effet, $\displaystyle\frac{|\mathcal{C}(x)|}{|Z(G)|}$ est un entier strictement supérieur à $1$, il vaut donc au moins $2$.\\
	De même, $\mathcal{C}(x)$ est un sous-groupe strict de $G$ (puisque $x\notin Z(G)$) donc $2|\mathcal{C}(x)|\leq n$.\\
	Enfin, si $x\in Z(G),\ \mathcal{C}(x)=G$.\\
	On reprend le calcul,\\
	$\p(A)=\displaystyle\frac{1}{n^2}\sum\limits_{x\in G}|\mathcal{C}(x)|=\frac{1}{n^2}\left(\sum\limits_{x\in Z(G)}|G|+\sum\limits_{x\in G\backslash Z(G)}|\mathcal{C}(x)|\right)=\frac{|Z(G)|}{n}+\frac{1}{n^2}\sum\limits_{x\in G\backslash Z(G)}|\mathcal{C}(x)|$.\\
	On peut alors majorer :
	$$\p(A)\leq \frac{|Z(G)|}{n}+\frac{1}{n^2}\sum\limits_{x\in G\backslash Z(G)}\frac{n}{2}\leq \frac{|Z(G)|}{n}+\frac{n-|Z(G)|}{2n}\leq\frac{n+|Z(G)|}{2n}=\frac{1}{2}+\frac{|Z(G)|}{2n}\leq \frac{1}{2}+\frac{1}{8}=\frac{5}{8}$$
	
	\subsection{\underline{Opérations sur les sous-groupes} \ccinp{2}}
	\label{Opérations sur les sous-groupes corrigé}
	\textcolor{blue}{\hyperref[Opérations sur les sous-groupes]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $H\cap K\subset H\subset G$. Notons $e$ le neutre de $G$\\
		$e\in H$ et $e\in K$ car $H$ et $K$ sont des sous-groupes de $G$.\\
		Soit $(a,b)\in \left(H\cap K\right)^2$.\\
		$ab^{-1}\in H$ et $ab^{-1}\in K$ donc $ab^{-1}\in H\cap K$.\\
		$H\cap K$ est un sous-groupe de $G$.
		\item Si $H\subset K$ ou $K\subset H$ alors $H\cup K=K$ ou $H\cup K=H$. Dans tous les cas $H\cup K$ est un sous-groupe de $G$.\\
		Réciproquement supposons que $H\cup K$ est un sous-groupe de $G$. Supposons que $H\not \subset K$ autrement dit que $H\backslash K\ne \emptyset$.\\
		Il existe donc $h\in H\backslash K$. Fixons $k\in K$.\\
		$k\in H\cup K$ et $h\in H\cup K$ donc $hk\in H\cup K$. Si $hk\in K$ alors comme $k^{-1}\in K,\ hkk^{-1}=h\in K$ ce qui est absurde.\\
		Donc $hk\in H$. Mais alors comme $h^{-1}\in H,\ h^{-1}hk=k\in H$.\\
		Ainsi $K\subset H$.\\
		On a donc montré que $H\subset K$ ou $H\subset K$.
	\end{enumerate}
	
	\subsection{\underline{Sous-groupes finis de $\U$} \telecom{2}}
	\label{Sous-groupes finis de U corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes finis de U]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $H$ un sous-groupe fini de $\U$.\\
		$\forall z\in H,\ z^{|H|}=1\implies H\subset \U_{|H|}$.\\
		De plus $|H|=|\U_{|H|}|$ donc $H=\U_{|H|}$.\\
		Les sous-groupes finis de $\U$ sont les $\U_n$ pour $n\in \N^*$.
		\item Si $m|n$ alors il existe $k\in \Z,\ n=mk$ et donc $\forall z\in \U_m,\ z^n=z^{mk}=\left(z^m\right)^k=1^k=1$. C'est à dire $\U_m\subset \U_n$.\\
		Si $m$ ne divise pas $n$ alors il existe $k\in \Z$ tel que $k$ divise $m$ et ne divise pas $n$ et $z_k=e^{2i\pi/k}\in \U_m\backslash \U_n$ :
		\begin{itemize}
			\item $z_k^m=e^{2ia\pi}=1$ car $a=\displaystyle\frac{m}{k}\in \Z$;
			\item $z_k^n=e^{2ib\pi}\ne 1$ car $b=\displaystyle\frac{n}{k}\notin \Z$.
		\end{itemize}
		\item $\U_m\cap \U_n$ est un sous-groupe fini de $\U$ donc d'après la question $1$ il existe $q\in \N^*$ tel que $\U_m\cap \U_n=\U_q$.\\
		Or $\U_m\cap \U_n\subset \U_m$ et $\U_m\cap \U_n\subset \U_n$ donc d'après la question $2$ $q|m$ et $q|n$.\\
		De plus, si $k\in \N^*$ tel que $k|m$ et $k|n$ alors $\U_k\subset \U_m$ et $\U_k\subset \U_n$. Donc $\U_k\subset \U_m\cap\U_m=\U_q$. Ainsi d'après la question $2$, $k|q$. Ainsi comme $q$ est positif, $q=m\wedge n$.
		\item D'après la question $1$, il existe $r\in \N^*$ tel que le sous-groupe engendré par $\U_m\cup \U_n$ soit $\U_r$. Montrons que $r=m\vee n$.\\
		Par définition du sous-groupe engendré $\U_m\cup \U_n\subset \U_r$ c'est à dire $\U_m\subset \U_r$ et $\U_n\subset \U_r$. Donc $m|r$ et $n|r$.\\
		De plus, si $k\in \N^*$ tel que $m|k$ et $n|k$ alors $\U_m\cup \U_n\subset \U_k$. Donc par définition du sous-groupe engendré $\U_r\subset \U_k$ c'est à dire $r|k$. Ainsi comme $r$ est positif, $r=m\vee n$.
	\end{enumerate}
	
	\subsection{Groupes quasi-cycliques de Prüfer \centraleponts{3}}
	\label{Groupes quasi-cycliques de Prüfer corrigé}
	\textcolor{blue}{\hyperref[Groupes quasi-cycliques de Prüfer]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $1=1^{p^0}\in G_p$. Fixons $(z_1,z_2)\in G_p^2$. Il existe $k_1,k_2\in \N$ tels que $z_1^{p^{k_1}}=1=z_2^{p^{k_2}}$.\\
		Donc $|z_1|=1$ et $\left(z_1z_2^{-1}\right)^{p^{k_1+k_2}}=\left(z_1^{p^{k_1}}\right)^{p^{k_2}}\left(z_2^{p^{k_2}}\right)^{-p^{k_1}}=1^{p^{k_2}}\times 1^{-p^{k_1}}=1$.\\
		Ainsi $G_p$ est un sous-groupe de $\U$.
		\item $G_p=\{z\in \C,\ \exists k\in \N,\ z\in \U_{p^k}\}=\displaystyle\bigcup\limits_{k\in \N}\U_{p^k}$.
		\item Montrons que les sous-groupes de $G_p$ sont les $\U_{p^k}$ pour $k\in \N$.\\
		Remarquons d'abord que la suite $\left(\U_{p^k}\right)_{k\in \N}$ est croissante pour l'inclusion de sorte que si $k\in \N$ et si $z\notin\U_{p^k}$ alors $\forall i\leq k,\ z\notin \U_i$.\\
		Soit $H$ un sous-groupe strict de $G_p$. Supposons par l'absurde que $H$ est infini.\\
		Alors $H$ n'est inclus dans aucun des $\U_{p^k}$ pour $k\in \N$. Autrement dit $\forall k\in \N,\ \exists z\in H,\ z^{p^k}\ne 1$. Ou encore puisque $H\subset G_p,\ \forall k\in \N^*,\ \exists z\in H,\ z^{p^k}=1\land z^{p^{k-1}}\ne 1$.\\
		Fixons alors $k\in \N^*$ et $z\in H\cap\U_{p^k}\backslash\U_{p^{k-1}}$.\\
		L'ordre de $z$ dans $\U_{p^k}$ divise l'ordre de $\U_{p^k}$ c'est à dire $p^k$. Donc comme $p$ est premier, l'ordre de $z$ est une puissance de $p$ inférieure à $p^k$. Cependant il ne peut pas être strictement inférieur car sinon on aurait $z\in \U_{p^{k-1}}$. Donc $z$ est d'ordre $p^k$ et est un générateur de $\U_{p^k}$. On en déduit que $\U_{p^k}\subset H$. Ceci étant vrai pour tout $k\in \N^*$, $G_p\subset H$ puis $H=G_p$ ce qui est absurde.\\
		Par conséquent $H$ est fini.
		Enfin, $\forall z\in H,\ z^{|H|}=1$. C'est à dire $H\subset \U_{|H|}$ puis $H=\U_{|H|}$ puisqu'il y a égalité des cardinaux.\\
		On en déduit que $H$ est cyclique.\\
		Pour être plus précis, $H$ est inclus dans $\U_{p^r}$ pour $r\in \N^*$ tel que $p^r\geq |H|$. Donc $|H|$ est lui-même une puissance de $p$.\\
		Pour conclure, les $\U_{p^k}$ pour $k\in \N$ sont bien des sous-groupes de $G_p$.
	\end{enumerate}
	
	\subsection{Sous-groupes de GL$_n(\C)$ qui intersectent trivialement le groupe spécial linéaire \centraleponts{3}}
	\label{Sous-groupes de GLn(C) qui intersectent trivialement le groupe spécial linéaire corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes de GLn(C) qui intersectent trivialement le groupe spécial linéaire]{[Enoncé]}}\\
	Considérons le morphisme $\det:G\to \C^*$. Par hypothèse, $\ker(\det)=G\cap\text{SL}_n(\C)=\{I_n\}$ donc $\det$ est injectif.\\
	Par conséquent $G$ est isomorphe à Im$(\det)$ ce que l'on note $G\simeq\text{Im}(\det)$ et Im$(\det)$ est un sous-groupe fini de $\C^*$. Notons $q$ son ordre. $\forall z\in \text{Im}(\det),\ z^q=1\implies \text{Im}(\det)\subset \U_q$. Par suite, Im$(\det)=\U_q$ par égalité des cardinaux.\\
	Ainsi $G\simeq \U_q\simeq \Z/q\Z$. D'après le cours $G$ est cyclique.
	
	\subsection{\underline{Matrices inversibles à coefficients entiers} \telecom{2}}
	\label{Matrices inversibles à coefficients entiers corrigé}
	\textcolor{blue}{\hyperref[Matrices inversibles à coefficients entiers]{[Enoncé]}}\\
	\begin{enumerate}
		\item Soit $M\in\M_{n}(Z)$\\
		Supposons que $M\in GL_n(\Z)$\\
		A l'aide de la formule du déterminant, on en déduit que $\det(M)\in\Z$.\\
		On a de même $\det(M^{-1})\in\Z$.\\
		On sait que $MM^{-1}=I_n$. Donc $\det(M)\det(M^{-1})=\det(MM^{-1})=1$. Donc $\det(M)=\pm1$\\
		Réciproquement, supposons que $det(M)=\pm1$\\
		Immédiatement, on en déduit que $M$ est inversible dans $\R$ et que $\displaystyle\frac{1}{\det(M)}\in\Z$.\\
		On rappelle que $\displaystyle M^{-1}=\frac{1}{\det(M)}\Com(M)^\top$. Les coefficients de la comatrice sont des déterminant donc $\Com(M)^\top\in\M_{n}(\Z)$.\\
		Finalement $M^{-1}\in\M_{n}(\Z)$ donc $\M_{n}(\Z)\in GL_n(\Z)$
		\item Il est clair que $I_n\in GL_n(\Z)$\\
		Par définition de l'ensemble, la stabilité par l'inverse est assuré.\\
		Et soient $M,N\in\ GL_n(\Z)$, $det(MN)=\det(M)\det(N)=\pm1$. Donc $MN\in GL_n(\Z)$
		
	\end{enumerate}
	
	\subsection{\underline{Sous-groupes de $(\Z,+)$} \ccinp{2}}
	\label{Sous-groupes de Z corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes de (Z,+)]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $\{0\}=0\Z$.
		\item Comme $G\ne \{0\}$, il existe $a_0\in G\backslash\{0\}$. Alors $-a_0\in G$ et on a toujours $a_0>0$ ou $-a_0>0$.\\
		Ainsi $G\cap \N^*$ n'est pas vide. Etant une partie de $\N^*$ elle admet un minimum $a$.
		\item $a\in G\implies \langle a\rangle=a\Z\subset G$.
		\item Soit $x\in G$.\\
		On note $x=aq+r$ la division euclidienne de $x$ par $a$.\\
		D'après la question précédente $aq\in G$ donc $r=x-aq\in G$.\\
		Par définition de $a$ si $r>0$ alors $r\geq a$ ce qui est absurde.\\
		Donc $r=0$ c'est à dire $a|x$ c'est à dire $x\in a\Z$.\\
		Ainsi $G=a\Z$.
	\end{enumerate}
	
	\subsection{\underline{Sous-groupes de $(\R,+)$} \centraleponts{3}}
	\label{Sous-groupes de R corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes de (R,+)]{[Enoncé]}}\\
	\begin{enumerate}
		\item $G\ne\{0\}$ donc il existe $a_0\in G$ non nul. Alors $-a_0\in G$ et on a toujours $a_0>0$ ou $-a_0>0$. Ainsi $G\cap \R^*_+$ est non vide.\\
		De plus $G\cap \R^*_+$ est minoré, par $0$ par exemple.\\
		On en déduit que $G\cap\R^*_+$ admet une borne inférieure $a$.
		\item \begin{enumerate}[label=\alph*.]
			\item $2a>a$ donc par définition de la borne inférieure il existe $x\in G\cap \R^*_+$ tel que $x<2a$. De plus $x\geq a$ par définition de $a$ et $x\ne a$ puisque $a\notin G$. Donc $x>a$ et par définition de la borne inférieure il existe $y\in G\cap \R^*_+$ tel que $y<x$. De même, $y>a$.
			\item On a $a<y<x<2a$ donc $0<x-y<2a-a=a$. Or $x-y\in G$ puisque $x\in G$ et $y\in G$. Ceci contredit le fait que $a$ est un minorant de $G\cap \R^*_+$.\\
			On en déduit que $a\in G$.
			\item On a déjà $\langle a\rangle=a\Z\subset G$. Fixons $x\in G$.\\
			On sait que pour $k=\lfloor\frac{x}{a}\rfloor$ on a $k\leq \displaystyle\frac{x}{a}<k+1$\\
			Donc $ka\leq x<(k+1)a$ puis $0\leq x-ka<a$.\\
			Si, $0<x-ak<a$ alors comme $x\in G$ et $ka\in G$, $x-ak\in G$.\\
			Ceci contredit le fait que $a$ est un minorant de $G\cap\R^*_+$ donc $x-ak=0$ c'est à dire $x=ka\in a\Z$.\\
			Finalement $G=a\Z$.
		\end{enumerate}
		\item Soient $\varepsilon>0$ et $t\in \R$.\\
		$a<\varepsilon$ donc $\exists x\in G,\ x\leq\varepsilon$.\\
		De même qu'en question $2.c$ il existe $k\in \Z$ tel que $kx\leq t<(k+1)x$.\\
		Alors $0\leq t-kx<x$ et on en déduit que $|t-kx|<\varepsilon$.\\
		Comme $kx\in G$ on a prouvé que $G$ est dense dans $\R$.
	\end{enumerate}
	
	\subsubsection{Fonction (multi)périodique \centraleponts{3}}
	On considère $P=\{T\in \R,\ \forall x\in \R,\ f(x+T)=f(x)\}$ l'ensemble des périodes de $f$.\\
	Montrons que $P$ est un sous-groupe de $\R$.\\
	Tout d'abord $0\in P$. Fixons $(T_1,T_2)\in P^2$. Soit $x\in \R$.\\
	$f(x+T_1-T_2)=f((x+T_1-T_2)+T_2)=f(x+T_1)=f(x)$.\\
	Donc $T_1-T_2\in P$.\\
	Ainsi $P$ est un sous-groupe de $\R$.\\
	Donc $P$ est soit dense dans $\R$ soit de la forme $a\Z$ avec $a\in \R$. Montrons que $P$ n'est pas de la forme $a\Z$.\\
	Supposons par l'absurde qu'il existe $a\in \R$ tel que $P=a\Z$.\\
	On sait que $1\in P$ et $\pi\in P$ donc il existe $k_1,k_2\in \Z$ tels que $ak_1=1$ et $ak_2=\pi$. Mais alors $a=\displaystyle\frac{1}{k_1}\in \Q$ d'où $\pi=ak_2\in \Q$ ce qui est absurde.\\
	Par conséquent $P$ est dense dans $\R$. On peut maintenant montrer que $f$ est constante.\\
	Soient $x,y\in \R$. $\exists (T_k)\in P^\N,\ \ukfty{\lim}T_k=y-x$.\\
	$\forall k\in \N,\ f(x+T_k)=f(x)$\\
	Donc comme $f$ est continue sur $\R$, par passage à la limite $f(y)=f(x+(y-x))=f(x)$.
	
	\subsubsection{$\cos(\N)$ \centraleponts{3}}
	Montrons que $\Z+2\pi\Z$ est dense dans $\R$.\\
	$0=0+0(2\pi)\in \Z+2\pi\Z$.\\
	Soient $x,y\in \Z+2\pi\Z$. $\exists a,b,c,d\in \Z,\ x=a+2b\pi,\ y=c+2d\pi$.\\
	Donc $x-y=a-c+2(b-d)\pi\in \Z+2\pi\Z$.\\
	Donc $\Z+2\pi\Z$ est un sous-groupe de $\R$.\\
	Supposons qu'il existe $a\in \R$ tel que $\Z+2\pi\Z=a\Z$.\\
	Alors $\exists b,c\in \Z,\ 1=ab,\ 2\pi=ac$. Mais alors $a=\displaystyle\frac{1}{b}\in \Q$ d'où $\pi=\displaystyle\frac{ac}{2}\in \Q$ ce qui est absurde.\\
	Par conséquent $\Z+2\pi\Z$ est dense dans $\R$.\\
	Soit $t\in [-1,1]$. $\exists x\in \R,\ t=\cos(x)$. $\exists (x_n)=(a_n+2\pi b_n)\in \left(\Z+2\pi\Z\right)^\N,\ x_n\unfty{\longrightarrow}x$.\\
	Alors par continuité de $\cos$ en $x$, $\cos(x_n)\unfty{\longrightarrow}\sin(x)$. De plus $\forall n\in \N,\ \sin(x_n)=\cos(a_n)=\cos(|a_n|)$.\\
	On en déduit que $\cos(|a_n|)\unfty{\longrightarrow}t$ avec $\forall n\in \N,\ |a_n|\in \N$.
	
	\subsubsection{Valeur d'adhérence du cercle unité \centraleponts{3}}
	Soit $z\in \C$. Si $z$ est une racine de l'unité alors la suite $(z^n)_{n\in \N}$ stationne à $1$ qui est donc sa seule valeur d'adhérence.\\
	Supposons maintenant que $z$ n'est pas une racine de l'unité. Cela revient à supposer, si l'on écrit $z=e^{2i\pi\theta}$ avec $\theta\in \R$, que $\theta\notin\Q$.\\
	On va alors montrer que $\Z+\theta\Z$ est dense dans $\R$. On montre aisément que $\Z+\theta\Z$ est un sous-groupe de $(\R,+)$. Montrons qu'il n'est pas monogène. Par l'absurde on suppose qu'il existe $a\in \R$ tel que $\Z+\theta\Z=a\Z$.\\
	Alors on sait qu'il existe un entier $k$ tel que $1=ak$. On en déduit que $a=\displaystyle\frac{1}{k}\in \Q$. Nonobstant on sait aussi qu'il existe un entier $m$ tel que $\theta=am$. Donc $\theta\in \Q$ ce qui est absurde.\\
	Ainsi $\Z+\theta\Z$ n'est pas monogène et d'après le résultat sur les sous-groupes de $(\R,+)$, il est dense dans $\R$.\\
	On en déduit par continuité et $1$-périodicité de la fonction $f:x\in \R\mapsto e^{2i\pi x}$ que $f(\Z+\theta\Z)=\{e^{2ki\pi\theta},\ k\in \Z\}=\{e^{2in\pi\theta},\ n\in \N\}=\{z^n,\ n\in \N\}$ est dense dans $f(\R)=\U$.\\
	En particulier $1$ est une valeur d'adhérence de la suite $(z^n)_{n\in \N}$.
	
	\subsection{Sous-groupes distingué \etoile{1}}
	\label{Sous-groupes distingué corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes distingués]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $g\in G$ soit $h\in Z(G)$. \[ghg^{-1}=hgg^{-1}=h\in Z(G)\]
		Donc $Z(G)$ est distingué dans $G$.
		\item Soit $g\in G$ soit $h\in \Ker(f)$. \[f(ghg^{-1})=f(g)f(h)f(g^{-1})=f(g)ef(g)^{-1}=e\]
		Donc $ghg^{-1}\in \Ker(f)$.\\
		Donc $\Ker(f)$ est distingué dans $G$.
	\end{enumerate}
	
	\subsection{Nature d'une suite}
	\label{Nature d'une suite corrigé}
	\textcolor{blue}{\hyperref[Nature d'une suite]{[Enoncé]}}\\
	
	\subsection{Une relation utile sur les morphismes de groupes \etoile{2}}
	\label{Une relation utile sur les morphismes de groupes corrigé}
	\textcolor{blue}{\hyperref[Une relation utile sur les morphismes de groupes]{[Enoncé]}}\\
	On admet que la relation sur $G$ suivante est une relation d'équivalence sur $G$. $$\forall g_1,g_2\in G , g_1 \sim g_2 \Leftrightarrow g_1^{-1}g_2\in \Ker(f)$$
	Ses classes d'équivalence sont les $g\Ker(f)=\{gk , k\in\Ker(f)\}$ pour $g\in G$.\\
	Montrons que $|g\Ker(f)|=|\Ker(f)|$\\
	On pose $\fonction{\varphi}{\Ker(f)}{g\Ker(f)}{k}{gk}$ et  $\fonction{\psi}{g\Ker(f)}{\Ker(f)}{k}{g^{-1}k}$.\\
	On remarque que : $\varphi\circ\psi=\Id_G=\psi\circ\varphi$ donc $\varphi$ est une bijection et $|g\Ker(f)|=|\Ker(f)|$.\\
	De plus, on remarque que $f$ est constante sur les classes d'équivalence définies précédemment.\\
	En effet, pour tout $g_1 , g_2 \in G$ tel que $g_1\sim g_2$, on a $f(g_1^{-1}g_2)=e$ donc $f(g_1)=f(g_2)$ car $f$ est un morphisme de groupe.\\
	Ainsi, on en déduit que le nombre de classes d'équivalences est $|\Ima(f)|$.\\
	Finalement, puisque les classes d'équivalences forment une partition de $G$, on en déduit : $$|G|=\sum_{g\in\Ima(f)}|g\Ker(f)|=\sum_{g\in\Ima(f)}|\Ker(g)|=|\Ker(f)|\times|\Ima(f)|$$
	
	\subsection{Automorphisme d'inversion \ccinp{1}}
	\textcolor{blue}{\hyperref[Automorphisme d'inversion]{[Enoncé]}}\\
	\label{Automorphisme d'inversion corrigé}
	Notons $e$ le neutre de $G$.
	Soit $x\in G$ tel que $f(x)=e$. Alors $x^{-1}=e$ c'est à dire $e=x$.\\
	$\forall (x,y)\in G^2,\ f(xy)=(xy)^{-1}=y^{-1}x^{-1}$.\\
	Donc $f\in \text{Aut}(G)\iff \forall (x,y)\in G^2,\ y^{-1}x^{-1}=f(xy)=f(x)f(y)=x^{-1}y^{-1} \iff \forall (x,y)\in G^2,\ (y^{-1}x^{-1})^{-1}=(x^{-1}y^{-1})^{-1}\iff \forall (x,y)\in G^2,\ xy=yx$.
	
	\subsection{Automorphismes intérieurs \ccinp{1}}
	\label{Automorphismes intérieurs corrigé}
	\textcolor{blue}{\hyperref[Automorphismes intérieurs]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Notons $e$ le neutre de $G$. Soient $x,y\in G$.\\
		$\varphi_a(xy)=axya^{-1}=(axa^{-1})(aya^{-1})=\varphi_a(x)\varphi_a(y)$.\\
		$\forall x\in G,\ \varphi_a(x)=e\implies axa^{-1}=e\iff ax=a\implies x=e$.\\
		Donc $\varphi_a\in \text{Aut}(G)$.
		\item Montrons que $\fonction{\Phi}{(G,*)}{(\text{Aut}(G),\circ)}{a}{\varphi_a}$ est un morphisme de groupes.\\
		Soient $a,b\in G$. Fixons $x\in G$.\\
		$\Phi(ab)(x)=abx(ab)^{-1}=a(bxb^{-1})a^{-1}=a\Phi(b)(x)a^{-1}=\Phi(a)\circ\Phi(b)(x)$.\\
		Donc $\Phi(ab)=\Phi(a)\circ\Phi(b)$.\\
		Ainsi $\mathfrak{J}(G)=\text{Im}(\Phi)$ est un sous-groupe de $(\text{Aut}(G),\circ)$.
	\end{enumerate}
	
	\subsection{Endomorphismes continus de $\R$ \telecom{2}}
	\label{Endomorphismes continus de R corrigé}
	\textcolor{blue}{\hyperref[Endomorphismes continus de R]{[Enoncé]}}\\
	Soit $f$ un endomorphisme de $(\R,+)$ continu.\\
	Par récurrence immédiate, $\forall n\in \N,\ f(n)=nf(1)$. Donc $\forall n\in \N,\ f(-n)=-nf(1)$.\\
	Ainsi $\forall k\in \Z,\ f(k)=kf(1)$.\\
	Soit $x=\displaystyle\frac{a}{b}\in \Q$ avec $a\in \Z$ et $b\in \N^*$.\\
	$bf(x)=f(bx)=f(a)=af(1)$. Donc $f(x)=\displaystyle\frac{a}{b}f(1)=xf(1)$.\\
	Les applications $f$ et $x\mapsto f(1)x$ coïncident sur $\Q$. Or $\Q$ est dense dans $\R$ et $f$ est continu sur $\R$ donc $f$ et $x\mapsto f(1)x$ coïncident sur $\R$.\\
	Autrement dit $f$ est l'homothétie de rapport $f(1)$.\\
	Réciproquement on vérifie que les homothéties sont des endomorphismes de $(\R,+)$ continus.
	
	\subsection{Morphismes de $\Q$ dans $\Z$ \ccinp{1}}
	\label{Morphismes de Q dans Z corrigé}
	\textcolor{blue}{\hyperref[Morphismes de Q dans Z]{[Enoncé]}}\\
	Soit $f$ un morphisme de $(\Q,+)$ dans $(\Z,+)$.\\
	Par récurrence immédiate, $\forall n\in \N,\ f(n)=nf(1)$. Fixons $p\in \N$.\\
	$\forall q\in \N^*,\ qf\left(\displaystyle\frac{p}{q}\right)=f(p)$ donc $f\left(\displaystyle\frac{p}{q}\right)=\displaystyle\frac{f(p)}{q}\in \Z$.\\
	Autrement dit $\forall q\in \N^*,\ q|f(p)$. Donc $f(p)=0$.\\
	Ainsi $f=0$.
	
	\subsection{Morphismes de $\Z/n\Z$ dans $\Z/m\Z$ \telecom{2}}
	\label{Morphismes de Z/nZ dans Z/mZ corrigé}
	\textcolor{blue}{\hyperref[Morphismes de Z/nZ dans Z/mZ]{[Enoncé]}}\\
	On note $\overline{k}$ les éléments de $\Z/n\Z$ et $\tilde k$ les éléments de $\Z/m\Z$.\\
	On raisonne par analyse-synthèse.\\
	Soit $f : \Z/n\Z \longrightarrow \Z/m\Z$. 
	Il est clair que pour tout $k\in \Z$, \[f(\overline{k})=f(k\overline{1})=kf(\overline{1})\]
	Ainsi, $f$ est entièrement déterminé par $f(\overline{1})$.
	De plus, \[f(\overline{n})=\tilde0\] donc $nf(\overline{1})\equiv 0 [m]$.\\
	Ainsi si $f: \Z/n\Z \longrightarrow \Z/m\Z$ est un morphisme alors $nf(\overline{1})\equiv 0[m]$.\\
	La réciproque est immédiate.
	
	\subsection{Morphismes de GL$_n(\R)$ dans $\Z /m\Z$ \xens{5}}
	\label{Morphismes de GLn(R) dans Z/mZ corrigé}
	\textcolor{blue}{\hyperref[Morphismes de GLn(R) dans Z/mZ]{[Enoncé]}}\\
	Soit $f$ un morphisme de $(\text{GL}_n(\R),\times)$ dans $(\Z/m\Z,+)$. On note $D_i(\lambda)=I_n+(1-\lambda)E^n_{ii}$ pour $i\in \crblanc{1}{n}$ et $\lambda\in \R^*$ les matrices de dilatations (où $E^n_{ij}$ représente la matrice carrée de taille $n$ ayant un $1$ en position $(i,j)$ et des $0$ partout ailleurs). On note $T_{ij}(\lambda)=I_n+\lambda E^n_{ij}$ pour $(i,j)\in \crblanc{1}{n}^2,i\ne j$ et $\lambda\in \R$ les matrices de transvections.\\
	On rappelle que multiplier une matrice de $M\in \M_n(\R)$ à gauche par $T_{ij}(\lambda)$ revient à effectuer l'opération $L_i\leftarrow L_i+\lambda L_j$ sur les lignes de $M$, multiplier une matrice de $M\in \M_n(\R)$ à droite par $T_{ij}(\lambda)$ revient à effectuer l'opération $C_i\leftarrow C_i+\lambda C_j$ sur les colonnes de $M$.\\
	Enfin, on peut échanger les lignes $L_i$ et $L_j$ "au signe près" en effectuant à la suite les opérations $L_i\leftarrow L_i+L_j,\ L_j\leftarrow L_j-L_i,\ L_i\leftarrow L_j+L_i$, autrement dit en multipliant à gauche par $T_{ij}(1)T_{ji}(-1)T_{ij}(1)$. La ligne $L_i$ sera transformée en la ligne $L_j$ et la ligne $L_j$ sera transformée en la ligne $-L_i$.\\
	Montrons par récurrence via l'algorithme du pivot de Gauss que pour toute matrice $A\in \text{GL}_n(\R)$, il existe $M,N\in \text{GL}_n(\R)$ telles que $MAN=D_n(\det A)$ avec $M$ et $N$ des produits de matrice de transvection.\\
	Si $n=1$, $A=D_1(\det A)$ et il n'y a rien à faire.\\
	Supposons le résultat vraie à un certain rang $n-1\geq 1$. Soit $A\in \text{GL}_n(\R)$. La première colonne de $A$ étant non nulle, il existe $i\in \crblanc{1}{n}$ tel que $a_{i,1}\ne 0$. L'opération $L_1\leftarrow L_1+\displaystyle\frac{1-a_{1,1}}{a_{i,1}}L_i$ permet de remplacer le coefficient en position $(1,1)$ par un $1$.\\
	Il est ensuite aisé d'annuler les coefficients de la première ligne et de la première colonne (hormis le $1$ en position $(1,1)$) avec des opérations sur les lignes et les colonnes :
	\begin{itemize}
		\item Pour tout $j\in \crblanc{2}{n}$, l'opération $C_j\leftarrow C_j-a_{1,j}C_1$ annule le coefficient en position $(1,j)$;
		\item Pour tout $k\in \crblanc{2}{n}$, l'opération $L_k\leftarrow L_k-a_{k,1}L_1$ annule le coefficient en position $(k,1)$.
	\end{itemize}
	Ainsi par le biais de produits de matrices de transvection :\\
	$\begin{cases}
		M_0=T_{n,1}(-a_{n,1})\dots T_{k,1}(-a_{k,1})\dots T_{2,1}(-a_{2,1})\\
		N_0=T_{2,1}(-a_{2,1})\dots T_{k,1}(-a_{k,1})\dots T_{n,1}(-a_{n,1})
	\end{cases}$\\
	On obtient $M_0AN_0=\left(\begin{array}{c|c}1&0\\\hline0&A_0\end{array}\right)$.\\
	$A_0$ est inversible puisque $\det(A_0)=\det(M_0AN_0)=\det(M_0)\det(A)\det(N_0)=\det(A)\ne 0$. En effet, les matrices de transvection ont un déterminant qui vaut $1$ car elles sont triangulaires supérieure/inférieure avec des $1$ sur leur diagonale. On peut donc appliquer l'hypothèse de récurrence à $A_0$ : il existe $M_1,N_1$ des produits de matrices de transvection telles que $M_1A_0N_1=D_{n-1}(\det A_0)$.\\
	On pose alors $M=\left(\begin{array}{c|c}1&0\\\hline0&M_1\end{array}\right)M_0$ et $N=N_0\left(\begin{array}{c|c}1&0\\\hline0&N_1\end{array}\right)$ de sorte que,\\
	$MAN=\left(\begin{array}{c|c}1&0\\\hline0&M_1\end{array}\right)\left(\begin{array}{c|c}1&0\\\hline0&A_0\end{array}\right)\left(\begin{array}{c|c}1&0\\\hline0&N_1\end{array}\right)=\left(\begin{array}{c|c}1&0\\\hline0&M_1A_0N_1\end{array}\right)=\left(\begin{array}{c|c}1&0\\\hline0&D_{n-1}(\det A)\end{array}\right)=D_n(\det A)$ et $M,N$ soient des produits de matrices de transvection.\\
	$\left(\begin{array}{c|c}1&0\\\hline0&M_1\end{array}\right)$ est bien une matrice de transvection car pour $(i,j)\in \crblanc{1}{n-1}^2$ et $\lambda\in \R$ tel que $M_1=I_{n-1}+\lambda E^{n-1}_{ij}$, on a $\left(\begin{array}{c|c}1&0\\\hline0&M_1\end{array}\right)=I_n+\lambda E^n_{ij}$. De même, $\left(\begin{array}{c|c}1&0\\\hline0&N_1\end{array}\right)$ est bien une matrice de transvection.\\\\
	\textit{Remarque : on a en fait montré que} SL$_n(\R)$ \textit{est engendré par les matrices de transvection et que} GL$_n(\R)$ \textit{est engendré par les matrices de transvection et les matrices de dilatation.}\\\\
	Revenons à $f$. Pour déterminer un morphisme, il suffit de déterminer l'image d'une partie génératrice. Comme $\Z/m\Z$ est d'ordre $m$, $\forall A\in \text{GL}_n(\R),\ f(A^m)=\overline{m}f(A)=\overline{0}$.\\
	Puisque $T_{ij}(\lambda)=\displaystyle T_{ij}\left(\frac{\lambda}{m}\right)^m$, on a $f(T_{ij}(\lambda))=0$. On en déduit que pour tout $A\in \text{GL}_n(\R),\ f(A)=f(D_n(\det A))$.\\
	Si $m$ est impair, pour tout $\lambda \in \R^*,\ D_n(\lambda)=D_n\left(\sqrt[m]\lambda\right)^m=\overline{0}$ et donc $f$ est le morphisme trivial.\\
	Si $m$ est pair, pour tout $\lambda\in \R^*_+,\ D_n(\lambda)=D_n\left(\sqrt[m]\lambda\right)^m$ donc $f(A)=\overline{0}$ pour tout $A\in \text{GL}_n^+(\R)$. De plus, si $\lambda \in \R^*_-,\ D_n(\lambda)=D_n(-1)D_n(-\lambda)$ avec $-\lambda>0$. Ainsi, $\forall A\in \text{GL}_n^-(\R),\ f(A)=f(D_n(-1))$. Or $D_n(-1)^2=I_n$ donc $f(D_n(-1))=\overline{0}$ ou $f(D_n(-1))=\overline{\displaystyle\frac{m}{2}}$.\\
	Finalement, $f$ est soit le morphisme trivial, soit le morphisme valant $\overline 0$ sur GL$_n^+(\R)$ et $\overline{\displaystyle\frac{m}{2}}$ et GL$_n^-(\R)$.
	
	\subsection{Quasi-morphisme \centraleponts{3}}
	\label{Quasi-morphisme corrigé}
	\textcolor{blue}{\hyperref[Quasi-morphisme]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Supposons par l'absurde que $f$ s'annule en $x\in G$.\\
		Alors, $\forall y\in G,\ |f(y)|=|f(yx^{-1}x)-f(yx^{-1})f(x)|\leq \delta$ ce qui contredit le caractère non bornée de $f$.\\
		Ainsi $f$ ne s'annule pas.
		\item Comme $f$ n'est pas bornée, on sait qu'il existe $(z_n)\in G^\N$ telle que $|f(z_n)|\unfty\longrightarrow+\infty$. Fixons $x\in G$.\\
		On peut écrire $\forall n\in \N,\ \displaystyle\left|\frac{f(xz_n)}{f(z_n)}-f(x)\right|\leq \frac{\delta}{|f(z_n)|}$.\\
		Par passage à la limite, $f(x)=\unfty\lim\displaystyle\frac{f(xz_n)}{f(z_n)}$.
		\item On considère $(z_n)_{n\in \N}$ comme définie précédemment. Fixons $x,y\in G$.\\
		$\unfty\lim\displaystyle\frac{|f(yz_n)|}{|f(z_n)|}=|f(y)|\ne 0$. Donc $(|f(yz_n)|)_{n\in \N}$ diverge vers $+\infty$ et on peut écrire comme $f$ ne s'annule pas :
		$$f(xy)=\unfty\lim\displaystyle\frac{f(xyz_n)}{f(z_n)}=\unfty\lim\frac{f(xyz_n)}{f(yz_n)}\cdot\frac{f(yz_n)}{f(z_n)}=f(x)f(y)$$
	\end{enumerate}
	
	\subsection{Morphisme de $\Z^\N$ dans $\Z$ presque nul \centraleponts{3}}
	\label{Morphisme de Z^N dans Z presque nul corrigé}
	\textcolor{blue}{\hyperref[Morphisme de Z^N dans Z presque nul]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $n\in \N$. $x_n\in \Z$ et $\pgcd(2^n,3^n)=1$ donc d'après le théorème de Bézout, $\exists (y_n,z_n)\in \Z^2,\ 2^ny_n+3^nz_n=x_n$.\\
		\item Soit $s>0$.\\
		$\Phi((2^ny_n))=\Phi(y_0,2y_1,\dots,2^{s-1}y_{s-1},0,0,\dots)+\Phi(0,\dots,0,2^sy_s,2^{s+1}y_{s+1},\dots)=2^s\Phi(0,\dots,0,y_s,2y_{s+1},\dots)$.\\
		Donc $\Phi((2^ny_n))$ est un multiple de $2^s$ pour tout $s>0$ d'où $\Phi((2^ny_n))=0$.\\
		De manière analogue, $\Phi((3^nz_n))=0$ puis $\Phi((x_n))=0$.\\
		Finalement $\Phi=0$.
	\end{enumerate}
	
	\subsection{Groupes de matrices \xens{3}}
	\label{Sous-groupes de GLn(R) corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes de GLn(R)]{[Enoncé]}}\\
	Notons $E$ le neutre de $G$.\\
	Par définition du neutre $E^2=E$ donc $E$ est une matrice de projection. On sait (ou redémontre aisément) alors que $E$ est semblable, par la matrice de passage $P$ de la base canonique de $\K^n$ à une base adaptée à la décomposition $\M_n(\K)=\ker(E-I_n)\bigoplus\ker(E)$, à $J_r=\left(\begin{array}{c|c}
		I_r&0\\
		\hline
		0&0
	\end{array}\right)$.\\
	En particulier l'application $\fonction{f}{\T{GL}_r(\K)}{G}{M}{P^{-1}\left(\begin{array}{c|c}
			M&0\\
			\hline
			0&0
		\end{array}\right)P}$ est un morphisme de groupes injectif.\\
	Ainsi $G$ est isomorphe à $f^{-1}(G)$ qui est un sous-groupe de GL$_r(\K)$.
	
	\subsection{Caractérisation de la finitude d'un groupe par ses sous-groupes \xens{3}}
	\label{Caractérisation de la finitude d'un groupe par ses sous-groupes corrigé}
	\textcolor{blue}{\hyperref[Caractérisation de la finitude d'un groupe par ses sous-groupes]{[Enoncé]}}\\
	Notons $g$ l'ensemble des sous-groupes de $G$.\\
	Si $G$ est fini alors $|\mathcal{P}(G)|=2^{|G|}$. Donc $g\subset \mathcal{P}(G)$ est fini.\\
	Supposons que $g$ est fini.\\
	On écrit $G=\displaystyle\bigcup\limits_{x\in G}<x>$.\\
	Par hypothèse, il existe une sous partie $G_0$ finie de $G$ telle que $\{<x>,\ x\in G\}=\{<x>,\ x\in G_0\}$.\\
	De plus, pour tout $x\in G$, $<x>$ est fini. En effet, Si $<x>$ est infini alors les ensembles $<x^{2^n}>$ pour $n\in \N$ sont des sous-groupes de $G$ tous distincts $\left(<x^{2^{n+1}}>\ \subset\ <x^{2^n}>\text{ et }x^{2^n}\notin <x^{2^{n+1}}>\right)$ et $g$ n'est pas fini.\\
	Donc $|G|=\displaystyle\left|\bigcup\limits_{x\in G_0}<x>\right|\leq \sum\limits_{x\in G_0}|<x>|<+\infty$.
	
	\subsection{Sous-groupe des éléments d'ordre fini \xens{4}}
	\label{Sous-groupes des éléments d'ordre fini corrigé}
	\textcolor{blue}{\hyperref[Sous-groupe des éléments d'ordre fini]{[Enoncé]}}\\
	Notons $H$ le sous-groupe engendré par les éléments de $E$. On va montrer que $E=H$. Par définition, $E\subset H$.\\
	Montrons d'abord que tout élément de $H$ s'écrit comme produit d'éléments distincts de $G$.\\
	On remarque que si $x=ab$ avec $a,b\in E$ alors $x=ba'$ avec $a'=b^{-1}ab\in E$ puisque l'ordre de $a'$ est le même que celui de $a$ en tant que conjugué de celui-ci.\\
	Si $x$ s'écrit $g_1\dots g_n$ avec $g_1,\dots,g_n\in E$ (On peut toujours écrire cette décomposition sans inverse puisque si $x\in E\implies x^{-1}\in E$) et $n$ le nombre minimal d'éléments de $E$ nécessaire pour décomposer $x$ alors, s'il existe $i<j$ tels que $g_i=g_j$, on répète la méthode décrite précédemment pour écrire $x=g_1\dots g_ig_jg_{i+1}'\dots g_{j-1}'g_{j+1}\dots g_n$ avec $g_{i+1}',\dots,g_{j-1}'\in E$. Or $g_ig_j=g_i^2\in E$ et donc on a écrit $x$ comme produit de $n-1$ éléments de $E$ ce qui est absurde.\\
	Notons $r=|H|$. La décomposition d'un élément de $H$ en produit d'élément de $E$ contient toujours au plus $r$ éléments distincts de $E$. On peut donc majorer le cardinal de $H$ par $\displaystyle\sum\limits_{k=0}^rk!$.\\
	Ainsi $H$ est un groupe d'ordre fini, tous ses éléments sont d'ordre fini i.e $H\subset E$.\\
	Finalement $E=H$.
	
	\subsection{Relations d'équivalence naturelles sur les groupes \etoile{1}}
	\label{Relations d'équivalence naturelles sur les groupes corrigé}
	\textcolor{blue}{\hyperref[Relations d'équivalence naturelles sur les groupes]{[Enoncé]}}\\
	Soit $(x,y,z)\in G^3$. On note $e$ le neutre de $G$.\\
	$x=ex=xe=e^{-1}xe$ donc $x\sim_g x,\ x\sim_d x$ et $x\sim x$.\\\\
	Si $x\sim y$ alors il existe $h\in H$ tel que $h^{-1}xh=y$. Alors $x=\left(h^{-1}\right)^{-1}yh^{-1}$ avec $h^{-1}\in H$ puisque $H$ est un groupe, d'où $y\sim x$.\\
	Si $x\sim_d y$ alors il existe $h\in H$ tel que $y=xh$. Alors $x=yh^{-1}$ avec $h^{-1}\in H$ d'où $y\sim_d x$.\\
	Si $x\sim_g y$ alors il existe $h\in H$ tel que $y=hx$. Alors $x=h^{-1}y$ avec $h^{-1}\in H$ d'où $y\sim_g x$.\\\\
	Si $x\sim y\sim z$ alors il existe $h_0,h_1\in H$ tels que $y=h_0^{-1}xh_0$ et $z=h_1^{-1}yh_1$. Alors $z=h_2^{-1}xh_2$ avec $h_2=h_0h_1\in H$ puisque $H$ est un groupe, d'où $x\sim z$.\\
	Si $x\sim_d y\sim_d z$ alors il existe $h_0,h_1\in H$ tels que $y=xh_0$ et $z=yh_1$. Alors $z=xh_2$ avec $h_2=h_0h_1\in H$ d'où $x\sim_d z$\\
	Si $x\sim_g y\sim_g z$ alors il existe $h_0,h_1\in H$ tels que $y=h_0x$ et $z=h_1y$. Alors $z=h_2x$ avec $h_2=h_1h_0\in H$ d'où $x\sim_g z$\\
	Ainsi $\sim,\ \sim_d$ et $\sim_g$ sont des relations d'équivalence sur $G$  
	
	\subsection{\underline{Théorème de Lagrange} \centraleponts{3}}
	\label{Théorème de Lagrange corrigé}
	\textcolor{blue}{\hyperref[Théorème de Lagrange]{[Enoncé]}}\\
	Soit $H$ un sous-groupe de $G$. On note $e$ le neutre de $G$.\\
	On définit la relation binaire $\sim$ sur $G$ par $x\sim y\iff \exists h\in H,\ x=yh$. $\sim$ est une relation d'équivalence :\\
	\begin{itemize}
		\item $\forall x\in G,\ x=xe\implies x\sim x$;
		\item $\forall x,y\in G,\ x\sim y\implies \exists h\in H,\ x=yh\implies \exists h'=h^{-1}\in H,\ y=xh'\implies y\sim x$;
		\item Si $x,y,z\in G$ tels que $x\sim y$ et $y\sim z$ alors il existe $h,h'\in H$ tels que $x=yh$ et $y=zh'$. Alors $h''=h'h\in H$ et $x=zh''$ d'où $x\sim z$.
	\end{itemize}
	Pour $x\in G$ on note $xH$ la classe d'équivalence de $x$ pour la relation $\sim$.
	Pour tout $x\in G$, l'application $h\in H\mapsto xh$ est bijective de $H$ sur $xH$. En effet, elle est clairement surjective et de plus, si $h_1,h_2\in H$ tels que $xh_1=xh_2$ alors en multipliant par $x^{-1}$ à gauche on obtient $h_1=h_2$. Ainsi pour $\forall x\in G,\ |H|=|xH|$.\\
	Notons $x_1,\dots,x_d$ un système de représentant de la relation $\sim$.\\
	$G=\displaystyle\bigsqcup\limits_{k=1}^dx_kH$ donc $|G|=\displaystyle\sum\limits_{k=1}^d|x_kH|=\sum\limits_{k=1}^d|H|=d|H|$.\\
	Ainsi $|H|$ divise $|G|$.
	
	\subsection{Un cas particulier du lemme de Cauchy \xens{4}}
	\label{Un cas particulier du lemme de Cauchy corrigé}
	\textcolor{blue}{\hyperref[Un cas particulier du lemme de Cauchy]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item \begin{enumerate}[label=\alph*.]
			\item $\forall x\in G,\ x^2=e\implies x=x^{-1}$.\\
			Donc $\forall x,y\in G,\ (xy)^2=xyxy=e\implies yx=x^{-1}y^{-1}=xy$
			\item Définissons la loi $+$ sur $G$ par $\forall (x,y)\in G^2,\ x+y=x*y$ et la loi $\cdot$ sur $\Z/2\Z\times G$ par :
			\begin{itemize}
				\item $\forall x\in G,\ \overline{0}x=e$
				\item $\forall x\in G,\ \overline{1}x=x$
			\end{itemize}
			Autrement dit $\forall (a,x)\in \{0,1\}\times G,\ \overline{a}\cdot x=x^a$ ($x^2=e$ assure que cela reste vrai pour $a\in \Z$).
			Vérifions que $(G,+,\cdot)$ est un $\Z/2\Z$ espace-vectoriel.\\
			Tout d'abord $(G,+)$ est un groupe commutatif. Ensuite fixons $a,b\in \{0,1\}$ ainsi que $x,y\in G$.
			\begin{itemize}
				\item $(\overline{a}+\overline{b})\cdot x=\overline{a+b}\cdot x=x^{a+b}=x^ax^b=\overline{a}\cdot x+\overline{b}\cdot x$;
				\item $\overline{a}\cdot(x+y)=\overline{a}\cdot(xy)=(xy)^a=x^ay^a=\overline{a}\cdot x+\overline{a}\cdot y$;
				\item $\overline{a}\cdot(\overline{b}\cdot x)=\overline{a}\cdot x^b=\left(x^b\right)^a=x^{ab}=\overline{ab}\cdot x$;
				\item $\overline{1}x=x$.
			\end{itemize}
			Ainsi $(G,+,\cdot)$ définit bien un espace vectoriel.\\
			Cet espace est fini et donc a fortiori de dimension finie.\\
			Posons $(e_1,\dots,e_p)$ une base de $G$.\\
			Alors $\fonction{\varphi}{G}{(\Z/2\Z)^p}{x=\displaystyle\sum\limits_{i=1}^pa_ie_i}{(a_1,\dots,a_p)}$ est bijective d'où $|G|=2^p$.
		\end{enumerate}
		\item Soit $x\in G\backslash\{e\}$. D'après le théorème de Lagrange, l'ordre $n$ de $x$ divise $2p$. Donc comme $p$ est premier, $n=2$ ou $n=p$ ou $n=2p$. Si $n=p$ alors il n'y a rien à faire. Si $n=2p$ alors $y=x^2$ est d'ordre $p$.\\
		Dans le dernier cas, tous les éléments de $G\backslash\{e\}$ sont d'ordre $2$.\\
		Mais alors d'après la question $1$ $|G|=2^q$ pour un certain entier $q$. Or $p>2$ apparaît dans la décomposition en facteur premier de $|G|$. Ceci est absurde donc il existe un élément dans $G$ qui n'est pas d'ordre $2$.
	\end{enumerate}
	
	\subsection{Groupe d'ordre premier \ccinp{1}}
	\label{Groupe d'ordre premier corrigé}
	\textcolor{blue}{\hyperref[Groupe d'ordre premier]{[Enoncé]}}\\
	Il s'agit de montrer que $G$ possède un élément d'ordre $p$. Soit $x\in G$.\\
	D'après le théorème de Lagrange l'ordre de $x$ divise $p$. Alors comme $p$ est premier, $x$ est d'ordre $1$ ou d'ordre $p$.\\
	Si tous les éléments de $G$ sont d'ordre $1$ alors $G=\{e\}$ n'est pas de cardinal $p\geq 2$.\\
	Ainsi il existe $x\in G$ d'ordre $p$ et donc $G=<x>$.
	
	\subsubsection{Plus petit groupe non commutatif \centraleponts{3}}
	\label{Plus petit groupe non commutatif corrigé}
	\textcolor{blue}{\hyperref[Plus petit groupe non commutatif]{[Enoncé]}}
	\begin{enumerate}
		\item Soit $G$ un groupe cyclique. Il existe $x\in G$ tel que $G=\langle x \rangle$. Donc $G$ est a fortiori commutatif.
		\item Déjà à partir de l'exercice précédent, nous pouvons éliminer les groupes d'ordre 2, 3 et 5. Et évidemment le groupe trivial est commutatif.\\
		Montrons que l'on peut également éliminer $4$.\\
		Soit $G$ un groupe d'ordre $4$.\\ 
		D'après le théorème de Lagrange, l'ordre de tout élément de $G$ divise l'ordre de $G$ ainsi les ordres possibles pour les éléments différents du neutre sont $2$ ou $4$.
		\begin{itemize}
			\item Si $G$ contient un élément $a$ d'ordre $4$ alors, $\langle a\rangle$ est un sous-groupe de $G$ d'ordre $4$. Donc $\langle a\rangle=G$ d'où $G$ est cyclique et puis $G$ est commutatif.
			\item Sinon tous les éléments de $G$ différents du neutre sont d'ordre $2$. D'après l'exercice \ref{Groupe composé d'involutions}, $G$ est commutatif.\\
		\end{itemize}
		Finalement, aucun groupe d'ordre 4 n'est non commutatif.
		l'entier recherché est 6. Il suffit de regarder le groupe symétriques $S_3$ qui n'est pas commutatif car $(12)(23)=(123)\ne(132)=(23)(12)$ par exemple.	
	\end{enumerate}
	
	\subsection{Sous-groupe d'un groupe cyclique \centraleponts{3}}
	\textcolor{blue}{\hyperref[Sous-groupe d'un groupe cyclique]{[Enoncé]}}\\
	\label{Sous-groupe d'un groupe cyclique corrigé}
	Notons $e$ le neutre de $G$.
	Posons $x$ un générateur de $G$ puis posons $y=x^{n/d}$.\\
	Par définition $x$ est d'ordre $n$ donc $y^d=x^n=e$ et $\forall k\in \crblanc{1}{d-1},\ \displaystyle\frac{nk}{d}<n\implies y^k=x^{nk/d}\ne e$. Ainsi $y$ est d'ordre $d$ d'où $\langle y\rangle$ est un sous-groupe (cyclique) de $G$ d'ordre $d$.\\
	Maintenant, soit $H$ un sous-groupe de $G$ d'ordre $d$. Fixons $z\in H$.\\
	Comme $z\in G,\ \exists k\in \crblanc{1}{n},\ z=x^k$.\\
	De plus $z\in H$ donc $z^d=x^{kd}=e$. Alors d'après le théorème de Lagrange $n|kd$ c'est à dire $\displaystyle\frac{n}{d}|k$.\\
	Ainsi $\exists q\in \Z,\ k=q\cdot\displaystyle\frac{n}{d}$. On a alors $z=x^{q\cdot n/d}=y^q$ : $z\in \langle y\rangle$.\\
	Finalement $H\subset \langle y\rangle$ et comme $|H|=d=|\langle y\rangle|,\ H=\langle y\rangle$.
	
	\subsection{Exposant d'un groupe abélien fini \xens{4}}
	\label{Exposant d'un groupe abélien fini corrigé}
	\textcolor{blue}{\hyperref[Exposant d'un groupe abélien fini]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $(xy)^{O(x)O(y)}=\left(x^{O(x)}\right)^{O(y)}\cdot\left(y^{O(y)}\right)^{O(x)}=e$ donc $O(xy)|O(x)O(y)$. De plus $(xy)^{O(xy)}=x^{O(xy)}y^{O(xy)}\implies x^{O(xy)}=y^{-O(xy)}$.\\
		Donc en passant à l'exposant $O(y),\ x^{O(xy)O(y)}=e$ d'où $O(x)|O(xy)O(y)$.\\
		Or $O(x)\land O(y)=1$ donc d'après le lemme de Gauss, $O(x)|O(xy)$.\\
		De même, $O(y)|O(xy)$ puis comme $O(x)\land O(y)=1,\ O(x)O(y)|O(xy)$.\\
		Finalement $O(xy)=O(x)O(y)$.\\
		Dans le cas général on a pas $O(xy)=\ppcm(O(x),O(y))$. Par exemple pour $x=y^{-1}$ avec $x\ne e,\ xy=e$ est d'ordre $1$ alors que $\ppcm(O(x),O(y))=\ppcm(O(x),O(x))=O(x)\ne 1$.
		\item Pour que $m'|m$ et $n'|n$ il faut et il suffit que pour tout $p$ premier $v_p(m')\leq v_p(m)$ et $v_p(n')\leq v_p(n)$. Pour que $m'$ et $n'$ soit premier entre eux il faut et il suffit que pour tout $p$ premier $v_p(n')=0$ ou $v_p(m')=0$. Enfin pour que $\ppcm(m,n)=m'n'$ il faut et il suffit que pour tout $p$ premier $\max(v_p(m),v_p(n))=v_p(m')+v_p(n')$.\\
		On pose donc naturellement $m'=\displaystyle\prod\limits_{p\text{ premier}}p^{\alpha_p}$ et $n=\displaystyle\prod\limits_{p\text{ premier}}p^{\beta_p}$ avec :\\
		$$\begin{cases}
			\alpha_p=v_p(m)&\mbox{ si }v_p(m)\geq v_p(n)\\
			\alpha_p=0&\mbox{ sinon}
		\end{cases}
		\text{ et }
		\begin{cases}
			\beta_p=v_p(n)&\mbox{ si }v_p(n)>v_p(m)\\
			\beta_p=0&\mbox{ sinon}
		\end{cases}$$
		\item Notons $G=\{x_1,\dots,x_n\}$. Montrons par récurrence que pour tout $k\in \crblanc{1}{n},\ \exists z_k\in G,\ O(z_k)=\ppcm(O(x_1),\dots,O(x_k))$.\\
		Pour $k=1,\ z_1=x_1\in G$ et $O(z_1)=O(x_1)$.\\
		Supposons le résultat vrai pour un certain $k\in \crblanc{1}{n-1}$.\\
		D'après la question précédente, il existe $m',n'\in \N^*$ tels que $m'|O(z_k),\ n'|O(x_{k+1}),\ \pgcd(m',n')=1,\ m'n'=\ppcm(O(z_k),O(x_{k+1}))$.\\
		Alors $z_k^{O(z_k)/m'}$ et $x_{k+1}^{O(x_{k+1})/n'}$ sont d'ordres $m'$ et $n'$ respectivement, et d'après la question $1$,\\
		pour $z_{k+1}=z_k^{O(z_k)/m'}x_{k+1}^{O(x_{k+1})/n'}\in G,\ O(z_{k+1})=m'n'=\ppcm(O(x_1),\dots,O(x_{k+1}))$.\\
		Par conséquent la propriété est vrai en particulier au rang $n$ ce qui répond à la question.
		\item Notons $n=|G|$. $G$ est cyclique si et seulement s'il existe un élément de $G$ d'ordre $n$, si et seulement si $G$ est d'exposant $n$. Notons $m$ l'exposant de $G$.\\
		$\forall x\in G,\ x^m=1$. C'est à dire $\forall x\in G,\ x^m-1=0$. Le polynôme $X^m-1$ a donc au moins $n$ racines dans $\K$. Or ce polynôme à au plus $m$ racines dans $\K$. Ainsi $n\leq m$.\\
		Or d'après la question précédente il existe un élément de $G$ d'ordre $m$. Donc $m\leq n$.\\
		On en déduit que $m=n$ et donc que $G$ est cyclique.
	\end{enumerate}
	\textit{Remarque : une conséquence de la question $4$ est le fait que $(\Z/p\Z)^*$ est cyclique.}
	
	\subsection{Un groupe d'inversibles non cyclique \centraleponts{3}}
	\label{Un groupe d'inversibles non cyclique corrigé}
	\textcolor{blue}{\hyperref[Un groupe d'inversibles non cyclique]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On écrit $a=2b+1$ avec $b\in \Z$. Montrons par récurrence sur $k$ le résultat suivant : "pour tout entier $k\geq 1$, $a^{2^k}\equiv 1[2^{k+2}]$".\\
		$k=1$ : $a^{2^1}=a^2=1+4b+4b^2=1+4b(b+1)$. Parmi $b$ et $b+1$ il y a toujours un entier pair donc $a^2\equiv 1[8]$.\\
		Supposons que pour un certain $k\in \N$ il existe $\mu_k\in \Z$ tel que $a^{2^k}=1+\mu_k2^{k+2}$.\\
		Alors $a^{2^{k+1}}=(1+\mu_k2^{k+2})^2=1+2\mu_k2^{k+2}+2^{2k+4}=1+(\mu_k+2^{k+1})2^{k+3}$.\\
		Donc $a^{2^{k+1}}\equiv 1[2^{k+3}]$.\\
		Ainsi pour tout $k\geq 3$, $a^{2^{k-2}}\equiv 1[2^k]$ et en particulier $a^{2^{n-2}}\equiv 1[2^n]$.
		\item On sait que $(\Z/2^n\Z)^\times$ est un groupe d'ordre $\varphi(2^n)=2^n-2^{n-1}=2^{n-1}>2^{n-2}$ et qu'il est composé des classes d'entiers premiers avec $2^n$ càd celles des entiers impairs. Or d'après la question 1 les entiers impairs sont d'ordre au plus $2^{n-2}$ dans $(\Z/2^n\Z)^\times$. Donc $(\Z/2^n\Z)^\times$ n'est pas cyclique.
	\end{enumerate}
	
	\subsection{Ordre dans un groupe de cardinal pair \telecom{2}}
	\label{Ordre dans un groupe de cardinal pair corrigé}
	\textcolor{blue}{\hyperref[Ordre dans un groupe de cardinal pair]{[Enoncé]}}\\
	\begin{enumerate}
		\item
		Réflexivité : Trivial, pour tout $x\in G, x=x$\\
		Symétrie : Par symétrie de l'égalité et par unicité de l'inverse, pour tout $x,y \in G, x=y \Rightarrow y=x $ et $ x=y^{-1} \Rightarrow y=x^{-1}$\\
		Transitivité : Soit $x,y,z$ tel que $x\mathcal{R} y$ et $y\mathcal{R} z$.
		\begin{itemize}
			\item Si $x=y$ alors on a immédiatement $x\mathcal{R} z$.
			\item Si $x=y^{-1}$, on sait que : $y=z$ ou $y=z^{-1}$ donc $x^{-1}=z$ ou $x^{-1}=z^{-1}$. D'où le résultat. 
		\end{itemize}
		Ainsi cette relation $\mathcal{R}$ sur $G$ est une relation d'équivalence sur $G$.
		\item On remarque que les classes d'équivalence sont de la forme : $\text{cl}(x)=\{x,x^{-1}\}$ pour tout $x\in G$. On remarque également que $\text{cl}(e)=\{e\}$. Ainsi chaque classe d'équivalence contient $1$ ou $2$ éléments. Et puisque $\text{cl}(e)$ contient un unique élément et que le cardinal de $G$ est pair, il existe une autre classe $\text{cl}(x)$ avec $x\ne e$ de cardinal $1$.\\
		Ainsi $x=x^{-1}$ cad $x^2=e$.
		Par conséquent, G admet un élément d'ordre deux.
	\end{enumerate}
	
	\subsection{Ordre dans $\Z/n\Z$ \telecom{2}}
	\label{Ordre dans Z/nZ corrigé}
	\textcolor{blue}{\hyperref[Ordre dans Z/nZ]{[Enoncé]}}\\
	D'une part, $\displaystyle\frac{k}{k\wedge n}\in \N\implies\frac{n}{n\wedge k}\overline k=\frac{k}{n\wedge k}\overline n=\overline 0$. Donc $d|\displaystyle\frac{n}{n\wedge k}$.\\\\
	D'autre part, $\overline{dk}=d\overline k=\overline 0$ donc $n|dk$.\\
	Alors $\displaystyle\frac{n}{n\wedge k}|d\cdot\frac{k}{n\wedge k}$. Or $\pgcd\left(\displaystyle\frac{n}{n\land k},\frac{k}{n\wedge k}\right)=1$ donc d'après le lemme de Gauss, $\dfrac{n}{n\wedge k}|d$.\\
	Ainsi comme $d\geq 0$ et $\displaystyle\frac{n}{n\wedge k}\geq 0,\ d=\frac{n}{n\wedge k}$.
	
	\subsection{Passage par les groupes \centraleponts{2}}
	\label{Passage par les groupes corrigé}
	\textcolor{blue}{\hyperref[Passage par les groupes]{[Enoncé]}}\\
	\begin{enumerate}
		\item On peut déjà éliminer le cas où n=2, car $2\nmid3$. On sait que $2\wedge p=1$ donc $2^{p-1}\equiv 1[p]$ d'après le lemme de Fermat.\\ Donc $m|p-1$.
		\item On a supposé que $n|2^n-1.$ Donc $2^n\equiv 1[n]$. Et puisque $p|n$, $2^n\equiv1[p]$. Donc $m|n$.
		\item Puisque $p$ est le plus petit diviseur premier de $n$ et que $m|n$ on n'en déduit que tous les facteurs premiers de $m$ sont supérieurs ou égals à $p$. Donc $m\geq p$\\
		Mais puisque $m|p-1$, cela signifie que $m<p$.\\
		On obtient donc une absurdité. Ainsi il n'existe pas de $n\geq 2$ tel que $n|2^n -1$. 
	\end{enumerate}
	
	\subsection{Groupe infini non monogène \ccinp{1}}
	\label{Groupe infini non monogène corrigé}
	\textcolor{blue}{\hyperref[Groupe infini non monogène]{[Enoncé]}}\\
	$(\Z^2,+)$ n'est pas monogène.\\
	$\forall (a,b)\in \Z^2,\ \langle (a,b)\rangle=\{(ka,kb),\ k\in \Z\}\ne \Z^2$.
	
	\subsection{Groupe non cyclique \telecom{2}}
	\label{Groupe non cyclique corrigé}
	\textcolor{blue}{\hyperref[Groupe non cyclique]{[Enoncé]}}\\
	Si $m\land n=1$ alors d'après le théorème des restes chinois on sait que $\Z/n\Z\times\Z/m\Z$ est isomorphe à $\Z/mn\Z$ en tant qu'anneau. Donc $(\Z/n\Z\times\Z/m\Z,+)$ est isomorphe à $(\Z/mn\Z,+)$ d'où $(\Z/n\Z\times\Z/m\Z,+)$ est cyclique.\\
	Réciproquement supposons que $(\Z/n\Z\times\Z/m\Z,+)$ est cyclique. Posons $(a,b)$ un générateur. On notera $\overline k$ et $\tilde k$ les classes respectives d'un entier $k$ dans $\Z/n\Z$ et $\Z/m\Z$.\\
	Comme $m\vee n$ est un multiple de $m$ et de $n$, $(m\vee n)a=\overline{0}$ et $(m\vee n)b=\tilde 0$. Donc l'ordre de $(a,b)$, qui vaut $mn$ en tant que générateur, divise $m\vee n$.\\
	Or $m\wedge n\times m\vee n=mn$. Donc $m\wedge n$ divise $1$ c'est à dire $m\wedge n=1$.
	
	\subsection{Ordre dans le groupe symétrique \ccinp{2}}
	\label{Ordre dans le groupe symétrique corrigé}
	\textcolor{blue}{\hyperref[Ordre dans le groupe symétrique]{[Enoncé]}}\\
	Soient $n\in \N^*$ et $\sigma\in \mathcal S_n$.\\
	On note $\sigma=\displaystyle\prod_{i=1}^pc_i$ la décomposition de $\sigma$ en produit de cycles à supports disjoints.\\
	On sait que deux cycles à supports disjoints commutent. Donc $\forall k\in \N^*,\ \sigma^k=\displaystyle\prod_{i=1}^pc_i^k$.\\
	Notons pour $1\leq i\leq p,\ m_i$ l'ordre de $c_i$ et posons $m=\ppcm(m_1,\dots,m_p)$. par unicité de la décomposition en produit de cycles à supports disjoints :
	\begin{align*}
		\forall k\in \N^*,\ &\sigma^k=\Id\\
		\iff&\forall i\in \crblanc{1}{p},\ c_i^k=\Id\\
		\iff&\forall i\in \crblanc{1}{p},\ m_i|k\\
		\iff& m|k
	\end{align*}
	Ainsi $\sigma$ est d'ordre $m$.
	
	\subsection{Sous-groupe engendré par les nombres premiers \ccinp{1}}
	\label{Sous-groupe engendré par les nombres premiers corrigé}
	\textcolor{blue}{\hyperref[Sous-groupe engendré par les nombres premiers]{[Enoncé]}}\\
	On a : \[\langle \mathcal{P}\rangle=\left\{\prod_{i=1}^{r}p_i^{k_i}, r\in\N ,\forall i\in\crblanc{1}{r}, p_i\in \mathcal{P}, k_i\in\Z\right\}\]
	Il est clair que : \[\langle \mathcal{P}\rangle \subset \Q^*\]
	Soit $q=\frac{a}{b}$.\\
	D'après la décomposition en facteurs premiers de $a$ et $b$, on en déduit que $q\in \langle \mathcal{P}\rangle$.\\
	Par conséquent, \[\langle \mathcal{P}\rangle=\Q^*\]
	
	\subsection{Sous-groupe engendré par le complémentaire d'un sous-groupe \telecom{2}}
	\label{Sous-groupe engendré par le complémentaire d'un sous-groupe corrigé}
	\textcolor{blue}{\hyperref[Sous-groupe engendré par le complémentaire d'un sous-groupe]{[Enoncé]}}\\
	On note $e$ le neutre de $G$. On notera $\langle E\rangle$ le sous-groupe engendré par une partie $E$.
	Commençons par des exemples.
	\begin{itemize}
		\item $\Z\backslash a\Z$ contient $1$ donc tous ses multiples i.e $\langle\Z\backslash a\Z\rangle=\Z$.
		\item Si $e^{i\theta}\in \U$ alors $e^{i\theta}=2e^{i\theta}\times 2^{-1}\in \langle\C^*\backslash\U\rangle$. $\langle\C^*\backslash\U\rangle=\C^*$.
		\item $\mathcal S_n\backslash\mathcal A_n$ est l'ensemble des permutations de signature $-1$. Il contient toutes les transpositions. Celle-ci engendrant $\mathcal S_n$, $\langle S_n\backslash\mathcal A_n\rangle=\mathcal S_n$.
	\end{itemize}
	Il semble que le résultat attendu soit $\langle G\backslash H\rangle=G$. Notons $E=G\backslash H$ et montrons $H\subset \langle E\rangle$. Par hypothèse il existe $x\in E$. Soit $y\in H$. On pose $z=xy$.\\
	Si $z\in H$ alors $x=zy^{-1}\in H$ car $H$ est un groupe, ce qui est exclus. Donc $z\in E$ et par suite $x^{-1}z=y\in\langle E\rangle$.\\
	Ainsi $H\subset \langle E\rangle$. De plus $E\subset \langle E\rangle$ donc $G=H\cup E\subset \langle E\rangle$ i.e $\langle G\backslash H\rangle=G$.
	
	\subsection{Partie génératrice \etoile{3} (HP)}
	\label{Partie génératrice corrigé}
	\textcolor{blue}{\hyperref[Partie génératrice]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item D'après le théorème de Lagrange $\dfrac{|G|}{|H|}$ est un entier supérieur strictement à $1$, c'est donc au moins $2$ i.e $2|H|\leq |G|$.
		\item $G$ possède une partie génératrice (lui même), on peut donc considérer une partie génératrice de $G$ de cardinal minimal. Soit $E=\{x_1,\dots,x_m\}$ une telle partie. $m\leq \log_2(n)\iff 2^m\leq n$.\\
		Posons pour $i\in \crblanc{1}{m},\ G_i=\langle x_1,\dots,x_i\rangle$.\\
		On remarque que $\forall i\in \crblanc{1}{m-1},\ G_i\subsetneq G_{i+1}$. En effet, si pour un certain $i_0\in \crblanc{1}{m-1},\ G_{i_0}=G_{i_0+1}$ alors $\{x_1,\dots,x_{i_0},x_{i_0+2},\dots,x_m\}$ est une partie génératrice de $G$ ayant strictement moins d'éléments que $E$ ce qui est absurde.\\
		Soit $i\in \crblanc{1}{m-1}$. $G_i$ est un sous-groupe strict de $G_{i+1}$ donc d'après la question 1, $2|G_i|\leq |G_{i+1}|$.\\
		Par récurrence immédiate $n=|G|=|G_m|\geq 2^m$.
		\item Soit $G=((\Z/2\Z)^m,+)$. $G$ est un $\Z/2\Z$-espace vectoriel de dimension $m$, il est donc de cardinal $m$ (même sans théorie sur les espaces vectoriels sur des corps finis on peut vérifier à la main qu'il y a unicité de l'écriture dans la base canonique).\\
		Soit $E$ une partie de $G$ de cardinal inférieur strictement à $m$. D'après la question 1 de l'exercice sur le théorème de Cauchy, le cardinal de $\langle E\rangle$ est une puissance de $2$. Par définition de $E$, $|\langle E\rangle|<|G|$.
	\end{enumerate}
	
	\subsection{Groupe alterné \centraleponts{3}}
	\label{Groupe alterné corrigé}
	\textcolor{blue}{\hyperref[Groupe alterné]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item L'application signature $\varepsilon :\mathcal S_n\to \{-1,1\}$ est un morphisme de groupes donc $\mathcal A_n=\ker \varepsilon$ est un sous-groupe de $\mathcal S_n$.
		\item On sait que tout élément de $\mathcal S_n$ peut s'écrire comme composition de transpositions. La signature d'une transposition valant $-1$, tout élément de $\mathcal A_n$ s'écrit comme composition d'un nombre pair de transpositions. Quitte à rassembler ces transpositions deux par deux, il suffit de montrer que la composée de $2$ transposition peut toujours s'écrire comme une composition de $3$-cycle :\\
		Soient $\tau_1,\tau_2$ deux transpositions :\\
		Si $\tau_1=\tau_2,\ \tau_1\circ\tau_2=\Id=(1,2,3)\circ(2,3,1)$;\\
		Si $\tau_1=(i,j)$, et $\tau_2=(j,k)$ avec $i,j,k$ distincts alors $\tau_1\circ\tau_2=(i,j,k)$;\\
		Si $\tau_1=(i,j)$, et $\tau_2=(k,l)$ avec $i,j,k,l$ distincts alors $\tau_1\circ\tau_2=(i,j,k)\circ(j,k,l)$.
	\end{enumerate}
	
	\subsection{Cardinal minimal d'une famille de transpositions engendrant $\S_n$ \centraleponts{3}}
	\label{Cardinal minimal d'une famille de transpositions engendrant Sn corrigé}
	\textcolor{blue}{\hyperref[Cardinal minimal d'une famille de transpostions engendrant Sn]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On rappelle que $\S_n$ est engendré par les transpositions.\\
		Ainsi, il suffit de montrer que toutes transpositions se décompose dans $(t_2,\dots,t_n)$.\\
		Soit $(i,j)\in \crblanc{1}{n}^2$ \[(i \, j)= (1 \, i)(1 \, j)(1 \, i)\]
		Donc $\{t_2,\dots,t_n\}$ engendre $\S_n$.
		\item Pour la transposition $s=(i \, j)$, géométriquement, cela revient à dire que $u$ est une réflexion orthogonale d'hyperplan \[H=\{x\in \R^n, x_i=x_j\}\]
		\item \textit{Première méthode} : Si $s$ est le produit de $p$ transpositions alors $\Ker(u_s-\Id_E)$ contient l'intersection des $p$ hyperplans associées à chacune des transpositions.\\
		Or, on remarque que $\Ker(u_s-\Id_E)=\Vect(e_1+\dots+e_n)=1$ d'où le résultat.\\
		\textit{Deuxième méthode} :On observe que lorsque l'on multiplie une permutation $\sigma$ par une transposition, cela revient à rassembler deux cycles ou au contraire à couper en 2 un cycle.\\
		Ainsi, en partant de l'identité qui a $n$ cycles, on en déduit que $s$ a au moins $n-p$ cycles d'après ce qui précède. Ainsi, puisque $s$ est un $n$-cycle, il a un unique cycle, donc on en déduit que $n-p\leq 1$, d'où $p\geq n-1$.
		\item D'après ce qui précède, on en déduit que $n-1$ est ce minimum.
	\end{enumerate}
	
	\subsection{Partie génératrice de $\O(E)$ \xens{4}}
	\label{Partie génératrice de O(E) corrigé}
	\textcolor{blue}{\hyperref[Partie génératrice de O(E)]{[Enoncé]}}\\
	On rappelle qu'une réflexion (orthogonale) $s$ de $E$ est une symétrie orthogonale par rapport à un hyperplan $H$ de $E$. De plus, si l'on note $a\in E$ tel que $H=\Vect(a)^\bot$ alors $\forall x\in E,\ s(x)=x-2\dfrac{\proscal{a}{x}}{\norme{a}^2}a$. En effet $\forall x\in E,\ \exists h\in H,\ x=\displaystyle\proscal{\frac{a}{\norme a}}{x}\frac{a}{\norme a}+h$. Donc $s(x)=\displaystyle\frac{\proscal{a}{x}}{\norme{a}^2}s(a)+s(h)=-\frac{\proscal{a}{x}}{\norme{a}^2}a+h=x-2\frac{\proscal{a}{x}}{\norme{a}^2}a$.\\
	Par récurrence sur $n=\dim(E)$ :\\
	$n=1$ : $\O(E)=\{\Id,-\Id\}$ est le groupe engendré par $-\Id$ qui est un réflexion.\\
	Supposons le résultat vrai pour un certain $n\in \N^*$. Soit $E$ un espace euclidien de dimension $n+1$. Soit $u\in \O(E)$. Si $u=\Id$ alors $u$ est le produit de $0$ réflexions. Sinon $\exists x\in E\backslash\{0\},\ u(x)\ne x$. Alors il existe une réflexion orthogonale qui échange $u(x)$ et $x$. En effet, $y=u(x)-x\ne 0$ donc $\Vect(y)^\bot$ est un hyperplan de $E$ et la symétrie orthogonale $s$ associée vérifie
	\begin{align*}
		s(u(x))&=u(x)-2\frac{\proscal{y}{u(x)}}{\norme{y}^2}y\\
		&=u(x)-2\frac{\proscal{u(x)-x}{u(x)}}{\norme{u(x)-x}^2}y\\
		&=u(x)-2\frac{\norme{u(x)}^2-\proscal{u(x)}{x}}{\norme{u(x)}^2-2\proscal{u(x)}{x}+\norme{x}^2}y\\
		&=u(x)-2\frac{\norme{x}^2-\proscal{u(x)}{x}}{2(\norme{x}^2-\proscal{u(x)}{x})}y\quad\quad\quad\quad\quad\quad (\T{car }u\in \O(E))\\
		&=u(x)-y\\
		&=x
	\end{align*}
	Ainsi $s\circ u$ stabilise $\Vect(x)$ et donc $(s\circ u)^*=u^*\circ s^*=u^{-1}\circ s$ stabilise $H:=\Vect(x)^\bot$. Il induit donc un endomorphisme (orthogonal car $u^{-1}\circ s$ l'est) $v$ sur $H$. Par hypothèse de récurrence $v$ s'écrit comme un produit de réflexions de $H$ (on peut l'écrire sans inverse car une réflexion est une involution) $v=\tilde s_1\dots\tilde s_m$.\\
	Pour conclure il suffit de définir $s_i\in \L(E)$ par ${s_i}_{|H}=\tilde s_i$ et $s_i(x)=x$ pour tout $1\leq i\leq m$ de sorte que $s_i^2=\Id_E$ et $\ker(s_i-\Id_E)=\ker(\tilde s_i-\Id_H)\bigoplus\Vect(x)$ soit un hyperplan de $E$.\\
	On a alors $u=ss_m\dots s_1$.\\\\
	\textit{\underline{Remarque :} La même preuve en adaptant l'hypothèse de récurrence montre en fait que tout endomorphisme orthogonal de E est produit d'au plus dim(E) réflexions.}
	
	\subsection{Groupe dans un plan euclidien \centraleponts{3}}
	\label{Groupe dans un plan euclidien corrigé}
	\textcolor{blue}{\hyperref[Groupe dans un plan euclidien]{[Enoncé]}}\\
	On note dans tout l'exercice $B=(e_1,e_2)$ une base orthonormée de $E$.
	\begin{enumerate}
		\item On sait que $\fonction{\varphi}{\mathcal{SO}(E)}{\mathcal{SO}_2(\R)}{u}{\Mat_B(u)}$ et on montre que $\fonction{\psi}{\mathcal{SO}_2(\R)}{\U}{\begin{pmatrix}
				\cos\theta&-\sin\theta\\
				\sin\theta&\cos\theta
		\end{pmatrix}}{e^{i\theta}}$ sont des isomorphismes de groupes. Ainsi on peut reporter l'étude au sous-groupes finis de $\U$.\\
		Celle-ci est simple puisque si $H$ est un sous-groupe fini de $\U$ alors $\forall z\in H,\ z^{|H|}=1$ d'où $H\subset\U_{|H|}$ puis $H=\U_{|H|}$ par égalité des cardinaux.\\
		En notant $r_\theta=\varphi^{-1}\left(\begin{pmatrix}
			\cos\theta&-\sin\theta\\
			\sin\theta&\cos\theta
		\end{pmatrix}\right)$ on déduit de ce qui précède que les sous-groupes fini de $\mathcal{SO}(E)$ sont les $U_n:=\{r_{2k\pi/n},\ 0\leq k\leq n-1\}$ pour $n\in \N^*$.
		\item On rappelle que $\O(E)$ ne contient que des rotations et des réflexions, qui sont distinguées par le signe de leurs déterminants.\\
		Soit $G$ un sous-groupe fini de $\O(E)$. On note $G^+$ (resp. $G^-$) l'ensemble des automorphismes de $G$ dont le déterminant est positif (resp. négatif). $G^+$ n'est pas vide puisque si $s\in G$ est une symétrie, $s^2=I_2\in G^+$.\\
		$G^+=G\cap\mathcal{SO}(E)$ est un sous-groupe fini de $\mathcal{SO}(E)$. Il est donc de la forme $U_n$ pour un certain $n\in \N^*$.\\
		Si $G^-$ est vide on a fini. Sinon, on dispose d'une symétrie $s\in G$. Alors $\fonction{f}{G^-}{G^+}{u}{s\circ u}$ est clairement une application bijective.\\
		Ainsi, $G^-=sU_n$. Finalement en notant $r=r_{2\pi/n},\ G=\{\Id_E,r,\dots,r^{n-1},s,sr,\dots,sr^{n-1}\}=\langle r,s\rangle$ (tous ces éléments sont bien distincts car $G^+\cap G^-=\emptyset$).\\
		Finalement $G$ en notant est de la forme :
		$$\langle r_{2\pi/n}\rangle\T{ ou }\langle r_{2\pi/n},s\rangle$$
		avec $n\in \N^*$ et $s$ une réflexion de $E$.\\\\
		\textit{\underline{Remarque :} le groupe engendré par les matrices de $r_{2\pi/n}$ et de la réflexion d'axe $e_1$ (càd $\begin{pmatrix}
				1&0\\
				0&-1
			\end{pmatrix}$) est appelé groupe diédral d'ordre $2n$. Quitte à changer la base au départ on peut toujours se ramener à cette matrice. On a donc montré que les sous-groupes finis de $\O_2(\R)$ sont cycliques ou diédraux. L'exercice suivant s'intéresse de plus près aux groupes diédraux.}
		\item Il me manque tellement rien pour y arriver sans gros théorème de Maschke :[
	\end{enumerate}
	
	\subsection{Groupe diédral \centraleponts{3}}
	\label{Groupe diédral corrigé}
	\textcolor{blue}{\hyperref[Groupe diédral]{[Enoncé]}}
	\begin{enumerate}[leftmargin=*]
		\item $R$ est une matrice de rotation et $S$ une matrice de symétrie donc $D_n$ est un sous-groupe de $\O_2(\R)$. On rappelle $\O_2(\R)$ est composé seulement de matrices rotations et de réflexions, qui sont distinguées (hormis $I_2$) par le signe de leur déterminant.\\
		Tout d'abord, $SR=R^{-1}S\iff SR=(SR)^{-1}\iff (SR)^2=I_2$. Autrement dit il s'agit de montrer que $SR$ est une symétrie. Cela est vrai car $\det(RS)=\det(R)\det(S)=-1$.\\\\
		\textit{Remarque : Ici le calcul matriciel est simple mais la méthode présentée fonctionne pour n'importe quelle rotation et n'importe quelle réflexion.}
		\item Commençons par remarquer que comme $R$ est la matrice de rotation d'angle $\dfrac{2\pi}{n}$, elle est d'ordre $n$ càd $\langle R\rangle=\{I_2,R,\dots,R^{n-1}\}$. En particulier $R^{-1}=R^{n-1}$. Ainsi avec $S^2=I_2$, la question 1 justifie qu'un mot de $\langle R,S\rangle$ s'écrit toujours $R^iS^j$ avec $0\leq i\leq n-1$ et $j\in \{0,1\}$.\\
		Reste à montrer que ces éléments sont tous distincts. On a déjà justifier que les $R^i$, $1\leq i\leq n-1$ le sont. Il sont distincts des $R^iS$ car ces derniers sont des symétries non triviales et non des rotations. Enfin, les $R^iS$ sont distincts entre eux car $R^i\mapsto R^iS$ est une bijection.\\
		Ainsi $D_n=\{I_2,R,\dots,R^{n-1},S,SR,\dots,SR^{n-1}\}$.
		\item $D_1=\{I_2,S\}$ et $D_2=\{I_2,-I_2,S,-S\}$ sont clairement abéliens. Et si $n\geq 3$, $R^{-1}$ est la matrice de rotation d'angle $-\dfrac{2\pi}{n}\not\equiv\dfrac{2\pi}{n}[2\pi]$. Donc $SR=R^{-1}S\ne RS$.
		\item Une approche pratique pour les notations est de travailler dans $\C$. On va montrer que les points $z_k$ d'affixes $e^{2ik\pi/n}$ pour $0\leq k\leq n-1$ sont les sommets de $G_n$ dans le plan complexe. $z_0=e^{i0}=1$ donc il suffit de montrer que les distances $|z_{k+1}-z_k|$ sont toutes égales.\\
		On calcule $|z_{k+1}-z_k|=|e^{2i\pi/n}z_k-z_k|=|e^{2i\pi/n}-1||z_k|=|e^{2i\pi/n}-1|$.\\
		Ainsi les coordonnées des sommets de $G_n$ sont $\left(\cos\left(\dfrac{2k\pi}{n}\right),\sin\left(\dfrac{2k\pi}{n}\right)\right)$ pour $0\leq k\leq n-1$.
		\item Une composée de transformations du plan qui conservent une figure conserve cette même figure. Les isométries vectorielles de $\R^2$ formant un groupe, $\Delta_n$ est bien un groupe.\\
		Ensuite, en notant $\B=((1,0),(0,1))$ la base canonique de $\R^2$ on dispose d'un isomorphisme de $\O(\R^2)$ dans $\O_2(\R)$ donné par $\varphi:u\mapsto \Mat_\B(u)$. Montrons que $\varphi(\Delta_n)=D_n$. En premier $G_n$ est invariant par rotation d'angle $\dfrac{2\pi}{n}$ et par symétrie par rapport à l'axe des abscisses. Donc $\varphi^{-1}(D_n)\subset\Delta_n$.\\
		Réciproquement, si $u\in \Delta_n$ alors :
		\begin{itemize}
			\item Si $\det(u)>0$, $u$ est une rotation dont on note $\theta$ l'angle. $u(1,0)=(\cos\theta,\sin\theta)$ est un des sommets de $G_n$, autrement dit $\exists k\in \crblanc{0}{n-1},\ \theta=\dfrac{2k\pi}{n}$. Et on donc $\varphi(u)=R^k\in D_n$.
			\item Si $\det(u)<0$, alors en notant $s=\varphi^{-1}(S)$, $s\circ u\in \Delta_n$ avec $\det(s\circ u)>0$, donc $\varphi(s\circ u)=R^k$ pour un certain $0\leq k\leq n-1$, càd $\varphi(u)=\varphi(s)^{-1}R^k=S^{-1}R^k=SR^k\in D_n$.
		\end{itemize}
		Dans tous les cas, $\varphi(u)\in D_n$. Donc $\varphi(\Delta_n)\subset D_n$ puis $\varphi(\Delta_n)=D_n$.
	\end{enumerate}
	
	\subsection{Matrices de permutation \etoile{1}}
	\label{Matrices de permutation corrigé}
	\textcolor{blue}{\hyperref[Matrices de permutation]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $(\sigma,\tau)\in \mathcal S_n^2$. Fixons $(i,j)\in \crblanc{1}{n}^2$.\\
		$(f(\sigma)f(\tau))_{i,j}=\displaystyle\sum\limits_{k=1}^nf(\sigma)_{i,k}f(\tau)_{k,j}=\sum\limits_{k=1}^n\delta_{i,\sigma(k)}\delta_{k,\tau(j)}=\delta_{\sigma^{-1}(i),\tau(j)}$.\\
		Or $\sigma^{-1}(i)=\tau(j)\iff i=\sigma\circ\tau(j)$. Donc $\delta_{\sigma^{-1}(i),\tau(j)}=\delta_{i,\sigma\circ\tau(j)}$.\\
		Ainsi $f(\sigma)f(\tau)=P_{\sigma\circ\tau}=f(\sigma\circ\tau)$.\\
		Donc $f$ est un morphisme et on en déduit que $\mathcal P_n=\text{Im}(f)$ est un sous-groupe de GL$_n(\R)$.\\
		De plus, $\forall \sigma\in \mathcal S_n,\ f(\sigma)=I_n\implies \forall i\in \crblanc{1}{n},\ \delta_{i,\sigma(i)}=1\implies \forall i\in \crblanc{1}{n},\ \sigma(i)=i\implies \sigma=\Id$. Donc $f$ est un isomorphisme de groupes.
		\item $\forall \sigma\in \mathcal S_n,\ \forall (i,j)\in \crblanc{1}{n}^2,\ (P_\sigma^\top)_{i,j}=\delta_{j,\sigma(i)}=\delta_{\sigma^{-1}(j),i}=(P_{\sigma^{-1}})_{i,j}=(P_\sigma^{-1})_{i,j}$.\\
		Donc $\forall \sigma\in \mathcal S_n,\ P_\sigma^\top=P_\sigma^{-1}$.\\
		On a montré que $\mathcal P_n\subset \O_n(\R)$.
		\item Soit $\sigma\in \mathcal S_n$.\\
		Par la formule du déterminant : On sait que $\det(P_\sigma)=\displaystyle\sum\limits_{\tau\in \mathcal S_n}\varepsilon(\tau)\prod\limits_{i=1}^n(P_\sigma)_{i,\tau(i)}=\sum\limits_{\tau\in \mathcal S_n}\varepsilon(\tau)\prod\limits_{i=1}^n\delta_{i,\sigma\circ\tau(i)}$.\\
		Or $\tau\ne \sigma^{-1}\implies \exists i\in \crblanc{1}{n},\ i\ne \sigma\circ\tau(i)\implies \displaystyle\prod\limits_{i=1}^n\delta_{i\sigma\circ\tau(i)}=0$ et, $\tau=\sigma^{-1}\implies \forall i\in \crblanc{1}{n},\ i=\sigma\circ\tau(i)\implies \displaystyle\prod\limits_{i=1}^n\delta_{i\sigma\circ\tau(i)}=1$.\\
		Finalement $\det(P_\sigma)=\varepsilon(\sigma^{-1})=\varepsilon(\sigma)^{-1}=\varepsilon(\sigma)$.\\
		Par la définition de la signature :\\
		On rappelle que dans le programme de MPSI l'application signature est définie comme l'unique morphisme de groupes de $S_n$ dans $\{-1,1\}$ qui vaut $-1$ sur les transpositions.\\
		$\det\circ f=\sigma\mapsto\det(P_\sigma)$ est bien un morphisme de groupes de $S_n$ dans $\{-1,1\}$.\\
		Fixons une transposition $\tau=(i,j)\in \mathcal S_n$ (avec $i<j$). En notant $(E_1,\dots,E_n)$ la base canonique de $\M_{n,1}(\R)$ on a :
		\begin{align*}
			\det(P_\tau)&\ \ \ \ =\det\left(\begin{array}{c|c|c|c|c|c|c|c|c|c|c}
				E_1&\dots&E_{i-1}&E_j&E_{i+1}&\dots&E_{j-1}&E_i&E_{j+1}&\dots&E_n
			\end{array}\right)\\
			&\underset{E_i\longleftrightarrow E_j}{=}-\det\left(\begin{array}{c|c|c|c|c|c|c|c|c|c|c}
				E_1&\dots&E_{i-1}&E_i&E_{i+1}&\dots&E_{j-1}&E_j&E_{j+1}&\dots&E_n
			\end{array}\right)\\
			&\ \ \ \ =-\det(I_n)\\
			&\ \ \ \ =-1
		\end{align*}
		D'où $\det(P_\sigma)=\varepsilon(\sigma)$.
		\textit{Remarque} : On peut simplement regarder le déterminant des colonnes et remarquer que les colonnes de $P_\sigma$ correspond à la permutation $\sigma$ des colonnes de $I_n$ d'où le résultat car $\det$ est alterné.
	\end{enumerate}
	
	\subsection{Groupe dérivé \etoile{4}}
	\label{Groupe dérivé corrigé}
	\textcolor{blue}{\hyperref[Groupe dérivé]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Notons pour $(x,y)\in G^2,\ [x,y]=xyx^{-1}y^{-1}$. Fixons $(x,y)\in G^2$\\
		$[x,y]^2=(xyx^{-1}y^{-1})(xyx^{-1}y^{-1})=x^2(x^{-1}y)^2(y^{-1})^2$ donc $[x,y]\in C$.\\
		Ainsi $D\subset C$.
		\item Soit $x\in G$. $\exists x_1,\dots,x_n\in G,\ x=x_1\dots x_n\land\forall k\in \crblanc{1}{n},\ x_k=x_k^{-1}$.\\
		On montre par récurrence que pour tout $i\in \crblanc{2}{n},\ x_i^{-1}\dots x_2^{-1}=(x_2\dots x_i)^{-1}$ :\\
		Pour $i=2,\ x_2=x_2^{-1}$. Si le résultat est vraie pour un certain $i\in \crblanc{2}{n-1}$ alors,\\
		$(x_2\dots x_{i+1})^{-1}=x_{i+1}^{-1}(x_2\dots x_i)^{-1}=x_{i+1}^{-1}\dots x_2^{-1}$.\\
		Ainsi, $x^2=(x_1\dots x_n)(x_1\dots x_n)=x_1(x_2\dots x_n)x_1^{-1}(x_2\dots x_n)^{-1}(x_2\dots x_n)^2=[x_1,x_2\dots x_n](x_2\dots x_n)^2$.\\
		On va donc raisonner par récurrence sur $n$ :\\
		$n=1$ : $x^2=x_1^2\in D$.\\
		$n=2$ : $x^2=x_1x_2x_1x_2=x_1x_2x_1^{-1}x_2^{-1}=[x_1,x_2]\in D$.\\
		Fixons $n\geq 1$ et supposons que le produit de $n-1$ involutions ($y^{-1}=y$) de $G$ est dans $D$.\\
		Alors $x^2=[x_1,x_2\dots x_n](x_2\dots x_n)^2\in D$ comme produit de deux éléments de $D$.\\
		Ainsi $C\subset D$ d'où $D=C$.
		\item
		\begin{enumerate}[label=\alph*.]
			\item Soit $M=\begin{pmatrix}\cos(\theta)&-\sin(\theta)\\\sin(\theta)&\cos(\theta)\end{pmatrix}\in \text{SO}_2(\R)$. Soient $A=\begin{pmatrix}\cos(\alpha)&\sin(\alpha)\\\sin(\alpha)&-\cos(\alpha)
			\end{pmatrix}$ et $B=\begin{pmatrix}\cos(\beta)&\sin(\beta)\\\sin(\beta)&-\cos(\beta)
			\end{pmatrix}$ deux réflexions.\\
			$M=AB\iff \begin{cases}
				\cos(\theta)=\cos(\alpha-\beta)\\
				\sin(\theta)=\sin(\alpha-\beta)\\
				-\sin(\theta)=\sin(\beta-\alpha)\\
				\cos(\theta)=\cos(\alpha-\beta)
			\end{cases}\iff\theta\equiv\alpha-\beta[2\pi]$.\\
			Il y a donc existence mais pas unicité de l'écriture de $M$ comme produit de deux symétries.
			\item SO$_2(\Q)\subset \text{SO}_2(\R)$ qui est un groupe abélien donc les éléments de SO$_2(\Q)$ commutent tous entre eux. $I_2\in\O_2(\Q)$. De plus, $\det(\O_2(\Q))\subset \det(\O_2(\R))=\{-1,1\}$. Donc si $M=\begin{pmatrix}a&b\\c&d\end{pmatrix}$ et $M'=\begin{pmatrix}a'&b'\\c'&d'\end{pmatrix}$ sont dans $\O_2(\Q)$ alors en notant $\varepsilon=\displaystyle\frac{1}{\det(M(M')^{-1})}\in \{-1,1\}$,\\    $M(M')^{-1}=\begin{pmatrix}a&b\\c&d\end{pmatrix}\cdot\varepsilon\begin{pmatrix}d'&-b'\\-c'&d'\end{pmatrix}=\varepsilon\begin{pmatrix}ad'-bc'&bd'-ab'\\cd'-dc'&dd'-db'\end{pmatrix}\in \M_2(\Q)$ d'où $M(M')^{-1}\in \O_2(\Q)$. De plus, si $\det(M)=\det(M')=1$ alors $\det(M(M')^{-1})=1$ donc SO$_2(\Q)$ est un sous groupe de $\O_2(\Q)$.\\
			Il est évident que $D\subset \text{SO}_2(\Q)$. Supposons par l'absurde que $D=C=\text{SO}_2(\Q)$. Comme tous les éléments de $\O_2(\Q)\backslash \text{SO}_2(\Q)$ ont un carré égal au neutre, $C$ est engendré par les carrés des éléments de $\text{SO}_2(\Q)$.\\
			Ensuite, les éléments de $\O_2(\Q)$ de déterminant $-1$ sont des réflexions.\\
			Soit $M\in \text{SO}_2(\Q)$. D'après les calculs fait en question $3.a$, en prenant $\alpha=\theta$ et $\beta=0$ on a $M=AB$ avec $A=\begin{pmatrix}\cos(\theta)&\sin(\theta)\\\sin(\theta)&-\cos(\theta)
			\end{pmatrix}\in \O_2(\Q),\ B=\begin{pmatrix}1&0\\0&-1
			\end{pmatrix}\in \O_2(\Q),\ A^2=B^2=I_2$.\\
			Ainsi $\O_2(\Q)$ est bien engendré par les involutions d'où $D=C$ d'après la question $2$.\\
			Or comme SO$_2(\Q)$ est commutatif, un produit de carré de matrices de SO$_2(\Q)$ est encore un carré dans SO$_2(\Q)$.\\
			Et donc pour montrer que $D$ est distinct de SO$_2(\Q)$ il suffit de montrer qu'il existe une matrice de SO$_2(\Q)$ qui n'est pas un carré dans SO$_2(\Q)$. C'est le cas de $M=\begin{pmatrix}0&-1\\1&0\end{pmatrix}$. Supposons qu'il existe une matrice $N\in \O_2(\Q)$ telle que $M=N^2$. On a forcément $N\in \text{SO}_2(\Q)$ puisque sinon $N^2=I_n$. On peut donc écrire $N=\begin{pmatrix}\cos(\theta)&-\sin(\theta)\\\sin(\theta)&\cos(\theta)\end{pmatrix}$ avec $\theta\in \R$. Mais alors $2\theta\equiv \displaystyle\frac{\pi}{2}[2\pi]$ et donc $\cos(\theta)=\pm\displaystyle\frac{\sqrt{2}}{2}\notin \Q$ ce qui est absurde.
		\end{enumerate}
	\end{enumerate}
	
	\subsection{Sous-groupe discret de $\C$ et de SL$_2(\R)$ \xens{5}}
	\label{Sous-groupe discret de C et de SL2(R) corrigé}
	\textcolor{blue}{\hyperref[Sous-groupe discret de C et de SL2(R)]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item l'ensemble des entiers relatifs $\Z$ et l'ensemble des entiers de Gauss $\Z[i]=\{a+bi,\ (a,b)\in \Z^2\}$ sont des sous-groupes discret de $\C$. SL$_2(\Z)$ en est un de SL$_2(\R)$.\\\\
		\textit{Remarque : Plus précisément, $\Z[i]$ est un \underline{réseau} de $\C$. C'est à dire qu'en plus d'être un sous-groupe discret, il engendre $\C\simeq\R^2$ en tant qu'espace vectoriel réel. A contrario $\Z$ n'est pas un réseau de $\C$, mais est un réseau de $\R$. L'étude des réseaux à des applications dans de multiples branches des mathématiques comme la théorie des groupes la géométrie convexe ou la géométrie des nombres. De manière plus pragmatique les réseaux apparaissent dans des problèmes de pavage du plan ou en cristallographie}
		\item $\Gamma$ n'étant pas réduit à $\{0\}$ on peut considérer un élément $\gamma\in \Gamma\backslash\{0\}$ de module minimal. Par hypothèse $\exists \gamma'\in \Gamma,\ \gamma=\lambda\gamma'$. De plus $\gamma'\ne 0$ puisque $\C$ est intègre et $\gamma\ne 0$. On a $|\gamma|=|\lambda||\gamma'|\geq |\lambda||\gamma|$ donc $|\lambda|\leq 1$. Or si $|\lambda|<1$ alors $(\lambda^n\gamma)_{n\in \N}$ est une suite de $\Gamma$ qui converge vers $0\in \Gamma$. Dans ce cas $0$ n'est pas un point isolé de $\Gamma$ ce qui est absurde. On en déduit que $|\lambda|=1$. Notons $\lambda=e^{i\theta}$ avec $\theta\in \R$.\\
		On remarque que $\displaystyle\frac{1}{\lambda}\gamma=\gamma'\in \Gamma$. Donc $\lambda\gamma+\displaystyle\frac{1}{\lambda}\gamma=2\cos(\theta)\gamma=(e^{i\theta}+e^{-i\theta})\gamma=2\cos(\theta)\gamma\in \Gamma$.\\
		Comme $\Gamma$ est un sous-groupe additif, $\{2\cos(\theta)\}\gamma\in \Gamma$ en notant $\{2\cos(\theta)\}=2\cos(\theta)-\lfloor2\cos(\theta)\rfloor\in [0,1[$.\\
		Si $\{2\cos(\theta)\}\ne 0$ alors en passant au module $\{2\cos(\theta)\}\geq 1$ ce qui est absurde. Donc $\{2\cos(\theta)\}=0$ i.e $2\cos(\theta)\in \Z$. De plus $2\cos(\theta)\in [-2,2]$ donc $\cos(\theta)\in \displaystyle\left\{-1,-\frac{1}{2},0,\frac{1}{2},1\right\}$.\\
		Si $\cos(\theta)\in \{-1,0,1\}$ alors $\lambda^4=1$ et si $\cos(\theta)\in \displaystyle\left\{-\frac{1}{2},\frac{1}{2}\right\}$ alors $\lambda^6=1$.
		\item Notons $M=\begin{pmatrix}1&1\\0&1\end{pmatrix}$ et $D=\begin{pmatrix}\lambda&0\\0&\lambda^{-1}\end{pmatrix}$. On montre classiquement que $\forall k\in \Z,\ M^k=\begin{pmatrix}1&k\\0&1\end{pmatrix}$.\\
		Donc $\forall m,n\in \Z,\ M^nD^m=\begin{pmatrix}\lambda^m&n\lambda^{-m}\\0&\lambda^{-m}\end{pmatrix}$ et $D^mM^n=\begin{pmatrix}\lambda^m&n\lambda^m\\0&\lambda^{-m}\end{pmatrix}$.
	\end{enumerate} 
	\newpage
\section{Correction Anneaux et corps}
	\subsection{Centre d'un anneau \etoile{1}}
	\label{Centre d'un anneau corrigé}
	\textcolor{blue}{\hyperref[Centre d'un anneau]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $Z(A)\subset A$ vérifie :
		\begin{itemize}
			\item $0\in Z(A)$ car $\forall a\in A,\ 0\cdot a=0=a\cdot 0$
			\item $1\in Z(A)$ car $\forall a\in A,\ 1\cdot a=a=a\cdot 1$;
			\item $\forall (x,y)\in Z(A)^2,\ \forall a\in A,\ axy=xay=xya\implies xy\in A$;
			\item $\forall (x,y)\in Z(A)^2,\ \forall a\in A,\ (x-y)a=xa-ya=ax-ay=a(x-y)\implies x-y\in Z(A)$.
			\item $\forall (x,y)\in Z(A)^2,\ y\in A\implies xy=yx$.
		\end{itemize}
		Ainsi $Z(A)$ est un sous-anneau commutatif de $A$.
		\item Tout d'abord $(A,+,\times)$ est un anneau commutatif.\\
		Il reste à vérifier que $(A,+,\cdot)$ est un $Z(A)$-espace vectoriel. Si $(x,y)\in A^2$ et $(\lambda,\mu)\in Z(A)^2$ il est clair que :
		\begin{itemize}
			\item $1\cdot x=x$;
			\item $(\lambda+\mu)\cdot x=\lambda x+\mu x$;
			\item $\lambda\cdot(x+y)=\lambda\cdot x+\lambda \cdot y$;
			\item $\lambda\cdot(\mu\cdot x)=(\lambda\mu)\cdot x$.
		\end{itemize}
		Ainsi $(Z(A),+,\times,\cdot)$ est une $Z(A)$-algèbre unitaire, associative et commutative.
	\end{enumerate}
	
	\subsection{Calcul d'un inverse \etoile{3}}
	\label{Calcul d'un inverse corrigé}
	\textcolor{blue}{\hyperref[Calcul d'un inverse]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Il existe $n\in \N^*$ tel que $(ab)^n=0$.\\
		Alors $(ba)^{n+1}=(ba)(ba)\dots(ba)=b(ab)\dots(ab)a=b(ab)^na=0$. On s'inspire de la formule : $\forall x\in ]-1,1[,\ (1-x)^{-1}=\displaystyle\sum_{k=0}^{+\infty}x^k$.\\
		Notons $p$ l'indice de nilpotence de $ab$.\\
		$(1-ba)\displaystyle\sum_{k=0}^p(ba)^k=\sum_{k=0}^p(ba)^k-\sum_{k=0}^p(ba)^{k+1}=1+\sum_{k=1}^p(ba)^k-\sum_{k=1}^p(ba)^k=1+(ba)^{p+1}=1$. De même, $\displaystyle\sum_{k=0}^p(ba)^k(1-ba)=1$.\\
		Ainsi $1-ba$ est inversible et $(1-ba)^{-1}=\displaystyle\sum_{k=0}^p(ba)^k$.\\
		On vérifie de même que $(1-ab)^{-1}=\displaystyle\sum_{k=0}^{p-1}(ab)^k$ de sorte que $(1-ba)^{-1}=1+b\left(\displaystyle\sum_{k=0}^{p-1}(ab)^k\right)a=1+b(1-ab)^{-1}a$.
		\item Montrons que l'inverse de $(1-ba)$ est encore $1+b(1-ab)^{-1}a$.\\
		$(1-ba)(1+b(1-ab)^{-1}a)=1+b(1-ab)^{-1}a-ba-bab(1-ab)^{-1}a=1-ba+b(1-ab)(1-ab)^{-1}a=1-ba+ba=1$. De même, $(1+b(1-ab)^{-1}a)(1-ba)=1$.\\
		Ainsi $(1-ba)$ est inversible et $(1-ba)^{-1}=1+b(1-ab)^{-1}a$.
	\end{enumerate}
	
	
	\subsection{Anneau de Boole \etoile{2}}
	\label{Anneau de Boole corrigé}
	\textcolor{blue}{\hyperref[Anneau de Boole]{[Enoncé]}}\\
	On rappelle que si $A$ est une partie de $E$ alors la fonction indicatrice de $A$ est définie par $\fonction{\mathbbm 1_A}{E}{\{0,1\}}{x}{\begin{cases}1&\mbox{si }x\in A\\0&\mbox{si }x\notin A\end{cases}}$ et que $\forall (A,B)\in \mathcal P(E)^2,\ A=B\iff \mathbbm 1_A=\mathbbm 1_B$. On notera pour toute partie $A$ de $E$, $\overline A=E\backslash A$ le complémentaire de $A$ dans $E$ ainsi que $1=\mathbbm 1_E$.\\
	On rappelle enfin les règles de calcul :
	\begin{itemize}
		\item $\mathbbm 1_{\overline A}=1-\mathbbm 1_A$;
		\item $\mathbbm 1_{A\cap B}=\mathbbm 1_A\mathbbm 1_B$;
		\item $\mathbbm 1_{A\cup B}=\mathbbm 1_A+\mathbbm 1_B-\mathbbm 1_A\mathbbm 1_B$.
	\end{itemize}
	\begin{enumerate}
		\item Soit $(A,B,C)\in \mathcal P(E)^3$.\\
		$\mathbbm 1_{A\Delta B}=\mathbbm 1_{A\backslash B}+\mathbbm 1_{B\backslash A}$ car $(A\backslash B)\cap(B\backslash A)=\emptyset$.\\
		Donc $\mathbbm 1_{A\Delta B}=\mathbbm 1_{A\cap \overline B}+\mathbbm 1_{B\cap \overline A}=\mathbbm 1_A\mathbbm 1_{\overline B}+\mathbbm 1_B\mathbbm 1_{\overline A}=\mathbbm 1_A(1-\mathbbm 1_B)+\mathbbm 1_B(1-\mathbbm 1_A)=\mathbbm 1_A+\mathbbm 1_B-2\mathbbm 1_A\mathbbm 1_B$.\\
		On doit vérifier que $(\mathcal P(E),\Delta)$ est un groupe abélien:
		\begin{itemize}
			\item $\Delta$ est une loi de composition interne : il est clair que $A\Delta B\in \mathcal P(E)$;
			\item $\Delta$ est commutative : $A\Delta B=(A\backslash B)\cup (B\backslash A)=(B\backslash A)\cup (A\backslash B)=B\Delta A$;
			\item Existence du neutre : $\emptyset\Delta A=A=A\Delta\emptyset$;
			\item Tout élément est symétrisable (il n'y a même que des involution): $A\Delta A=\emptyset$;
			\item $\Delta$ est associative :
			\begin{align*}
				\mathbbm 1_{(A\Delta B)\Delta C}&=\mathbbm 1_{A\Delta B}+\mathbbm 1_C-2\mathbbm 1_{A\Delta B}\mathbbm 1_C\\
				&=\mathbbm 1_A+\mathbbm 1_B-2\mathbbm 1_A\mathbbm 1_B+\mathbbm 1_C-2(\mathbbm 1_A+\mathbbm 1_B-2\mathbbm 1_A\mathbbm 1_B)\mathbbm 1_C\\
				&=\mathbbm 1_A+\mathbbm 1_B+\mathbbm 1_C-2(\mathbbm 1_A\mathbbm 1_B+\mathbbm 1_A\mathbbm 1_C+\mathbbm 1_B\mathbbm 1_C)+4\mathbbm 1_A\mathbbm 1_B\mathbbm 1_C\\
			\end{align*}
			Et
			\begin{align*}
				\mathbbm 1_{A\Delta(B\Delta C)}&=\mathbbm 1_A+\mathbbm 1_{B\Delta C}-2\mathbbm 1_A\mathbbm 1_{B\Delta C}\\
				&=\mathbbm 1_A+\mathbbm 1_B+\mathbbm 1_C-2\mathbbm 1_B\mathbbm 1_C-2\mathbbm 1_A(\mathbbm 1_B+\mathbbm 1_C-2\mathbbm 1_B\mathbbm 1_C)\\
				&=\mathbbm 1_A+\mathbbm 1_B+\mathbbm 1_C-2(\mathbbm 1_A\mathbbm 1_B+\mathbbm 1_A\mathbbm 1_C+\mathbbm 1_B\mathbbm 1_C)+4\mathbbm 1_A\mathbbm 1_B\mathbbm 1_C
			\end{align*}
		\end{itemize}
		Ensuite il faut que la loi $\cap$ soit distributive sur la loi $\Delta$ :\\
		$\mathbbm 1_{A\cap(B\Delta C)}=\mathbbm 1_A\mathbbm 1_{B\Delta C}=\mathbbm 1_A(\mathbbm 1_B+\mathbbm 1_C-2\mathbbm 1_C\mathbbm 1_B)=\mathbbm 1_A\mathbbm 1_B+\mathbbm 1_A\mathbbm 1_C-2\mathbbm 1_A\mathbbm 1_B\mathbbm 1_C=\mathbbm 1_{A\cap B}+\mathbbm 1_{A\cap C}-2\mathbbm 1_{A\cap B}\mathbbm 1_{A\cap C}=\mathbbm 1_{(A\cap B)\Delta(A\cap C)}$.\\
		L'avant dernière égalité est obtenue en utilisant $\mathbbm 1_A^2=\mathbbm 1_{A\cap A}=\mathbbm 1_A$ et en écrivant $\mathbbm 1_A\mathbbm 1_B\mathbbm 1_C=\mathbbm 1_A^2\mathbbm 1_B\mathbbm 1_C=(\mathbbm 1_A\mathbbm 1_B)(\mathbbm 1_A\mathbbm 1_C)$.\\
		Puis la loi $\cap$ doit être associative et commutative :
		\begin{itemize}
			\item $A\cap B=B\cap A$;
			\item $(A\cap B)\cap C=A\cap(B\cap C)$.
		\end{itemize}
		Enfin la loi $\cap$ doit posséder un neutre (différent de celui de la loi $\Delta$):\\
		$A\cap E=A=E\cap A$ et $E\ne\emptyset$.\\\\
		Finalement $(\mathcal P(E),\Delta,\cap)$ est un anneau commutatif.\\
		\textit{Remarque : En prenant un peu de recul on peut faire une analogie entre la différence symétrique et les portes logiques dans l'algèbre de Boole. L'opération de différence symétrique représente la porte ''XOR'' ou le ''OU exclusif'' (A ou B, mais pas les deux). Plus classiquement l'opération d'intersection traduit le ''ET'' ou la porte ''AND''. C'est l'origine de l'appellation anneau de Boole pour cet anneau et cela permet de mieux saisir la structure de ces opérations ainsi que leurs interactions.}
		\item Soit $A\subset E$ inversible pour la loi $\cap$. On note $B$ l'inverse de $A$.\\
		On sait que $A\cap B=E$. Si $A\subsetneq E$ alors $A\cap B\subset A\implies A\cap B\ne E$. Donc $A=E$. Le neutre est toujours inversible donc $E$ est le seul élément de $(\mathcal P(E),\Delta,\cap)$ inversible pour la loi $\cap$.
		\item On cherche à montrer ou réfuter la proposition $"\forall (A,B)\in \mathcal P(E)^2,\ A\cap B=\emptyset\implies A=\emptyset\vee B=\emptyset"$.\\
		S'il existe une partie $A$ de $E$ non vide et non égale à $E$ en entier, autrement dit si $E$ n'est pas un singleton, alors $A\cap \overline A=\emptyset$ avec $A\ne\emptyset$ et $\overline A\neq\emptyset$.\\
		Dans le cas contraire on note $E=\{x\}$. On a alors $\mathcal P(E)=\{\emptyset,\{x\}\}$. Dans ces conditions il est clair que $\forall (A,B)\in \mathcal P(E)^2,\ A\cap B=\emptyset\implies A=\emptyset\vee B=\emptyset$.\\
		Par conséquent $(\mathcal P(E),\Delta,\cap)$ est un anneau intègre si et seulement si $E$ ne possède qu'un seul élément.
	\end{enumerate}
	
	\subsubsection{L'anneau de Boole est principal \etoile{3}}
	Soit $F\subset E$. $\emptyset\in \mathcal P(F)$.\\
	Si $(A,B)\in \mathcal P(F)^2$ alors $A\Delta B\subset A\cup B\subset F$, c'est à dire $A\Delta B\in \mathcal P(F)$. Trivialement le symétrique de $A$ reste dans $\mathcal P(F)$ puisque que c'est lui-même. Donc $\mathcal P(F)$ est un sous groupe de $(\mathcal P(E),\Delta)$. De plus si $C\subset E$ alors $A\cap C\subset A\subset F$ donc $A\cap C\in \mathcal P(F)$ : $\mathcal P(F)$ est absorbant.\\
	On en déduit que $\mathcal P(F)$ est un idéal de $\mathcal P(E)$.\\\\
	Donnons nous maintenant un idéal $I$ de $\mathcal P(E)$.\\
	Comme $E$ est fini, l'ensemble $M=\{\Card(A),\ A\in I\}$ est majoré par $\Card(E)$. C'est une partie majorée de $\N$, elle admet donc un maximum. On note $F$ tel que $\Card(F)=\max(M)$. Montrons que $I=\mathcal P(F)$.\\
	Pour cela on va montrer que $I$ est stable par réunion. Fixons $(A,B)\in I^2$. D'un point de vue logique, il s'agit de construire la porte ''OR'' à partir des portes ''AND'' et ''XOR''.\\
	Intuitivement $A\Delta(\overline A\cap B)$ à l'air de fonctionner.\\
	On vérifie :\\
	$\mathbbm 1_{A\Delta(\overline A\cap B)}=\mathbbm 1_A+\mathbbm 1_{\overline A\cap B}-\mathbbm 1_A\mathbbm 1_{\overline A\cap B}=\mathbbm 1_A+\mathbbm 1_{\overline A}\mathbbm 1_B-\mathbbm 1_A\mathbbm 1_{\overline A}\mathbbm 1_B=\mathbbm 1_A+(1-\mathbbm 1_A)\mathbbm 1_B-\mathbbm 1_{A\cap \overline A}\mathbbm 1_B=\mathbbm 1_1+\mathbbm 1_B-\mathbbm 1_A\mathbbm 1_B=\mathbbm 1_{A\cup B}$.\\
	$\overline A\cap B\in I$ par absorbance et $A\in I$ donc $A\cup B=A\Delta(\overline A\cap B)\in I$.\\
	Ainsi si $F'\in I$ alors $F\cup F'\in I$. Si $F'$ n'est pas inclus dans $F$ alors $F'\cup F$ a un cardinal strictement plus grand que celui de $F$ ce qui contredit la maximalité de $\Card(F)$ dans $M$. Donc $F'\in \mathcal P(F)$.\\
	Réciproquement si $F'\subset F$ alors $F'=F'\cap F\in I$ par absorbance.\\
	Finalement $I=\mathcal P(F)$.\\\\
	\textit{Remarque : on a montré que l'anneau $(\mathcal P(E),\Delta,\cap)$ est principal (cf. \ref{Anneau principal (1)}).}
	
	\subsection{Condition suffisante pour qu'un anneau soit commutatif \etoile{3}}
	\label{Condition suffisante pour qu'un anneau soit commutatif corrigé}
	\textcolor{blue}{\hyperref[Condition suffisante pour qu'un anneau soit commutatif]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $(x,y)\in A^2$.\\
		$x+y=(x+y)^2=x^2+xy+yx+y^2=x+xy+yx+y$ donc $xy=-yx=(-yx)^2=(yx)^2=yx$.
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $x\in A$ nilpotent. Par récurrence double immédiate $\forall n\in \N^*,\ x^n\in \{x,x^2\}$. Donc $x=0$ ou $x^2=0$. Dans tous les cas $x=x^3=0$.\\
			Le seul élément nilpotent de $A$ est $0$.
			\item $b^2=ea(1-e)\cdot ea(1-e)=(ea-eae)ea(1-e)=(eae-eae)a(1-e)=0$.\\
			On en déduit que $b$ est nul c'est à dire $ea=eae$.\\
			En considérant de même $c=(1-e)ae$ on obtient $ae=eae$. Il en résulte que $a$ et $e$ commute. Ainsi $e\in Z(A)$.\\
			Soit $x\in A$. $(x^2)^2=x^4=x^2$ donc d'après ce qui précède, $x^2\in Z(A)$.
			\item On montre que $Z(A)$ est un sous-anneau (cf. \ref{Centre d'un anneau}) de $A$. Fixons $x\in A$.\\
			Alors $2x=(x+1)^2-x^2-1\in Z(A)$ et $3x=(x+1)^3-x^3-3x^2-1=x+1-x-3x^2-1=-3x^2\in Z(A)$.\\
			Ainsi $x=3x-2x\in Z(A)$. On en déduit que $Z(A)=A$ c'est à dire que $A$ est commutatif.
		\end{enumerate}
	\end{enumerate}
	
	\subsection{Anneaux commutatifs ou anti-commutatifs \etoile{4}}
	\label{Anneaux commutatifs ou anti-commutatifs corrigé}
	\textcolor{blue}{\hyperref[Anneaux commutatifs ou anti-commutatifs]{[Enoncé]}}\\
	On considère $Z=\{x\in A,\ \forall a\in ax=xa\}$ et $Y=\{x\in A,\ \forall a\in A,\ ax=-xa\}$. On montre aisément que $Z$ et $Y$ sont des sous-groupes de $(A,+)$.\\
	Montrons que $A=Z\cup Y$. Il est évident que $Z\cup Y\subset A$. Supposons par l'absurde qu'il existe $x\in A\backslash(Z\cup Y)$.\\
	Comme $x\notin Z$, il existe $a\in A$ tel que $xa\ne ax$. Donc $xa=-ax$.\\
	Comme $x\notin Y$, il existe $b\in A$ tel que $xb\ne -bx$. Donc $xb=bx$.\\
	Alors $(b+a)x=x(b-a)$. Or par hypothèse, $(b+a)x\in \{x(b+a),-x(b+a)\}$.\\
	Si $(b+a)x=x(b+a)$ alors $xb-xa=xb+xa$ d'où $2xa=0$ c'est à dire $xa=-xa=ax$ ce qui est contraire à la définition de $a$.\\
	De même, si $(a+b)x=-x(a+b)$ alors $bx=-xb$ ce qui est contraire à la définition de $b$.\\
	On a montré que $A=Z\cup Y$. Or il est classique de montrer que l'union de deux sous-groupe est un sous-groupe si et seulement si l'un des deux contient l'autre (cf \ref{Opérations sur les sous-groupes}). Comme $Z\cap Y=\emptyset$, cela impose $Y=\emptyset$ ou $Z=\emptyset$ autrement dit $A$ est commutatif ou anti-commutatif.\\\\
	Supposons maintenant que $(A,+,\times)$ est un anneau. Supposons que $A$ est anti-commutatif.\\
	Alors $\forall x\in A,\ x=1_Ax=-x1_A=-x$. Ainsi $\forall (x,y)\in A^2,\ xy=-yx=yx$.\\
	Dans tous les cas $A$ est commutatif.
	
	\subsection{Anneau régulier \etoile{4}}
	\label{Anneau régulier corrigé}
	\textcolor{blue}{\hyperref[Anneau régulier]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $(\Z,+,\times)$ n'est pas régulier, par exemple il n'existe pas d'entier $u$ tel que $2\times u\times 2=2$.
		\item Soit $(K,+,\times)$ un corps.
		\begin{itemize}
			\item $0_K\times0_K\times0_K=0_K$;
			\item Si $a\in K\backslash\{0_K\}$ alors $a$ est inversible et $a^{-1}\in K$ et $aa^{-1}a=a$.
		\end{itemize}
		Un corps est régulier.
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $(a,b)\in A\times B$. On sait qu'il existe $(u,v)\in A\times B$ tel que $aua=a$ et $bvb=b$.\\
			Donc $(a,b)(u,v)(a,b)=(aua,bvb)=(a,b)$ d'où $A\times B$ est régulier.
			\item Excluons les cas triviaux $n=0$ et $n=1$ où $\Z/0\Z$ est isomorphe à $\Z$ et est donc régulier et $\Z/1\Z=\{\overline 0\}$ est clairement régulier. Fixons $n\geq 2$.\\
			Si $p$ est un nombre premier, $\Z/p\Z$ est un corps et est donc régulier d'après la question $2$. Si $n$ est \textit{quadratfrei}, c'est à dire qu'il s'écrit comme un produit de nombres premiers distinct $n=p_1\dots p_k$ alors d'après le théorème des restes chinois $\Z/n\Z$ est isomorphe à $\Z/p_1\Z\times\dots\times\Z/p_k\Z$. Ce dernier anneau est régulier par récurrence sur la question $3.a$ donc $\Z/n\Z$ est régulier.\\
			Supposons maintenant qu'il existe un nombre premier $p$ tel que $p^2|n$. Supposons par l'absurde que $\Z/n\Z$ est régulier.\\
			Alors $\exists u\in \Z,\ \overline p\times\overline u\times\overline p=\overline{up^2}=\overline p$. C'est à dire $\exists (u,v)\in \Z^2,\ p=up^2+vn$.\\
			Or $p^2|up^2+vn$ donc $p^2|p$ ce qui est absurde.
			Finalement, $\Z/n\Z$ est régulier si et seulement si $n=0,\ n=1$ ou $n\geq 2$ et est quadratfrei.
		\end{enumerate}
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $u\in \mathcal L(E)$. On pose $S$ un supplémentaire de $\Ker(u)$.\\
			On sait d'après le théorème du rang que $\tilde u=u_{|S}^{|\text{Im}(u)}$ est un isomorphisme.\\
			On définit alors $v$ sur $E=S\oplus\Ker(u)$ par $v(x)=
			\begin{cases}
				0&\mbox{si }x\in \Ker(u)\\
				\tilde u^{-1}(x)&\mbox{si }x\in S
			\end{cases}$.\\
			$v$ est bien un endomorphisme de $E$ et,
			\begin{itemize}
				\item $\forall x\in \Ker(u),\ u\circ v\circ u(x)=0=u(x)$;
				\item $\forall x\in S,\ u\circ v\circ u(x)=u\circ \tilde u^{-1}(\tilde u(x))=u(x)$.
			\end{itemize}
			Donc $u\circ v\circ u=u$.
			\item Notons $\varphi$ l'endomorphisme associé à $A$ dans la base canonique notée $(e_1,\dots,e_n)$.\\
			On a $\Ker(\varphi)=\Ker(A)=\Vect(e_1)$ et $\text{Im}(\varphi)=\text{Im}(A)=\Vect(e_1,\dots,e_{n-1})$. Donc en posant $S=\Vect(e_2,\dots,e_n)$, on a $\Ker(A)\oplus S=\M_{n,1}(\K)$.\\
			On pose $\tilde\varphi=\varphi_{|S}^{|\text{Im}(\varphi)}$. On sait que $\tilde\varphi$ est un isomorphisme, déterminons son inverse.\\
			$\forall i\in \crblanc{2}{n},\ \tilde\varphi(e_i)=e_{i-1}$. Ainsi $\tilde\varphi^{-1}$ est définie par $\forall i\in \crblanc{1}{n-1},\ \tilde\varphi^{-1}(e_i)=e_{i+1}$.\\
			On définit enfin $u$ sur $E=S\oplus\Ker(\varphi)$ par $u(e_n)=0$ et $\forall i\in \crblanc{1}{n-1},\ u(e_i)=e_{i+1}$ et on a $\varphi\circ u\circ\varphi=\varphi$.\\
			Donc en notant $U=\operatorname{Mat}_{(e_1,\dots,e_n)}(u)=
			\left(\begin{array}{cccc|c}
				0&\cdots&\cdots&0&0\\
				\hline
				1&0&\cdots&0&0\\
				0&\ddots&\ddots&\vdots&\vdots\\
				\vdots&\ddots&\ddots&0&\vdots\\
				0&\cdots&0&1&0
			\end{array}\right)$ on a $AUA=A$.
		\end{enumerate}
		\item Soit $A$ un anneau régulier. Notons $Z$ le centre de $A$ et fixons $a\in Z$. Comme $a\in A$, il existe $u\in A$ tel que $aua=a$. On cherche $v\in Z$ tel que $ava=a$. On va montrer que $v=uau$ convient.\\
		Tout d'abord, $ava=a(uau)a=(aua)ua=aua=a$. Fixons ensuite $b\in A$. Comme $a\in Z$,\\
		$(1-au)bau=(a-aua)bu=0$ et $aub(1-au)=ub(a-aua)=0$.\\
		Donc $bau-aubau=0=aub-aubau$ c'est à dire $bau=aub$.\\
		Ainsi $vb=uaub=ubau=aubu=bau^2=bv$.
	\end{enumerate}
	
	\subsection{Anneau intègre fini \etoile{1}}
	\label{Anneau intègre fini corrigé}
	\textcolor{blue}{\hyperref[Anneau intègre fini]{[Enoncé]}}\\
	Il s'agit de montrer que tout élément non nul de $A$ est inversible. Fixons $a\in A\backslash\{0_A\}$.\\
	On sait que $\forall n\in \N^*,\ a^n\in A$. Comme $A$ est fini, on en déduit qu'il existe deux entiers naturels non nuls $p>q$ tels que $a^q=a^p$.\\
	Mais alors $a^q-a^p=0$ c'est à dire $a^p(a^{q-p}-1)=0$ c'est à dire $a^p=0$ ou $a^{q-p}=1$ par intégrité. Encore par intégrité, $a^p=0\implies a=0$.\\
	Donc $a^{p-q}=1$. On en déduit que $a$ est inversible d'inverse $a^{p-q-1}$ qui est bien défini puisque $p-q-1\geq 0$.
	
	\subsection{Anneau principal (1)\etoile{3}}
	\label{Anneau principal (1) corrigé}
	\textcolor{blue}{\hyperref[Anneau principal (1)]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item L'idéal nul $\{0\}=0\Q$ est évidemment principal. Soit $I$ un idéal non nul de $\Q$.\\
		Il existe $a\in I$ non nul. Alors $a\times\displaystyle\frac{1}{a}=1\in I$ par absorbance. Mais alors quel que soit $b\in \Q$, $b=1\times b\in I$ par absorbance. Donc $I=\Q=1\Q$.\\
		Ainsi $(\Q,+,\times)$ est principal.\\
		\textit{\underline{Remarque} : On peut en fait montrer de manière plus général que les seuls idéaux d'un corps sont l'idéal nul et le corps tout entier (cf. \ref{Idéaux d'un corps}) et donc en particulier qu'un corps est toujours un anneau principal}
		\item Un idéal de $(\Z,+,\times)$ est avant tout un sous-groupe de $(\Z,+)$. Or on montre classiquement que les sous-groupes de $\Z$ sont les $a\Z$ pour $a\in \Z$ (cf. \ref{Sous-groupes de (Z,+)}). On en déduit que $(\Z,+,\times)$ est principal.
		\item Soit $I$ un idéal non nul de $\D$. On se donne $x\in I\backslash\{0\}$. Quitte à considérer $-x$ on peut supposer $x>0$. On écrit $x=\displaystyle\frac{a}{10^n}$ avec $a\in \N^*$ et $n\in \N$.\\
		Par absorbance, $a=x\times 10^n\in I$ donc $a\in I\cap\N^*$. Posons $\alpha=\min(I\cap \N^*)$. Le minimum existe car il s'agit d'une partie non vide de $\N$. $\alpha\in I\implies \alpha\D\subset I$.\\
		Réciproquement fixons $y=\displaystyle\frac{b}{10^m}\in I$. $b=y\times 10^m\in I\cap \Z$. Notons $b=\alpha q+r$ la division euclidienne de $b$ par $\alpha$. Comme $b\in I$ et $\alpha\in I$, $r=b-\alpha q\in I$.\\
		Or si $r>0$ alors $r\in I\cap\N^*$ et $r<\alpha$ ce qui absurde. Donc $r=0$ et $b\in \alpha\Z$.\\
		On en déduit que $y=\displaystyle\frac{b}{10^m}\in \alpha\D$. Ainsi $I=\alpha\D$ et $\D$ est principal.\\\\
		Plus précisément on sait que le numérateur dans l'écriture d'un nombre décimal $x=\displaystyle\frac{a}{10^n}$ peut toujours être pris entre $0$ et $9$. Notons $C=\{a\in \crblanc{0}{9},\ a\in I\}$
		\begin{itemize}
			\item Si $a\in C\cap\{1,2,4,5,8\}$ alors $\displaystyle\frac{1}{a}\in \D$ d'où $1\in I$ puis $I=\D$ par absorbance.
			\item Si $6\in C$ alors $3=6\times\displaystyle\frac{1}{2}\in I$ donc $6\D=3\D$.
		\end{itemize}
		Finalement les seuls idéaux de $\D$ sont $\{0\},\ 3\D,\ 7\D,\ 9\D$ et $\D$.
		\item Comme $\forall (d,k)\in \Z^2,\ d\overline k=\overline d\times\overline k$, les idéaux de $\Z/n\Z$ sont les sous-groupes de $\Z/n\Z$. Or $\Z/n\Z$ est cyclique en tant que groupe additif (il est engendré par $\overline 1$). On montre alors (cf. \ref{Sous-groupe d'un groupe cyclique}) que ses sous-groupes sont cycliques. Autrement dit ses idéaux sont principaux et $\Z/n\Z$ est un anneau principal.
	\end{enumerate}
	
	\subsection{Anneau principal (2) \etoile{3}}
	\label{Anneau principal (2) corrigé}
	\textcolor{blue}{\hyperref[Anneau principal (2)]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item D'après le cours les sous-groupes de $(\Z,+)$ sont les $a\Z$ pour $a\in \Z$. Un idéal de $\Z$ étant avant tout un sous-groupe additif de $\Z$, $(\Z,+,\times)$ est principal.
		\item Considérons $I=2\Z[X]+X\Z[X]$. Supposons que $I=P\Z[X]$ pour un certain $P\in \Z[X]$.\\
		Alors $P|2\in I$ d'où $P\in \{-2,-1,1,2\}$. Or $2$ ne divise pas $X$ dans $\Z[X]$ donc $P=\pm1$.\\
		On peut alors trouver $U,V\in \Z[X]$ vérifiant $1=2U+XV$. En évaluant en $0$ on obtient $1=2U(0)$ ce qui est absurde car $U(0)\in \Z$.\\
		Ainsi $\Z[X]$ n'est pas principal.
		\item D'après les questions précédentes la condition "$A$ est principal" n'est pas suffisante pour assurer la principalité de $A[X]$. On sait par contre d'après le cours que $\K[X]$ est principal pour $\K=\R$ ou $\C$. On conjecture alors que $A$ doit être un corps. On va s'inspirer de la démonstration faîte pour $\C[X]$ et $\R[X]$.\\
		Supposons que $A$ est un corps et donnons nous $I$ un idéal de $A[X]$. Si $I$ est nul alors il est principal, on suppose donc dans la suite que ce n'est pas le cas. L'ensemble des degrés des polynômes non nuls de $I$ est une partie non vide de $\N$, elle admet donc un minimum. On pose $\Pi$ qui réalise ce minimum. Montrons que $I=\Pi A[X]$.\\
		On a déjà $\Pi A[X]\subset I$ par absorbance. Pour montrer l'inclusion réciproque dans le cas $\K=\R$ ou $\C$ on réalise des divisions euclidiennes par $\Pi$. Montrons un résultat similaire dans $A[X]$ :\\
		$$\forall P\in A[X],\ \exists (Q,R)\in A[X],\ (P=Q\Pi+R\land\deg(R)<\deg(\Pi))$$\\
		Fixons $P=\displaystyle\sum_{k=0}^{d_0}a_kX^k\in I\backslash\{0\}$ avec $d_0:=\deg(P)$.\\
		$\Pi=\displaystyle\sum_{k=0}^pb_kX^k$ est non nul donc $p:=\deg(\Pi)\in \N$. Si $p>d_0$ alors $P=0\times\Pi+P$ fait l'affaire. Sinon, on considère $P_1(X)=P(X)-a_{d_0}b_p^{-1}X^{d_0-p}\Pi(X)\in A[X]$ car $A$ est un corps. Par construction $d_1:=\deg(P_1)<d_0$. Si $d_1<p$ alors $P=a_{d_0}b_p^{-1}X^{d_0-p}\Pi+P_1$ fait l'affaire. Sinon on réitère le processus en remplaçant $P$ par $P_1$ etc...\\
		Finalement on peut trouver des polynômes $Q$ et $R$ comme voulus.\\
		Maintenant, $R$ ne peut qu'être nul puisque sinon il contredirait la minimalité du degré de $\Pi$ dans $I\backslash\{0\}$. Par conséquent $R=0$ et $P=Q\Pi\in \Pi A[X]$. Ainsi $I=\Pi A[X]$ est principal et par suite, $A[X]$ est principal.\\\\
		Pour montrer le sens réciproque on va raisonner par contraposition. Supposons que $A$ n'est pas un corps. On se donne un élément $a$ non nul et non inversible de $A$. On considère comme dans la question précédente $I=aA[X]+XA[X]$.\\
		Si $A[X]$ est principal alors il existe $P\in A[X]$ tel que $I=PA[X]$. Dans ce cas $P|a\in I$ donc $P$ est constant. De plus $P|X\in I$ donc $\exists Q\in A[X],\ PQ=X$. $Q$ est forcément un monôme de degré $1$ et donc en identifiant les coefficients on a $P|1$ d'où $1\in I$.\\
		On en déduit qu'on peut trouver deux polynômes $U,V\in A[X]$ vérifiant $1=aU+VX$. En évaluant en $0$ on obtient que $a$ est inversible d'inverse $U(0)\in A$ ce qui est absurde.\\\\
		Finalement, $A[X]$ est principal si et seulement si $A$ est un corps.
	\end{enumerate}
	
	\subsection{Anneau euclidien \etoile{2}}
	\textcolor{blue}{\hyperref[Anneau euclidien]{[Enoncé]}}\\
	\label{Anneau euclidien corrigé}
	\begin{enumerate}[leftmargin=*]
		\item $\Z$ est un anneau euclidien en considérant pour $(a,b)\in \Z\times\Z\backslash\{0\}$ la division euclidienne de $a$ par $|b|$ et $\varphi$ l'application $x\in \Z\mapsto |x|$.\\
		$\K[X]$ est euclidien en considérant la division euclidienne de deux polynômes et $\varphi$ l'application $P\in \K[X]\mapsto \deg(P)$.
		\item Soit $A$ un anneau euclidien. Soit $I$ un idéal de $A$. Si $I=\{0_A\}$ alors $I=0_AA$. Si $I\ne \{0\}$ alors on se donne un élément $x\in I$ non nul. Considérons $E=\{\varphi(a),a\in I\backslash\{0_A\}\}$. $E$ admet un minimum en tant que partie non vide de $\N$. On note $b\in A$ qui réalise ce minimum. $bA\subset I$ par absorbance. Fixons $a\in A$.\\
		On sait qu'il existe un couple $(q,r)\in A^2$ tel que $a=bq+r$ et ($r=0_A$ ou $\varphi(r)<\varphi(b)$).\\
		Supposons que $r\ne 0_A$. Alors comme $b\in I,\ bq\in I$ puis comme $I$ est un sous-groupe additif de $A,\ r=a-bq\in I$. Cependant $\varphi(r)<\varphi(b)$ ce qui contredit la minimalité de $\varphi(b)$. On en déduit que $r=0_A$ et que $a=bq\in bA$.\\
		Ainsi $I=bA$ et $A$ est principal.
	\end{enumerate}
	
	\subsection{Entiers de Gauss \etoile{5}}
	\label{Entiers de Gauss corrigé}
	\textcolor{blue}{\hyperref[Entiers de Gauss]{[Enoncé]}}\\
	\begin{enumerate}
		\item Tout d'abord $\Z[i]\subset \C$ qui est un anneau commutatif intègre.\\
		Ensuite $0=0+0i\in \Z[i]$ et $1=1+0i\in \Z[i]$. Fixons $(x,y)\in \Z[i]$. Il existe $a,b,c,d\in \Z$ tel que $x=a+ib$ et $y=c+id$.\\
		D'une part $x-y=a-c+i(b-d)\in \Z[i]$ car $(a-c,b-d)\in \Z^2$, d'autre part $xy=ac-bd+i(ad+bc)\in \Z[i]$ car $(ac-bd,ad+bc)\in \Z^2$.\\
		Ainsi $\Z[i]$ est un anneau commutatif intègre en tant que sous anneau d'un anneau commutatif intègre.
		\item Soit $x=a+ib\in \Z[i]$ inversible. Alors $x$ est inversible dans $\C$ d'où $x^{-1}=\displaystyle\frac{1}{a+ib}=\frac{a-ib}{a^2+b^2}$ et de plus $x^{-1}\in \Z[i]$ donc $\displaystyle\frac{a}{a^2+b^2}\in \Z$ et $\displaystyle\frac{b}{a^2+b^2}\in \Z$.\\
		Or si $b\ne 0$ alors $a^2+b^2>a^2\geq |a|$ d'où $\displaystyle\frac{a}{a^2+b^2}\in\Z\implies a=0$. On a alors $\displaystyle\frac{1}{b}\in \Z$ d'où $b=\pm1$.\\
		De même si $a\ne 0$ alors $b=0$ et $a=\pm 1$.\\
		De plus $1,-1,i,-i$ sont inversible dans $\Z[i]$ (d'inverse respectif $1,-1,-i,i$) donc $\Z[i]^\times=\{1,-1,i,-i\}$.
		\item Soit $(x,y)\in \Z[i]\times\Z[i]\backslash\{0\}$. Notons $\displaystyle\frac{x}{y}=u+iv$ avec $u,v\in \R$. On pose $q=a+ib$ où $a$ est l'entier le plus proche de $u$ et $b$ l'entier le plus proche de $v$. $q$ est donc dans $\Z[i]$ et de plus $|u-a|\leq \displaystyle\frac{1}{2}$ et $|v-b|\leq \displaystyle\frac{1}{2}$.\\
		Par conséquent $\displaystyle N\left(\frac{x}{y}-q\right)=N(u-a+i(v-b))=(u-a)^2+(v-b)^2\leq \displaystyle\left(\frac{1}{2}\right)^2+\left(\frac{1}{2}\right)^2=\frac{1}{2}<1$ ou encore $N(x-qy)<N(y)$.\\
		Ainsi on a $x=qy+r$ avec $q\in \Z[i], r=x-qy\in \Z[i]$ et $N(r)<N(y)$ où $N:x=a+ib\in \Z[i]\mapsto |x|^2=a^2+b^2\in \N$ ce qui montre que $\Z[i]$ est euclidien.\\
		On montre alors (cf. \ref{Anneau euclidien}) qu'il est principal.
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $a\in \Z[i]$ tel que $N(a)$ est un nombre premier. Fixons $(b,c)\in \Z[i]^2$ tel que $a=bc$.\\
			Alors $N(a)=N(b)N(c)$. Or $N(a)$ étant premier, on sait que $(N(b),N(c))\in\{(1,N(a)),(N(a),1)\}$. On a montré dans la question $1$ que $x\in \Z[i]^\times\iff N(x)=1$. Ainsi on a bien $b\in \Z[i]^\times$ ou $c\in \Z[i]^\times$ d'où $a$ est irréductible dans $\Z[i]$.
			\item (i)$\implies$(ii) :\\
			Raisonnons pas l'absurde, si $p\not \equiv 3[4]$ alors $\exists x\in \Z,\ x^2\equiv -1[p]$. Donc $p$ divise $x^2+1=(x-i)(x+i)$ dans $\Z$ et donc dans $\Z[i]$. $p$ est irréductible dans $\Z[i]$ donc $p$ divise $x-i$ ou $p$ divise $x+i$ dans $\Z[i]$. Or ceci est impossible puisque $\displaystyle\frac{x}{p}-\frac{i}{p}$ et $\displaystyle\frac{x}{p}+\frac{i}{p}$ ne sont pas des éléments de $\Z[i]$.\\
			(ii)$\implies$(iii) :\\
			Encore une fois par l'absurde, supposons qu'il existe $x=a+ib\in \Z[i]$ tel que $p=N(x)=a^2+b^2$. On remarque qu'un carré est toujours congrus à $0$ ou $1$ modulo $4$ (il suffit de faire tous les cas). Donc $p$ ne peut être congrus qu'à $0$, $1$ ou $2$ modulo $4$ ce qui est absurde.\\
			(iii)$\implies$(i) :\\
			Toujours par l'absurde, soit $(a,b)\in \Z[i]^2$ tel que $p=ab$. On a $p^2=N(p)=N(ab)=N(a)N(b)$. Si $a$ et $b$ ne sont pas inversibles alors $N(a)$ et $N(b)$ sont tous deux différents de $1$. On en déduit comme $p$ est premier que $N(a)=N(b)=p$. Mais alors $p^2=N(a)^2$ d'où $p=N(a)$ ce qui est absurde.
			\item D'après les questions précédentes on sait déjà que les entiers premiers congrus à $3$ modulo $4$ et les éléments $a$ de $\Z[i]$ tels que $N(a)$ soit premier sont irréductibles dans $\Z[i]$. Montrons que ce sont les seuls.\\
			Soit $x$ irréductible dans $\Z[i]$. On remarque que $x$ divise $N(x)>1$ dans $\Z[i]$. $N(x)$ s'écrit comme un produit de puissance de nombre premier. Puisque $x$ est irréductible, il divise un de ces nombres premiers $p$ (dans $\Z[i]$). Autrement dit $\exists y\in \Z[i],\ p=xy$. Et donc $p^2=N(x)N(y)$. Puisque $N(x)\ne 1$ on sait que $N(x)=p$ ou $N(x)=p^2$. Si $N(x)=p^2$ alors $N(y)=1$ c'est à dire $y$ est inversible dans $\Z[i]$ c'est à dire $p$ est irréductible dans $\Z[i]$ (il est associé à $x$) et donc $p$ est congrus à $3$ modulo $4$ d'après la question précédente. Et si $N(x)=p$ alors $N(y)=p$ et $p^2=N(x)^2$ d'où $N(x)=p$ est premier.
		\end{enumerate}
		\item On montre classiquement que la relation $\sim$ définie par : $\forall (x,y)\in \Z[i]^2,\ x\sim y\iff \exists e\in \Z[i]^\times,\ x=ey$ est une relation d'équivalence sur $\Z[i]$. D'après la question $1$, si $x\in \Z[i]$ alors sa classe d'équivalence est $C_x=\{x,-x,ix,-ix\}$. On remarque que si $x\in \Z[i]$ alors $\forall y\in C_x,\ N(y)=N(x)$. On va donc partitionner la somme suivant la décomposition $\{x\in \Z[i],\ N(x)=n\}=\displaystyle\bigsqcup_{i=1}^pC_{a_i}$ où les $C_{a_i}$ sont les classes d'équivalence de la relation $\sim$ :\\
		$S_{n,k}=\displaystyle\frac{1}{4}\sum_{\substack{x\in \Z[i]\\N(x)=n}}x^k=\frac{1}{4}\sum_{i=1}^pa_i^k\left(1^k+(-1)^k+i^k+(-i)^k\right)$.\\
		Or la valeur de la somme $s_k=1^k+(-1)^k+i^k+(-i)^k$ est déterminée par la classe de $k$ dans $\Z/4\Z$ :
		\begin{itemize}
			\item Si $k\equiv0[4]$ alors $s_k=4$;
			\item Si $k\equiv1[4]$ alors $s_k=0$;
			\item Si $k\equiv2[4]$ alors $s_k=0$;
			\item Si $k\equiv3[4]$ alors $s_k=0$.
		\end{itemize}
		Finalement, $S_{n,k}=
		\begin{cases}
			\displaystyle\sum_{i=1}^pa_i^k&\mbox{si }4|k\\
			0&\mbox{sinon}
		\end{cases}$.\\
		Dans tous les cas, $S_{n,k}\in \Z[i]$.\\
		De plus, $\overline S_{n,k}=\displaystyle\frac{1}{4}\sum_{\substack{x\in \Z[i]\\N(x)=n}}\overline x^k=\frac{1}{4}\sum_{\substack{\overline x\in \Z[i]\\N(\overline x)=n}}x^k=\frac{1}{4}\sum_{\substack{x\in \Z[i]\\N(x)=n}}x^k=S_{n,k}$.\\
		On en déduit que la partie imaginaire de $S_{n,k}$ est nulle et donc que $S_{n,k}\in \Z$.
	\end{enumerate}
	
	\subsection{Anneau Noethérien \etoile{4}}
	\label{Anneau Noethérien corrigé}
	\textcolor{blue}{\hyperref[Anneau Noethérien]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $(A,+,\times)$ un anneau commutatif.\\
		(i)$\implies$(ii) :\\
		On suppose $A$ noethérien et on se donne $(I_n)_{n\in \N}$ une suite d'idéaux de $A$ croissante pour l'inclusion, c'est à dire que $\forall n\in \N,\ I_n\subset I_{n+1}$.\\
		Notons $N=\displaystyle\bigcup_{n\in \N}I_n$. Montrons que $N$ est un idéal de $A$ :
		\begin{itemize}
			\item $0_A\in I_0\implies 0_A\in N$;
			\item Si $(x,y)\in N^2$ alors il existe $n,m\in \N$ tels que $x\in I_n$ et $y\in I_m$. Alors $(x,y)\in I_{\max(m,n)}^2$ d'où $x+y\in I_{\max(m,n)}$ puis $x+y\in N$;
			\item Si $a\in A$ et $x\in N$ alors il existe $n\in \N$ tel que $x\in I_n$. Alors $ax\in I_n$ par absorbance puis $ax\in N$.
		\end{itemize}
		On en déduit, comme $A$ est noethérien, que $N$ est engendré par une famille $(a_i)_{i\in \crblanc{1}{p}}$ de $A$ finie.\\
		Chacun des éléments de la famille appartient à $N$, il existe donc pour tout $i\in \crblanc{1}{p}$ un entier naturel $n_i$ pour lequel $a_i\in I_{n_i}$. Ainsi $(a_i)_{i\in \crblanc{1}{p}}$ est une famille de $I_m$ pour $m=\max(n_1,\dots,n_p)$. On en déduit que la partie engendré par cette famille, qui est $N$, est contenue dans $I_m$. Ainsi $N=I_m$ et donc $(I_n)_{n\in \N}$ stationne à $I_m$.\\\\
		(ii)$\implies$(iii) :\\
		On raisonne par contraposé. Supposons qu'il existe un ensemble non vide $E$ d'idéaux de $A$ qui n'admet pas d'élément maximal. Soit $I_0\in E$. On suppose qu'il existe pour un certain $p\in \N$ une famille $(I_n)_{n\in \crblanc{1}{p}}$ de $E$ vérifiant $\forall n\in \crblanc{0}{p-1},\ I_n\subsetneq I_{n+1}$. Alors comme $I_p$ n'est pas un élément maximal de $N$, il existe $I_{p+1}\in E$ tel que $I_p\subsetneq I_{p+1}$. Par récurrence on a construit une suite d'idéaux de $A$ strictement croissante, elle ne peut donc pas stationner. On en déduit que $E$ admet un élément maximal pour l'inclusion.\\\\
		(iii)$\implies$(i) :\\
		Considérons $E$ l'ensemble des idéaux de $A$ qui sont de type fini.\\
		$E$ n'est pas vide car l'idéal nul appartient à $E$ par exemple. On sait donc que $E$ possède un élément $I$ maximal pour l'inclusion. Montrons que $A=I$.\\
		On sait que $I$ est de type fini donc il existe une famille fini $(a_i)_{i\in \crblanc{1}{q}}$ qui engendre $I$. Supposons qu'il existe un élément $a\in A\backslash I$. Alors en posant $b_{q+1}=a$ et $\forall i\in \crblanc{1}{q},\ b_i=a_i$, l'idéal $J$ engendré par $(b_i)_{i\in \crblanc{1}{q+1}}$ est un idéal de $A$ de type fini. C'est donc un élément de $E$. Cependant $I$ ne peut contenir $J$ puisque $a\in J\backslash I$. Ceci contredit la maximalité de $I$ dans $E$ et on en déduit que $A=I$. Par conséquent que $A$ est de type fini (en tant qu'idéal de lui-même).\\
		Ainsi un idéal quelconque de $A$ se doit d'être de type fini puisqu'il est contenu dans $A$.
		\item La question précédente justifie immédiatement que tout anneau principal, où les idéaux sont engendrés par un seul élément, est noethérien et vérifie donc (ii). On sait ou on redémontre (cf. \ref{Sous-groupes de (Z,+)}) que $\Z$ est principal ce qui termine la question.\\
		Toutefois on peut aussi le faire de manière directe : On se donne une suite croissante $(I_n)_{n\in \N}$ d'idéaux de $\Z$. On sait qu'il existe une suite $(N_n)_{n\in \N}$ d'entiers positifs tels que $\forall n\in \N,\ I_n=N_n\Z$. Si la suite $(I_n)_{n\in \N}$ ne stationne pas à $\{0\}$ alors APCR $N_n>0$.\\
		Or $\forall n\in \N,\ N_n\Z\subset N_{n+1}\Z\implies N_{n+1}|N_n$. Or comme APCR $N_n>0$, la suite $(N_n)_{n\in \N}$ est une suite d'entiers positifs décroissante APCR. On montre alors classiquement qu'elle stationne et donc que $(I_n)_{n\in \N}$ stationne.
		\item De même, le cours fournit que $\K[X]$ est principal ce qui, associé à la question $1$, termine la question. Si l'on veut faire sans :\\
		On se donne une suite croissante $(I_n)_{n\in \N}$ d'idéaux de $\K[X]$.\\
		On sait qu'il existe une suite $(P_n)_{n\in \N}$ de $\K[X]$ telle que $\forall n\in \N,\ I_n=P_n\Z$. Si la suite $(I_n)_{n\in \N}$ ne stationne pas à $\{0_{\K[X]}\}$ alors APCR $\deg(P_n)\geq 0$.\\
		Or $\forall n\in \N,\ (P_n\K[X]\subset P_{n+1}\K[X]\land P_n\ne 0_{\K[X]})\implies P_{n+1}|P_n\implies \deg(P_{n+1})\leq \deg(P_n)$. Or comme $\forall n\in \N,\ \deg(P_n)\geq 0$, la suite $(\deg(P_n))_{n\in \N}$ est une suite d'entiers positifs décroissante. On montre alors classiquement qu'elle stationne. On sait alors que APCR $P_n$ et $P_{n+1}$ sont associés. Deux polynômes associés engendrant le même idéal, on en déduit que $(I_n)_{n\in \N}$ stationne.
	\end{enumerate}
	\subsection{Morphismes d'anneaux de fonctions réelles}
	\label{Morphismes d'anneaux de fonctions réelles corrigé}
	\textcolor{blue}{\hyperref[Morphismes d'anneaux de fonctions réelles]{[Enoncé]}}\\
	\subsection{Caractérisation d'un corps par ses idéaux \etoile{2}}
	\label{Caractérisation d'un corps par ses idéaux corrigé}
	\label{Idéaux d'un corps}
	\textcolor{blue}{\hyperref[Caractérisation d'un corps par ses idéaux]{[Enoncé]}}\\
	Supposons que $A$ est un corps. Soit $I$ un idéal non nul de $A$.\\
	Alors il existe $x\in \backslash\{0_A\}$. Comme $A$ est un corps, $x$ est inversible et par absorbance, $1_A=xx^{-1}\in I$. On en déduit encore par absorbance que $\forall a\in A,\ a=1_A\times a\in I$. Ainsi $A=I$.\\
	Réciproquement supposons que les seuls idéaux de $A$ soient $\{0_A\}$ et $A$. Fixons $x\in A$ non nul et montrons qu'il est inversible.\\
	On considère $I=\{xa,\ a\in A\}$. $I$ est un idéal de $A$ (c'est l'idéal engendré par $x$), on sait alors qu'il est soit nul soit égal à $A$.\\
	Or il ne peut pas être nul car sinon comme $x\in I$, $x$ serait nul. Donc $I=A$ et en particulier $1_A\in I$. C'est à dire qu'il existe $a\in A$ tel que $xa=1_A$. Comme $A$ est commutatif, cela montre que $x$ est inversible d'inverse $a$.
	
	\subsection{Opérations sur les idéaux, idéaux principaux \etoile{3}}
	\label{Opérations sur les idéaux, idéaux principaux corrigé}
	\textcolor{blue}{\hyperref[Opérations sur les idéaux, idéaux principaux]{[Enoncé]}}\\
	\begin{enumerate}
		\item Soient $I$ et $J$ deux idéaux de $A$.\\
		On sait (cf. \ref{Opérations sur les sous-groupes}) que $I\cap J$ est un sous groupe de $(A,+)$. De plus, si $a\in A$ et $x\in I\cap J$ alors $ax\in I$ et $ax\in J$ par absorbance des idéaux $I$ et $J$ c'est à dire $ax\in I\cap J$. Ainsi $I\cap J$ est un idéal de $A$.\\
		Tout d'abord $I+J\subset A$ et $0_A=0_A+0_A$ et $0_A\in I$ et $0_A\in J$ donc $0_A\in I+J$.\\
		Ensuite si $x,y\in I+J$ alors $\exists (a,b),(c,d)\in I\times J,\ x=a+b,\ y=c+d$. Donc $x-y=a-c+b-d\in I+J$ car $a-c\in I$ et $b-d\in J$. Ainsi $I+J$ est un sous-groupe de $(A,+)$. Enfin, si $a\in A$ et $x\in I+J$ alors il existe $(i,j)\in I+J$ tel que $x=i+j$. Donc $ax=ai+aj\in I+J$ car $ai\in I$ et $aj\in J$ par absorbance. Ainsi $I+J$ est un idéal de $A$.
		\item \begin{enumerate}[label=\alph*.]
			\item D'après le cours on sait que $a\Z+b\Z=\pgcd(a,b)\Z$ et $a\Z\cap b\Z=\ppcm(a,b)\Z$.
			\item Dans $\Z$ on sait qu'en écrivant $a=\pgcd(a,b)u$ et $b=\pgcd(a,b)v$ avec $u,v\in \Z$, alors comme $\ppcm(a,b)\times\pgcd(a,b)=|ab|$, on a $\ppcm(a,b)=|\pgcd(a,b)uv|$.\\ 
			Soit $(a,b)\in A^2$. On suppose qu'il existe $d\in A$ tel que $aA+bA=dA$. $b\in dA$ et $a\in dA$ donc $\exists u,v\in A,\ a=du,\ b=dv$. En s'inspirant du résultat dans $\Z$ on pose $c=duv$. Montrons que $cA=aA\cap bA$.\\
			$c=av=bu$ donc $c\in aA\cap bA$. Donc $cA\subset aA\cap bA$.\\
			Réciproquement si $x\in aA\cap bA$ alors $\exists \alpha,\beta\in A,\ x=\alpha a=\beta b$. Par ailleurs il existe $\lambda,\mu\in A$ tels que $d=\lambda a+\mu b$.\\
			Ainsi $x=\alpha a=\alpha ud=\alpha u(\lambda a+\mu b)=\lambda ux+\alpha\mu c=\lambda\beta udv+\alpha\mu c=(\beta\lambda+\alpha\mu)c\in cA$. Donc $aA\cap bA\subset cA$ puis $aA+bA=cA$.
		\end{enumerate}
	\end{enumerate}
	
	\subsection{Idéal premier \etoile{2}}
	\label{Idéal premier corrigé}
	\textcolor{blue}{\hyperref[Idéal premier]{[Enoncé]}}\\
	Soit $a\in A\backslash\{0_A\}$.\\
	$a^2A$ est un idéal de $A$ et $a^2=a\times a\in a^2A$ donc $a\in a^2A$.\\
	Autrement dit $\exists b\in A,\ a=a^2b$ i.e $a(ab-1_A)=0_A$.\\
	Or $A$ est intègre, en effet $\{0_A\}$ est un idéal de $A$ donc $\forall x,y\in A,\ xy=0_A\implies (x=0_A\text{ ou }y=0_A)$.\\
	Donc comme $a\ne 0_A$, $ab=1_A$ c'est à dire $a$ est inversible (d'inverse $b$).\\
	Ainsi $A$ est un corps.
	
	\subsection{Idéal maximal \etoile{4}}
	\label{Idéal maximal corrigé}
	\textcolor{blue}{\hyperref[Idéal maximal]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $I$ un idéal de $A$.\\
		Supposons que $I$ est maximal et fixons $a\in A\backslash I$. $I+aA$ est un idéal de $A$ qui contient $I$, il est donc égal à $I$ ou à $A$. Or $a\in I+aA$ et $a\notin I$ donc $I+aA=A$.\\
		Réciproquement supposons que $\forall a\in A\backslash I,\ I+aA=A$ et fixons $J$ un idéal de $A$ qui contient strictement $I$.\\
		On sait qu'il existe $a\in J\backslash I$. Comme $a\in J,\ aA\subset J$. Par suite, $I+aA\subset J$. Enfin comme $a\notin I,\ I+aA=A$. Ainsi $J=A$ et $I$ est maximal.
		\item Soit $I$ un idéal maximal de $A$. Fixons $a,b\in A\backslash I$.\\
		D'après la question précédente $I+aA=A=I+bA$. On sait alors qu'il existe $x\in I$ ainsi que $u\in A$ tel que $1_A=x+au$.\\
		Donc $b=bx+bau$. Si $ab\in I$ alors $bau\in I$. De plus $bx\in I$ car $x\in I$ donc $b\in I$ ce qui est absurde. Ainsi $ab\notin I$ et $I$ est premier.
		\item Dans n'importe quel anneau intègre, l'idéal nul est premier : cela veut exactement dire que l'anneau est intègre.\\
		Dans $\Z$, si $p$ est un nombre premier alors $p\Z$ est premier : c'est le lemme d'Euclide. De plus ce sont les seuls idéaux premiers, en effet si $a$ est un entier composée alors en écrivant $a=bc$ avec $b\ne a$ et $c\ne a$ alors $b\notin a\Z$ et $c\notin a\Z$ pourtant $bc\in a\Z$.\\
		Notons aussi que si $p$ est premier alors $p\Z$ est maximal : $\forall a\notin p\Z,\ a\Z+p\Z=\pgcd(a,p)\Z=\Z$.
		\item
		\begin{enumerate}[label=\alph*.]
			\item La fonction identiquement nulle s'annule en $0$ et appartient donc à $I$. Fixons $(f,g,h)\in I^2\times A$.\\
			$(f-g)(0)=f(0)-g(0)=0$ donc $f-g\in I$ et $(f\times h)(0)=f(0)h(0)=0$ donc $f\times h\in I$.\\
			Ainsi $I$ est un idéal de $A$. Montrons qu'il est principal.\\
			Soit $f\in I$. Montrons que $g:x\mapsto \begin{cases}
				\displaystyle\frac{f(x)}{x}&\mbox{si }x\ne 0\\
				f'(0)&\mbox{si }x=0
			\end{cases}$ est $\mathcal C^\infty$ sur $\R$.\\
			$\forall x\in \R^*,\ f(x)=\displaystyle\int_0^xf'(t)dt=x\int_0^1f'(ux)du$ c'est à dire $\forall x\in \R^*,\ g(x)=\displaystyle\int_0^1f'(ux)du$. L'égalité est encore vraie pour $x=0$.\\
			De plus, $h:(u,x)\mapsto f'(ux)$ est de classe $\mathcal C^\infty$ sur $\R\times[0,1]$ par composition et pour tout $k\in \N,\ \dpartial{h}{k}{x}(u,x)=u^kf^{(k+1)}(ux)$ est continue sur $\R\times[0,1]$. On peut donc dominer $u\mapsto\dpartial{h}{k}{x}$ par $\varphi:u\mapsto \normep{\infty,[a,b]}{u\mapsto\dpartial{h}{k}{x}}$ sur un segment $[a,b]\subset \R$.\\
			Ainsi $g\in A$ par le théorème de transfert $\mathcal C^\infty$.\\
			Donc $f=\Id_\R\times g\in \Id_\R A$.\\
			Réciproquement $\Id_\R\in I$ donc $\Id_\R A\subset I$ d'où $I=\Id_\R A$.\\
			Montrons que $I$ est maximal. Soient $f\notin I$ et $g\in A$.\\
			$\forall x\in \R,\ g(x)=\displaystyle\left(g(x)-\frac{g(0)}{f(0)}f(x)\right)+\frac{g(0)}{f(0)}f(x)$.\\
			$h:x\in \R\mapsto \displaystyle g(x)-\frac{g(0)}{f(0)}f(x)\in I$ et $\displaystyle\frac{g(0)}{f(0)}f\in fA$ donc $g\in I+fA$.\\
			Ainsi $I+fA=A$ et $I$ est maximal. D'après la question précédente il est donc premier.
			\item La fonction nulle est évidemment élément de $J$. Fixons $(f,g,h)\in J^2\times A$.\\
			$\forall k\in \N,\ (f-g)^{(k)}(0)=f^{(k)}(0)-g^{(k)}(0)=0$ donc $f-g\in J$ et d'après la formule de Leibniz,\\
			$\forall k\in \N,\ (f\times h)^{(k)}(0)=\displaystyle\sum_{i=0}^k\binom{k}{i}f^{(i)}(0)h^{(k-i)}(0)=0$ donc $f\times h\in J$.\\
			Ainsi $J$ est un idéal de $A$. Il n'est pas maximal puisqu'il est contenu strictement dans $I$. Nonobstant il est premier :
			Si $f,g\notin I$ alors on note $n$ l'entier tel que $f^{(n)}$ est la plus petite dérivée de $f$ qui ne s'annule pas en $0$. On note de même $m$ pour $g$. $(f\times g)^{(m+n)}$ ne s'annule pas en $0$ (il ne reste que le terme $f^{(n)}(0)g^{(m)}(0)$ qui est non nul).\\\\
			Montrons par l'absurde que $J$ n'est pas principal. On note $f$ telle que $J=fA$.\\
			Comme $f(0)=0$, d'après la question précédente il existe $g\in A$ telle que $f=\Id_\R g$.\\
			Posons $\varphi:x\in \R\mapsto\begin{cases}
				e^{-1/x^2}&\mbox{si }x\ne 0\\
				0&\mbox{si }x=0
			\end{cases}$. Il est classique de montrer que $\varphi\in J$ : On montre que les dérivés successives de $\varphi$ s'exprime sous la forme d'une fraction rationnelle que multiplie $e^{-1/x^2}$ et on utilise le théorème de la limite de la dérivée.\\
			On peut donc trouver une fonction $h\in A$ telle que $\varphi=fh$. Comme $\varphi$ ne s'annule qu'en $0$, il en est de même pour $f$. Il en est donc de même pour tous les éléments de $J$.\\
			On a montré dans la question précédente que $g=x\in \R\mapsto \displaystyle\int_0^1f'(ux)du$ et que en particulier, $\forall k\in \N,\ g^{(k)}(0)=\displaystyle\int_0^1u^kf^{(k+1)}(0)du=0$. Ainsi $g\in J$.\\
			On en déduit qu'il existe $h\in A$ telle que $g=fh=\Id_\R gh$. Or $g$ ne s'annule qu'en $0$ donc $\forall x\in \R^*,\ 1=xh(x)$. On obtient une absurdité en faisant tendre $x$ vers $0$.\\
			Finalement $J$ n'est pas principal.
		\end{enumerate}
	\end{enumerate}
	\subsection{Idéaux d'un espace de  fonction}
	\label{Idéaux d'un espace de fonction corrigé}
	\textcolor{blue}{\hyperref[Idéaux d'un espace de fonction]{[Enoncé]}}
	\subsection{Radical d'un idéal \etoile{1}}
	\label{Radical d'un idéal corrigé}
	\textcolor{blue}{\hyperref[Radical d'un idéal]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $(\forall x\in I,\ x^1\in I)\implies I\subset R(I)$. En particulier $0_A\in R(I)$. $R(I)$ est clairement inclus dans $A$. Soient $x,y\in R(I)$.\\
		$A$ étant commutatif, $\forall p\in \N,\ (x-y)^p=\displaystyle\sum_{k=0}^p\binom{p}{k}x^k(-y)^{p-k}$.\\
		On sait qu'il existe deux entiers naturels $n$ et $m$ tels que $x^n\in I$ et $y^m\in I$. Posons $p=n+m$.\\
		Si $n\leq k\leq p$ alors $\displaystyle\binom{p}{k}x^k(-y)^{p-k}=x^n\times \binom{p}{k}x^{k-n}(-y)^{p-k}\in I$ par absorbance.\\
		Et si $0\leq k\leq n-1$ alors $m+1\leq p-k$ donc $\displaystyle\binom{p}{k}x^k(-y)^{p-k}=y^m\times (-1)^{p-k}\binom{p}{k}x^ky^{p-k-m}\in I$ par absorbance.\\
		Finalement $(x-y)^p\in I$ car $I$ est stable par somme.\\
		De plus, $\forall a\in A,\ (ax)^n=a^nx^n\in I$ par absorbance.\\
		Ainsi $R(I)$ est un idéal de $A$.
		\item D'après la question précédente on a déjà $R(I)\subset R(R(I))$.\\
		Fixons $x\in R(R(I))$. On sait qu'il existe $n\in \N$ tel que $x^n\in R(I)$. Donc on sait qu'il existe $m\in \N,\ (x^n)^m=x^{nm}\in I$. $nm\in \N$ donc $x\in R(I)$.\\
		Ainsi $R(R(I))=R(I)$.
		\item Il est évident que si $E\subset F$ sont deux idéaux de $A$ alors $R(E)\subset R(F)$. $I\cap J\subset I$ et $I\cap J\subset J$ donc $R(I\cap J)\subset R(I)$ et $R(I\cap J)\subset R(J)$. Par conséquent $R(I\cap J)\subset R(I)\cap R(J)$.\\
		Réciproquement donnons nous $x\in R(I)\cap R(J)$. On sait qu'il existe deux entiers naturels $n$ et $m$ tels que $x^n\in I$ et $x^m\in I$. Posons $p=\max(n,m)\in \N$. $x^p=x^nx^{p-n}=x^mx^{p-m}\in I\cap J$ donc $x\in R(I\cap J)$.\\
		Ainsi $R(I\cap J)=R(I)\cap R(J)$.
	\end{enumerate}
	
	\subsection{Nilradical \etoile{2}}
	\label{Nilradical corrigé}
	\textcolor{blue}{\hyperref[Nilradical]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Tout d'abord, $\mathcal N(A)\subset A$ et $0_A=0_A^1\in \mathcal N(A)$. Fixons $(x,y,a)\in \mathcal N(A)^2\times A$.\\
		$A$ étant commutatif, $\forall p\in \N,\ (x-y)^p=\displaystyle\sum_{k=0}^p\binom{p}{k}x^k(-y)^{p-k}$.\\
		On sait qu'il existe deux entiers naturels $n$ et $m$ tels que $x^n\in I$ et $y^m\in I$. Posons $p=n+m$.\\
		Si $n\leq k\leq p$ alors $\displaystyle\binom{p}{k}x^k(-y)^{p-k}=x^n\times \binom{p}{k}x^{k-n}(-y)^{p-k}=0_A$.\\
		Et si $0\leq k\leq n-1$ alors $m+1\leq p-k$ donc $\displaystyle\binom{p}{k}x^k(-y)^{p-k}=y^m\times (-1)^{p-k}\binom{p}{k}x^ky^{p-k-m}=0_A$.\\
		Finalement $(x-y)^p\in I$ car $I$ est stable par somme.\\
		De plus, $(ax)^n=a^nx^n=0_A$.\\
		Ainsi $\mathcal N(A)$ est un idéal de $A$.\\
		\textit{Remarque : C'est le radical de l'idéal nul, $\mathcal N(A)=R(\{0_A\})$.}
		\item Soit $\overline k$ un élément nilpotent de $\Z/n\Z$ (on prend $k$ entre $0$ et $n-1$). On note $m\in \N^*$ tel que ${\overline k}^p=\overline{k^m}=\overline 0$, i.e $n|k^m$.\\
		Notons $p$ un diviseur premier. $p|n|k^m$ donc $p|k^m$. D'après le lemme d'Euclide, $p|k$. Ainsi $k$ doit admettre au moins tous les diviseurs premiers de $n$ comme diviseurs.\\
		Réciproquement si quel que soit $p$ premier, $p|n\implies p|k$ alors en notant $m$ la valuation $p$-adique de $n$ maximale, on peut affirmer que quel que soit $p$ un diviseur premier de $n$, $v_p(k)\geq 1$ d'où $v_p(n)\leq m\leq mv_p(k)=v_p(k^m)$. Donc $n|k^m$ c'est à dire ${\overline k}^m=\overline 0$, c'est à dire $\overline k\in \mathcal N(\Z/n\Z)$.\\
		Finalement, $\mathcal N(\Z/n\Z)=\{\overline k\ |\ k\in \crblanc{0}{n-1},\ \forall p\in \p,\ p|n\implies p|k\}$.
	\end{enumerate}
	
	\subsection{Radical de Jacobson \etoile{2}}
	\label{Radical de Jacobson corrigé}
	\textcolor{blue}{\hyperref[Radical de Jacobson]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $I$ un idéal de $A$.\\
		Supposons que $I$ est maximal et fixons $a\in A\backslash I$. $I+aA$ est un idéal de $A$ qui contient $I$, il est donc égal à $I$ ou à $A$. Or $a\in I+aA$ et $a\notin I$ donc $I+aA=A$.\\
		Réciproquement supposons que $\forall a\in A\backslash I,\ I+aA=A$ et fixons $J$ un idéal de $A$ qui contient strictement $I$.\\
		On sait qu'il existe $a\in J\backslash I$. Comme $a\in J,\ aA\subset J$. Par suite, $I+aA\subset J$. Enfin comme $a\notin I,\ I+aA=A$. Ainsi $J=A$ et $I$ est maximal.
		\item Soit $x\in A$.\\
		Supposons que $x\notin J$. On sait qu'il existe un idéal maximal $I$ de $A$ pour lequel $x\notin I$.\\
		Donc d'après la question 1, $I+xA=A$. En particulier $1_A\in I+xA$ i.e $\exists (y,a)\in I\times A,\ 1_A=y+ax$. Donc $1_A-ax=y\in I$. Or $I\ne A$ puisqu'il est maximal. Par conséquent tous ses éléments sont non inversibles, puisque sinon on aurait $1_A\in I$ par absorbance puis $I=A$ encore par absorbance. Ainsi $1_A-ax\notin A^\times$.\\\\
		Réciproquement supposons qu'il existe $a\in A$ tel que $y=1_A+ax\notin A^\times$.\\
		On sait alors que $yA\ne A$. D'après le théorème admis on peut l'inclure dans un idéal maximal $I$. Supposons par l'absurde que $x\in I$.\\
		Alors $ax\in I$ par absorbance et comme $y\in I$ par définition de $I$ on obtient que $1_A\in I$ d'où $I=A$ ce qui est absurde.\\
		Ainsi $x\notin I$ et par suite, $x\notin J$.
	\end{enumerate}
	\textit{Remarque : Le théorème de Krull est en fait équivalent à l'axiome du choix}
	
	\subsection{Idéaux de $\M_n(\K)$}
	\label{Idéaux de Mn(K) corrigé}
	\textcolor{blue}{\hyperref[Idéaux de Mn(K)]{[Enoncé]}}\\
	
	
	\subsubsection{Idéaux bilatère}
	
	
	\subsubsection{Idéaux à droite}
	
	
	\subsubsection{Idéaux à gauche}
	
	
	\subsection{Caractéristique d'un anneau}
	\label{Caractéristique d'un anneau corrigé}
	\textcolor{blue}{\hyperref[Caractéristique d'un anneau]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item \begin{itemize}
			\item $\phi(1)=1.1_A=1_A$.
			\item Soit $(k,l)\in\Z^2$. $\varphi(k+l)=(k+l)1_A=k1_A+l1_A=\varphi(k)+\varphi(l)$.
			\item Soit $(k,l)\in\Z^2$. $\varphi(kl)=(kl)1_A=k1_A\circ l1_A=\varphi(k)\circ\varphi(l)$.
			\end{itemize}
			Donc $\varphi$ est un morphisme d'anneaux.
		\item On sait que le noyau de $\varphi$ est un sous-groupe de $(\Z,+)$. Or les sous-groupes de $(\Z,+)$ sont les $n\Z$. \ref{Sous-groupes de (Z,+)}.\\ Donc il existe $n\in\N$ tel que $\Ker(\varphi)=n\Z$.
		\item  Supposons que $A$ est un anneau intègre.\\
		On note $n$ la caractéristique de $A$.\\
		Si $n=0$, on a terminé.\\
		Si $n>0$, montrons que $n$ est premier.
		Supposons que $n$ n'est pas premier. Il existe $(a,b)\in\Z^2$ tel que $n=ab$.\\
		On a $(a1_A)(b1_B)=(ab)1_A=n1_A=0$.\\
		Puisque $A$ est un anneau intègre, on en déduit que $a$ ou $b$ est nul.\\
		Ce qui contredit le fait que $n>0$, ce qui est absurde.\\
		Donc $n$ est premier.
		\item On sait qu'un corps est un anneau intègre, donc d'après la question précédent, $K$ est de caractéristique nulle ou égale à un nombre premier.\\
		Supposons que $p$ est nulle.\\
		Cela signifie que $\forall k\in\Z^*, k1_K\ne O_K$.\\
		Ainsi, on en déduit que $\forall(k,j)\in\N^*, k1_K\ne j1_K$, ce qui est absurde car $K$ est fini.
		Donc $p$ est un nombre premier.\\
		On sait que $(K,+,\times)$ est un corps, il suffit donc de munir $K$ d'une loi de composition externe $*$.\\
		$\forall \lambda\in\Z/p\Z, \exists k\in\Z$ tel que $\lambda=\overline{k}$.
		On pose : \[\fonction{f}{\Z/p\Z\times K}{K}{(\lambda,x)}{kx}\]
		Montrons que $f$ est bien définie.\\
		$\forall (k,k')\in\Z$ tel que $\overline{k}=\overline{k'}$.\\ Il existe $l\in\Z$, tel que $k'=k+lp$.
		\[\forall x\in K, k'x=kx+lpx=kx\]
		car $K$ est de caractéristique $p$.\\
		Il suffit maintenant de vérifier que : 
		\begin{itemize}
			\item Pour tout $x\in K$, $f((\overline{1},x))=x$
			\item Pour tous $(\overline{k},\overline{l})\in(\Z/p\Z)^2, x\in K$, $f((\overline{k+l},x))=f(\overline{k},x)+f(\overline{l},x)$
			\item Pour tous $(\overline{k},\overline{l})\in(\Z/p\Z)^2, x\in K$, $f((\overline{kl},x))=f(\overline{k},x)\times f(\overline{l},x)$
			\item Pour tous $\overline{k}\in\Z/p\Z, (x_1,x_2)\in K^2$, $f((\overline{k},x_1+x_2))=f((\overline{k},x_1))+f((\overline{k},x_2))$
		\end{itemize}
		Par conséquent, $K$ est alors un $\Z/p\Z$-espace vectoriel de dimension finie.\\
		Soit $(e_1,\dots,e_n)$ une base de $K$.\\
		On pose : \[\fonction{f}{(\Z/p\Z)^n}{K}{(\lambda_1,\dots,\lambda_n)}{\displaystyle\sum_{k=1}^{n}\lambda_k e_k}\]
		Cette application est bien évidemment un isomorphisme.\\
		Par conséquent, \[\operatorname{Card}(K)=p^n\]
		
	\end{enumerate}
	
	\subsection{Anneau intègre \etoile{2}}
	\label{Anneau intègre corrigé}
	\textcolor{blue}{\hyperref[Anneau intègre]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $(K,+,\times)$ un corps et soient $x\in K\backslash\{0\},\ y\in K$.\\
		Si $xy=0_K$ alors en multipliant par $x^{-1}$ à gauche on a $y=0_K$.\\
		Ainsi $K$ est intègre.
		\item L'anneau des nombres décimaux est intègre en tant que sous-anneau de $\C$ qui l'est. Pourtant ce n'est pas un corps car par exemple $3$ n'admet pas d'inverse dans $\D$. De même pour l'anneau des entiers relatifs $\Z$.
		\item Tout d'abord $A[X]$ est un anneau commutatif.\\
		Supposons que $A$ est intègre. Soient $P,Q\in A[X]\backslash\{0\}$. On écrit $P=\displaystyle\sum_{n=0}^{+\infty}a_nX^n$ et $Q=\displaystyle\sum_{n=0}^{+\infty}b_nX^n$.\\
		Alors $PQ=\displaystyle\sum_{n=0}^{+\infty}\left(\sum_{k=0}^na_kb_{n-k}\right)X^n=\sum_{n=0}^{+\infty}c_nX^n$. Notons $i$ l'indice du premier coefficient non nul de $P$ et $j$ l'indice du premier coefficient non nul de $Q$. On sait qu'ils en ont au moins un puisqu'ils sont non nuls.\\
		Alors $c_{i+j}=a_ib_j\ne 0$ d'où $PQ\ne 0$ et $A[X]$ est intègre.\\
		Réciproquement, si $A[X]$ est intègre alors :
		$$\forall x,y\in A,\ xy=0_A\implies x1_{A[X]}\times y1_{A[X]}=0_{A[X]}\implies x1_{A[X]}=0_{A[X]}\vee y1_{A[X]}=0_{A[X]}\implies x=0_A\vee y=0_A$$
		Donc $A$ est intègre.
	\end{enumerate}
	
	\subsection{Sous-corps minimal de $\C$ \etoile{1}}
	\label{Sous-corps minimal de C corrigé}
	\textcolor{blue}{\hyperref[Sous-corps minimal de C]{[Enoncé]}}\\
	Soit $K$ un sous-corps de $\C$. On a $0,1\in K$.\\
	Par somme et passage à l'opposé on a donc $\Z\subset K$. Puis par quotient, $\Q\subset K$.\\
	$\Q$ est un sous-corps de $\C$, c'est donc le sous-corps minimal de $\C$ au sens de l'inclusion.
	
	\subsection{Corps d'Attila}
	\label{Corps d'Attila corrigé}
	\textcolor{blue}{\hyperref[Corps d'Attila]{[Enoncé]}}\\
	\begin{enumerate}
		\item $(Vect(A),+)$ est un sous-espace vectoriel de $\M_n(\K)$ donc $(Vect(A),+)$ est un groupe abélien.\\
		Par associativité et distributivité du produit de $\K$, on en déduit que $\times$ est associative et distributive par rapport à $+$.\\
		On cherche maintenant le neutre de cette loi i.e. on cherche $\lambda\in\K$ tel que \[\forall \mu\in \K, \lambda A\mu A=\mu A\]
		Soit $\mu\in \K$.
		On remarque que $A^2=nA$ ainsi :
		\[\lambda\mu n A=\lambda \mu A^2=\mu A\]
		Donc $\lambda n=1$ i.e. \[\lambda=\frac{1}{n}\]
		Donc, le neutre pour cette loi est $\displaystyle\frac{1}{n} A$.\\
		\textit{Remarque : le neutre ici n'est pas $I_n$.}
		On vérifie que l'inverse de $\mu A$ est $\displaystyle \frac{1}{\mu n} A$.
		\item D'après ce qui précède, $(\Vect(A)\setminus{0},\times)$ est un groupe de neutre $\displaystyle\frac{1}{n}A$.
	\end{enumerate}
	\subsection{Endomorphisme de corps de $\R$ \etoile{2}}
	\label{Endomorphisme de corps de R corrigé}
	\textcolor{blue}{\hyperref[Endomorphisme de corps de R]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On sait que $f(0)=0$ et $f(1)=1$ donc par récurrence immédiate à l'aide de la relation $f(x+y)=f(x)+f(y),\ \forall n\in \N,\ f(n)=n$. Donc $\forall n\in \N,\ 0=f(0)=f(n-n)=f(n)-f(n)$ d'où $\forall k\in \Z,\ f(k)=k$.\\
		Fixons $x=\displaystyle\frac{p}{q}$ avec $p\in \Z$ et $q\in \N^*$.\\
		On a $qf(x)=f(qx)=f(p)=p$. Donc $f(x)=\displaystyle\frac{p}{q}=x$.\\
		Ainsi $f_{|\Q}=\Id_\Q$.
		\item Soit $z\geq 0$. $f(z)=f({\sqrt z}^2)=f(\sqrt z)^2\geq 0$.\\
		Soient $x,y\in \R$ $x\geq y$.\\
		$f(x)-f(y)=f(x-y)\geq 0$ car $x-y\geq 0$. Ainsi $f$ est croissante sur $\R$.
		\item Soient $x\in \R$. Supposons que $f(x)\ne x$.\\
		Si $f(x)>x$ alors par densité de $\Q$ dans $\R$, $\exists t\in \Q,\ f(x)>t>x$.\\
		Alors $f(x)-t=f(x)-f(t)=f(x-t)\geq 0$ d'où $x-t\geq 0$ par croissance de $f$. Ceci est absurde donc $f(x)<x$. Mais alors par densité de $\Q$ dans $\R$, $\exists u\in \Q,\ f(x)<u<x$. De même on obtient $x-u<0$ ce qui est absurde.\\
		On en déduit que $f(x)=x$ et $f=\Id_\R$.
	\end{enumerate}
	
	\subsection{Automorphismes de $\Q[\sqrt 2]$ \etoile{3}}
	\label{Automorphismes de Qsqrt2 corrigé}
	\textcolor{blue}{\hyperref[Automorphismes de Qsqrt2]{[Enoncé]}}\\
	L'application $\fonction{\Phi}{\Q[X]}{\Q[\sqrt2]}{P}{P(\sqrt 2)}$ est un morphisme d'algèbre.\\
	Donc $\Q[\sqrt 2]$ est une sous-algèbre de $\C$ et en particulier un sous-anneau de $\C$.\\
	De plus, si $a+b\sqrt 2\in \Q[\sqrt 2]\backslash\{0\}$ alors $\displaystyle\frac{1}{a+b\sqrt 2}=\frac{a-b\sqrt 2}{a^2-2b^2}=c+d\sqrt 2$ avec $c=\displaystyle\frac{a}{a^2-2b^2}\in \Q$ et $d=-\displaystyle\frac{b}{a^2-2b^2}\in \Q$.\\
	Donc $\displaystyle\frac{1}{a+b\sqrt 2}\in \Q[\sqrt 2]$ c'est à dire $\Q[\sqrt 2]$ est un sous-corps de $\C$.\\
	Donnons nous $f$ un automorphisme de $\Q[\sqrt 2]$.\\
	On remarque que $f(\sqrt 2)^2=f({\sqrt 2}^2)=f(2)=2f(1)=2$.\\
	Donc $f(\sqrt 2)=\varepsilon\sqrt 2$ avec $\varepsilon\in \{-1,1\}$.\\
	Ensuite, on sait que $f(0)=0$ et $f(1)=1$ donc par récurrence immédiate à l'aide de la relation $f(x+y)=f(x)+f(y),\ \forall n\in \N,\ f(n)=n$. Donc $\forall n\in \N,\ 0=f(0)=f(n-n)=f(n)-f(n)$ d'où $\forall k\in \Z,\ f(k)=k$.\\
	Fixons $x=\displaystyle\frac{p}{q}$ avec $p\in \Z$ et $q\in \N^*$.\\
	On a $qf(x)=f(qx)=f(p)=p$. Donc $f(x)=\displaystyle\frac{p}{q}=x$.\\
	Ainsi si $z=a+b\sqrt 2\in \Q[\sqrt 2]$ alors $f(x)=f(a)+f(b)f(\sqrt 2)=a+\varepsilon b\sqrt 2$.\\
	On vérifie que si $\varepsilon\in \{-1,1\},\ \fonction{f_\varepsilon}{\Q[\sqrt 2]}{\Q[\sqrt 2]}{a+b\sqrt 2}{a+\varepsilon b\sqrt 2}$ est un automorphisme de $\Q[\sqrt 2]$. Ce sont dont les seuls automorphismes de $\Q[\sqrt 2]$.
	
	\subsection{Algèbre des quaternions}
	\label{Algèbre des quaternions corrigé}
	\textcolor{blue}{\hyperref[Algèbre des quaternions]{[Enoncé]}}\\
	
	\subsection{Une définition de $\C$ \etoile{2}}
	\label{Une définition de C corrigé}
	\textcolor{blue}{\hyperref[Une définition de C]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soient $z=a+ib$ et $z'=a'+b'i$ des complexes. Soit $\lambda\in \R$.\\
		$\Phi(z+\lambda z')=\begin{pmatrix}
			a+\lambda a'&-(b+\lambda b')\\
			b+\lambda b'&a+\lambda a'\\
		\end{pmatrix}=\begin{pmatrix}
			a&-b\\
			b&a\\
		\end{pmatrix}+\lambda\begin{pmatrix}
			a'&-b'\\
			b'&a'\\
		\end{pmatrix}=\Phi(z)+\lambda \Phi(z')$,\\
		et $\Phi(zz')=\begin{pmatrix}
			aa'-bb'&-(ab'+a'b)\\
			ab'+a'b&aa'-bb'\\
		\end{pmatrix}=\begin{pmatrix}
			a&-b\\
			b&a\\
		\end{pmatrix}\times\begin{pmatrix}
			a'&-b'\\
			b'&a'\\
		\end{pmatrix}=\Phi(z)\Phi(z')$.\\
		Donc $\Phi$ est un morphisme de $\R$-algèbres.\\
		De plus, si $z\in \C$ tel que $\Phi(z)=I_2$ alors $\Re(z)=1$ et $\Im(z)=0$ c'est à dire $z=1$. Donc $\Phi$ est injective.
		\item Fixons $\theta\in \R$. $A_\theta=\Phi(i\theta)$.\\
		$\Phi$ étant linéaire sur $\C$ vu comme un $\R$-espace vectoriel de dimension finie, elle est continue.\\
		De plus, comme $\Phi$ est un morphisme d'algèbre, $\forall P\in \R[X],\ \Phi(P(z))=P(\Phi(z))$. Ainsi $\forall n\in \N,\ \displaystyle\sum_{k=0}^n\frac{A_\theta^k}{k!}=\Phi\left(\sum_{k=0}^{n}\frac{(i\theta)^k}{k!}\right)$.\\
		On sait que $\displaystyle\sum_{k=0}^{+\infty}\frac{(i\theta)^k}{k!}\unfty\longrightarrow e^{i\theta}$ et $\displaystyle\sum_{k=0}^{+\infty}\frac{A_\theta^k}{k!}\unfty\longrightarrow\exp(A_\theta)$.\\
		Donc par continuité de $\Phi$, $\exp(A_\theta)=\Phi(e^{i\theta})=\begin{pmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{pmatrix}$.
	\end{enumerate}
	
	\subsection{$\R$-algèbre commutative intègre de dimension finie \etoile{4}}
	\label{R algèbre commutative intègre de dimension finie corrigé}
	\textcolor{blue}{\hyperref[R algèbre commuttative intègre de dimension finie]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $\forall x,y\in \K,\ \forall \lambda\in \R,\ f(x+\lambda y)=a(x+y)=ax+\lambda ay=f(x)+\lambda f(y)$.\\
		De plus, si $z\in \K$ tel que $f(z)=0$ alors $az=0$ d'où $z=0$ par intégrité de $\K$, vu que $a$ n'est pas nul. Donc $f$ est un endomorphisme injectif de l'espace vectoriel $\K$ de dimension fini, c'est donc un automorphisme de $\K$. On en déduit qu'il existe $x\in \K,\ ax=1$. Autrement dit $a$ est inversible d'inverse $x$.
		\item Soient $\lambda,\mu\in \R$ tels que $\lambda+\mu a=0$. Si $\mu\ne 0$ alors $a=-\displaystyle\frac{\lambda}{\mu}\in \R$ ce qui est absurde. Donc $\mu=0$ et par suite $\lambda=0$. Par conséquent $(1,a)$ est libre dans $\K$.\\
		Ensuite, comme la famille $(1,a,\dots,a^n)$ est liée dans $\K$, $a$ admet un polynôme annulateur $P$ à coefficients réels.\\
		On écrit sa décomposition en produit de polynômes irréductibles dans $\R[X],\ P=\displaystyle\prod_{i=1}^rP_i$. On sait que $a$ est racine d'au moins un des $P_i$. Sans perte de généralité on suppose que c'est $P_1$.\\
		$P_1$ est irréductible dans $\R[X]$, il est donc de degré au plus $2$. Autrement dit en écrivant $P_1=\alpha+\beta X+\gamma X^2$ on a $\alpha+\beta a+\gamma a^2=0$ et $(1,a,a^2)$ est liée dans $\K$.
		\item Comme $\K$ est dimension supérieure à $2$ on sait qu'il existe $a\in \K\backslash \R$. D'après la question $2$ on peut écrire $\alpha +\beta a+a^2=0$ avec $(\alpha,\beta)\in \R^2$.\\
		Ainsi $\displaystyle\left(a+\frac{\beta}{2}\right)^2+\alpha-\frac{\beta^2}{4}=0$. On pose alors $i=\displaystyle\frac{2}{\sqrt{4\alpha-\beta^2}}\left(a+\frac{\beta}{2}\right)$.\\
		La quantité $4\alpha-\beta^2$ est bien positive puisque c'est l'opposé du déterminant du polynôme $X^2+\beta X+\alpha\in \R[X]$ dont on sait qu'il n'a pas de racines réelles ($a$ est une racine non réelle et la somme des racines vaut $-\beta\in \R$).\\
		$i$ est évidemment non réel donc la famille $(1,i)$ est libre dans $\K$. Montrons qu'elle est génératrice.\\
		Soit $a\in \K$. Si $a\in \R$ alors $a=a+0\cdot i\in \Vect(1,i)$. Et si $a\in \K\backslash \R$ alors $(1,a)$ est libre et $(1,a,a^2)$ est liée. Or d'après ce qui a été fait précédemment on peut écrire $i=\displaystyle\frac{2}{\sqrt{4\alpha-\beta^2}}\left(a+\frac{\beta}{2}\right)$ pour de certains réels $\alpha,\beta$.\\
		Mais alors $a=\displaystyle\frac{\sqrt{4\alpha-\beta^2}}{2}i-\frac{\beta}{2}\in \Vect(1,i)$.\\
		Finalement, $(1,i)$ est une base de $\K$ et donc $\K$ est de dimension $2$. On sait donc que $\K$ est isomorphe à $\C$ en tant que $\R$-algèbre.
	\end{enumerate}
	
	\subsection{Théorie algébrique des corps \etoile{4}}
	\label{Théorie algébrique des corps corrigé}
	\textcolor{blue}{\hyperref[Théorie algébrique des corps]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item cf. \ref{Anneau principal (2)}.
		\item Montrons que l'ensemble $I_a=\{P\in \K[X],\ P(a)=0\}$ est un idéal non nul de $\K[X]$. Déjà $I_a$ est non nul car $a$ est algébrique sur $\K$. Fixons $P,Q\in I_a$ et $R\in \K[X]$.\\
		$(P-Q)(a)=P(a)-Q(a)=0$ et $(RP)(a)=R(a)P(a)=0$ donc $I_a$ est un idéal de $\K[X]$. On sait d'après la question précédente qu'il existe un polynôme $P$ qui l'engendre.\\
		On pose $\mu$ le polynôme $P$ divisé par son coefficient dominant. $\mu$ et $P$ sont associés, ils engendrent donc le même idéal.\\
		Ainsi $\mu\K[X]=I_a$ c'est à dire $\forall Q\in \K[X],\ Q(a)=0\implies Q\in I_a\implies \mu|Q$.\\
		Supposons qu'il existe un autre polynôme $T$ unitaire tel que $I_a=T\K[X]$. Alors comme $\mu\in I_a,\ T|\mu$ et comme $T\in I_a,\ \mu|T$. $\mu$ et $T$ étant tout deux unitaires il suit que $T=\mu$.\\
		Vérifions maintenant que $\mu$ est irréductible. Soient $A,B\in \K[X]$ tels que $\mu=AB$.\\
		Alors $\mu(a)=0=A(a)B(a)$. $\K$ étant un corps, il est intègre d'où $A(a)=0$ ou $B(a)=0$. Donc $\mu|A$ ou $\mu|B$. Or $A$ et $B$ sont de degrés au plus $\deg\mu$. On en déduit que $A=\mu$ ou $B=\mu$ et donc que $\mu$ est irréductible.\\
		Ensuite montrons que $\F=(1,a,\dots,a^{\deg\mu-1})$ est une base de $\K[a]$. Soit $P\in \K[X]$. On écrit $P=Q\mu+R$ une division euclidienne de $P$ par $\mu$. On a $P(a)=Q(a)\mu(a)+R(a)=R(a)$ avec $\deg R\leq \deg\mu-1$. Ainsi $\K[X]=\Vect(\F)$.\\
		D'autre part, si $T\in \K[X]$ est de degré inférieur à $\deg\mu-1$ annule $a$ alors par définition de $\mu$, $\mu|T$. Ceci n'est possible que si $T\in \K[X]$ est nul. On a montré que la seule combinaison d'éléments de $\F$ à coefficients dans $\K$ qui est nulle est la combinaison nulle, c'est à dire que $\F$ est libre.\\
		Ainsi $\F$ est une base de $\K[a]$ et $\K[a]$ est un $\K$-espace vectoriel de dimension $\deg\mu$.\\
		Montrons que $\K[a]$ est un sous-corps de $\mathbb L$. Tout d'abord $0_{\K[X]}(a)=0\in \K[a]$ et $1_{\K[X]}(a)=1\in \K[a]$. Fixons $(P,Q)\in \K[X]^2$.\\
		$P-Q\in \K[X]$ donc $P(a)-Q(a)=(P-Q)(a)\in \K[a]$. De même, $P(a)Q(a)=(PQ)(a)\in \K[a]$.\\
		Supposons que $P(a)\ne 0$. $P$ n'est donc pas un multiple de $\mu$ et comme $\mu$ est irréductible, $P$ et $\mu$ sont premiers entre eux.\\
		Donc d'après le théorème de Bézout il existe $U,V\in \K[X]$ tels que $PU+\mu V=1$. Et en évaluant en $1$, $P(a)U(a)=1$ c'est à dire $P(a)^{-1}=U(a)\in \K[a]$.\\
		Par conséquent $\K[a]$ est un sous-corps de $\mathbb L$.
		\item Il s'agit de trouver un polynôme à coefficients rationnels qui annule $a$.\\
		On remarque que $(a-\sqrt 2)^3=2$ ce qui donne en développant $a^3+6a-2=\sqrt 2(3a^2+2)$ puis en élevant au carré :
		$$a^6-6a^4-4a^3+12a^2-24a-4=0$$
		On a montré que $a$ est algébrique (en fait c'est même un \textit{entier algébrique} puisque le polynôme annulateur est unitaire à coefficients entiers). On sait que son polynôme minimal $\mu$ est un diviseur de $P=X^6-6X^4-4X^3+12X^2-24X-4$.\\
		Considérons maintenant le $\Q$-espace vectoriel $\Q[a]$. D'après la question précédente il est de dimension $\deg\mu$.\\
		On remarque que la relation $a^3+6a-2=\sqrt 2(3a^2+2)$ montre que $\sqrt 2\in \Q[a]$, en effet $\Q[a]$ est un corps, $3a^2+2>0$ donc $\sqrt 2=(a^3+6a-2)(3a^3+6a-2)^{-1}\in \Q[a]$. On en déduit de plus que $\sqrt[3]2=a-\sqrt 2\in \Q[a]$. Enfin comme $\Q[a]$ est une $\Q$-algèbre, $\forall P\in \Q[X],\ P(\sqrt 2)\in \Q[a]$ et $P(\sqrt[3]2)\in \Q[a]$. On a donc les inclusions :
		$$\Q\subset \Q[\sqrt 2]\subset \Q[a]\text{ et }\Q\subset\Q[\sqrt[3]2]\subset\Q[a]$$
		Or le polynôme minimal de $\sqrt 2$ sur $\Q$ est $X^2-1$ et celui de $\sqrt[3]2$ est $X^3-2$, cela découle du fait qu'ils n'ont pas de racine rationnelle.\\
		La relation du multiplicité des degrés donne alors $2|\deg\mu$ et $3|\deg\mu$. Ainsi $6|\deg\mu$ d'où $6\leq \deg\mu$. Or $\mu|P$ donc $\deg\mu=6$.\\
		On en déduit que $\deg\mu=6$ et comme il est unitaire, $\mu=P$.
	\end{enumerate}
	
	\subsection{Famille $\Q$-libre \etoile{4}}
	\label{Famille Q-libre corrigé}
	\textcolor{blue}{\hyperref[Famille Q-libre]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Notons $\Q[\alpha]=\{P(\alpha),\ P\in \Q[X]\}$. Montrons que $\Q(\alpha)=\Q[\alpha]$. Dans un premier temps on montre (cf. \ref{Théorie algébrique des corps}) que $\Q[\alpha]$ est un sous-corps de $\C$ qui contient $\Q$ et $\alpha$. Dans un second temps, si $K$ est un corps qui contient $\Q$ et $\alpha$ alors il contient tous les polynômes en $\alpha$ à coefficients rationnels, c'est à dire $\Q[\alpha]$. Donc $\Q[\alpha]=\Q(\alpha)$.
		On montre ensuite (cf. \ref{Théorie algébrique des corps}) que $(1,\alpha,\dots,\alpha^{\deg\pi_\alpha-1})$ est une base de $\Q[\alpha]$. Donc $\Q(\alpha)$ est un $\Q$-espace vectoriel de dimension $\deg\pi_\alpha$.\\
		Enfin, montrons que $m_\alpha$ est un endomorphisme de $\Q(\alpha)$.\\
		$\forall x\in \Q(\alpha),\ \alpha\in \Q(\alpha)\implies m_\alpha(x)=\alpha x\in \Q(\alpha)$. De plus, $\forall x,y\in \Q(\alpha),\ \forall \lambda\in \Q,\ \ m_\alpha(x+\lambda y)=\alpha x+\lambda\alpha y=m_\alpha(x)+\lambda m_\alpha(y)$.\\
		\textit{Remarque : L'égalité $\Q(\alpha)=\Q[\alpha]$ n'est vraie que si $\alpha$ est un nombre algébrique. Par exemple pour $\pi$, $\displaystyle\frac{1}{\pi}\in \Q(\pi)$ puisque $\Q(\pi)$ est un corps. Or si il existait un polynôme $P\in \Q[X]$ tel que $P(\pi)=\displaystyle\frac{1}{\pi}$ alors le polynôme $Q=XP-1\in \Q[X]$ annulerait $\pi$. Or il est bien connu que $\pi$ est un nombre transcendant et donc que ceci n'est pas possible.}
		\item Le polynôme $X^2-d$ annule $\sqrt d$ donc $\sqrt d$ est algébrique. Déterminons son polynôme minimal.\\
		On sait que $\pi_{\sqrt d}|X^2-d$. Si $\pi_{\sqrt d}$ était de degré $1$ alors on aurait $\pi_{\sqrt d}=X-\sqrt d$.\\
		Or $\sqrt d\notin \Q$. En effet, supposons que $\sqrt d=\displaystyle\frac{a}{b}$ avec $a,b\in \N^*$ premiers entre eux. Alors $db^2=a^2$.\\
		Montrons qu'un entier positif est le carré d'un entier non nul si et seulement si toutes ses valuations $p$-adique sont paires. 
		Soit $m\in \N^*$. Si $m=1$ alors toutes ses valuation $p$-adique sont nulles donc c'est bon. Supposons que $m=k^2$ pour un certain $k\geq 2$. On sait que $m=\displaystyle\prod_{p\text{ premier}}p^{v_p(m)}=\left(\prod_{p\text{ premier}}p^{v_p(k)}\right)^2=\prod_{p\text{ premier}}p^{2v_p(k)}$. Donc par unicité de la décomposition en facteurs premiers, quel que soit $p$ premier $v_p(m)=2v_p(k)$.\\
		Réciproquement si quel que soit $p$ premier il existe $k_p\in \N$ tel que $v_p(m)=2k_p$, alors $m=k^2$ avec $k=\displaystyle\prod_{p\text{ premier}}p^{k_p}\in \N^*$.\\
		On en déduit que $d$ a un diviseur premier $p$ pour lequel $v_p(d)$ est impair. De plus on sait que $v_p(a^2)$ et $v_p(b^2)$ sont pairs. Mais alors l'égalité $db^2=a^2$ qui donne $v_p(d)+v_p(b^2)=v_p(a^2)$ est absurde. Ainsi $\sqrt d\notin \Q$.\\
		Par conséquent $\pi_\alpha$ est de degré au moins $2$ et par suite comme il est unitaire, $\pi_\alpha=X^2-d$.\\
		Alors d'après la question précédente $\Q(\sqrt d)$ est un $\Q$-espace vectoriel de dimension $2$ dont une base est $(1,\sqrt d)$. Autrement dit $\Q(\sqrt d)=\{a+b\sqrt d,\ (a,b)\in \Q^2\}$.
		\item Il est clair que $\forall k\in \N,\ \forall x\in \Q(\alpha),\ m_\alpha^k(x)=\alpha^kx$. Donc par linéarité, $\forall P\in \Q[X],\ \forall x\in \Q(\alpha),\ P(m_\alpha)(x)=P(\alpha)x$. Ainsi $\forall P\in \Q[X],\ P(m_\alpha)=0\iff P(\alpha)=0$ i.e $\pi_{m_\alpha}=\pi_\alpha$.\\
		Ensuite, $\forall x\in \Q(\alpha),\ \chi_{m_\alpha}(x)=\det(x\Id_{\Q(\alpha)}-m_\alpha)=\det(y\mapsto (x-\alpha)y)$.\\
		Donc $\chi_{m_\alpha}(\alpha)=\det(y\mapsto 0)=0$ d'où $\pi_\alpha|\chi_{m_\alpha}$.\\
		Ecrivons $\chi_{m_\alpha}=\pi_\alpha^r B$ avec $\pi_\alpha\wedge B=1$ et $B\in \Q[X]$ ce qui est possible car $\pi_\alpha$ est irréductible dans $\Q[X]$ (cf. \ref{Théorie algébrique des corps}). D'après le théorème de Bézout il existe $U,V\in \Q[X]$ tels que $U\pi_\alpha+BV=1$. En particulier cette relation montre que $\pi_\alpha$ et $B$ n'ont pas de racines communes dans $\C$. 
		si $B$ est non constant alors il possède une racine $z\in \C$. Alors $z$ est racine de $\chi_{m_\alpha}$ pourtant les racines de $\chi_{m_\alpha}$ sont les racines de $\pi_\alpha=\pi_{m_\alpha}$. Par conséquent $B$ est constant et donc comme $\chi_{m_\alpha}$ et $\pi_\alpha$ sont unitaires, $\chi_{m_\alpha}=\pi_\alpha^r$.
		\item D'après la question $2$, $\pi_{\sqrt{p_ip_j}}=X^2-p_ip_j$. Et d'après la question $3$, $\chi_{m_{\sqrt{p_ip_j}}}=(X^2-p_ip_j)^r$ pour un certain $r\in \N^*$. Ce polynôme est pair de degré $2r$, le coefficient en $X^{2r-1}$ est donc nul. Ainsi $\Tr(m_{\sqrt{p_ip_j}})=0$.\\
		Ensuite, $\pi_{p_i}=X-p_i$. Donc $\chi_{m_{p_i}}=(X-p_i)^{r_i}=\displaystyle\sum_{k=0}^{r_i}\binom{r_i}{k}(-p_i)^{r_i-k}X^k$ pour un certain $r_i\in \N^*$. On en déduit que $\Tr(m_{p_i})=-r_ip_i\ne 0$.\\
		En outre $(\Tr(m_{\sqrt{p_ip_j}}))_{1\leq i,j\leq n}=\diag(-r_kp_k,\ k\in \crblanc{1}{n})$ est inversible.
		\item On pose $\fonction{\Phi}{\Q}{\Q^n}{y}{(\Tr(m_{y\sqrt p_1}),\dots,\Tr(m_{y\sqrt p_n}))}$\\
		$\Phi$ est $\Q$-linéaire comme composée d'applications linéaires. Si la famille $(\sqrt p_1,\dots,\sqrt p_n)$ était $\Q$-liée alors la famille $(\Phi(\sqrt p_1),\dots,\Phi(\sqrt p_n))$ le serait aussi. Ceci n'est pas possible puisque la matrice dont cette famille représente les lignes est inversible. Donc par contraposée, la famille $(\sqrt p_1,\dots,,\sqrt p_n)$ est $\Q$-libre.
	\end{enumerate}
	
	\subsection{Corps des nombres algébriques \etoile{4}}
	\label{Corps des nombres algébriques corrigé}
	\textcolor{blue}{\hyperref[Corps des nombres algébriques]{[Enoncé]}}\\
	Fixons $(x,y)\in \mathbb A^2$ avec $x\ne 0$. On note $P,Q\in \Q[X]$ tels que $P(x)=Q(y)=0$. Quitte à les diviser par leurs coefficients dominants on peut supposer que $P$ et $Q$ sont unitaires.\\
	Enfin on écrit $P(X)=\displaystyle\sum_{k=0}^na_kX^k=\prod_{k=1}^n(X-\alpha_k)$ et $Q(X)=\displaystyle\sum_{k=0}^mb_kX^k=\prod_{k=1}^m(X-\beta_k)$.
	\begin{itemize}
		\item $\mathbb A\subset \C$. $0$ est racine du polynôme $X$ et $1$ est racine du polynôme $X-1$ donc $0,1\in \mathbb A$.
		\item $S(X):=P(-X)=\displaystyle\sum_{k=0}^n(-1)^ka_kX^k\in \Q[X]$ et $S(-x)=P(x)=0$. Donc $-x\in \mathbb A$.
		\item $R(X):=\displaystyle X^nP\left(\frac{1}{X}\right)=\sum_{k=0}^na_kX^{n-k}=\sum_{k=0}^na_{n-k}X^k\in \Q[X]$ et $\displaystyle R\left(\frac{1}{x}\right)=\frac{P(x)}{x^n}=0$. Donc $\displaystyle\frac{1}{x}\in \mathbb A$.
		\item On va construire $A$ par blocs. Fixons $i\in \crblanc{0}{n-1}$ et Notons $V_i=\begin{pmatrix}y^i\\y^ix\\\vdots\\y^ix^{n-1}\end{pmatrix}=y^iV_0$.\\
		On sait que la matrice compagnon de $P$ : $C_P=
		\begin{pmatrix}
			0&\dots&\dots&0&-a_0\\
			1&\ddots&\ &\vdots&-a_1\\
			0&\ddots&\ddots&\vdots&\vdots\\
			\vdots&\ddots&\ddots&0&-a_{n-2}\\
			0&\dots&0&1&-a_{n-1}
		\end{pmatrix}$ est à coefficients rationnels et admet $x$ comme valeur propre. On remarque que $y^iV_0$ vecteur propre associé.\\
		La matrice $C_P$ ne dépend pas de $i$ on va donc construire $A$ comme étant la matrice diagonale par blocs $$A=
		\left(\begin{array}{c|c|c|c}
			C_P&0&\cdots&0\\
			\hline
			0&\ddots&\ddots&\vdots\\
			\hline
			\vdots&\ddots&\ddots&0\\
			\hline
			0&\cdots&0&C_P
		\end{array}\right)$$
		On calcule $$AV=A
		\left(\begin{array}{c}
			V_0\\
			\hline V_1\\
			\hline\vdots\\
			\hline V_{n-1}
		\end{array}\right)=
		\left(\begin{array}{c}
			C_PV_0\\
			\hline C_PV_1\\
			\hline\vdots\\
			\hline C_PV_{n-1}
		\end{array}\right)=
		\left(\begin{array}{c}
			xV_0\\
			\hline xV_1\\
			\hline\vdots\\
			\hline xV_{n-1}
		\end{array}\right)=xV$$
		Donc $x$ est racine de $\chi_A$. Or $\chi_A\in \Q[X]$ car ses coefficients sont polynomiaux en ceux de $A$ qui sont rationnels et car $\Q$ est un anneau.\\
		Pour $B$ on va se ramener au cas précédent. On remarque qu'en permutant les coordonnées de $V$ on peut se ramener à $W=\begin{pmatrix}1\\y\\y^2\\\vdots\\y^{m-1}\\x\\yx\\y^2x\\\vdots\\y^{m-1}x\\\vdots\\\vdots\\y^{m-1}x^{n-1}\end{pmatrix}$. Formellement on pose la matrice de permutation $P=(\delta_{i,\sigma(j)})_{0\leq i,j\leq nm-1}$ avec $\sigma$ la permutation de $\crblanc{0}{nm-1}$ définie par $\forall k\in \crblanc{0}{nm-1},\ \sigma(k)=j+im$ avec $k=i+jn$ la division euclidienne de $k$ par $n$. $P$ est construite de sorte à vérifier $W=PV$.\\
		On a alors $B'W=yB$ avec $B'=\left(\begin{array}{c|c|c|c}
			C_Q&0&\cdots&0\\
			\hline
			0&\ddots&\ddots&\vdots\\
			\hline
			\vdots&\ddots&\ddots&0\\
			\hline
			0&\cdots&0&C_Q
		\end{array}\right)$. D'où en posant $B=P^{-1}B'P$ :\\
		$BV=P^{-1}B'PV=P^{-1}B'W=P^{-1}(yW)=yP^{-1}(PV)=yV$.\\
		$B$ est bien à coefficients rationnels car, $B'$ l'est et $P$ l'est ce qui impose que son inverse aussi (cela découle de la formule $P^{-1}=\displaystyle\frac{1}{\det P}\Com(P)^\top$ en utilisant la structure de corps de $\Q$).\\
		Finalement il reste à remarquer que $A+B,AB\in \M_{nm}(\Q)$ et $(A+B)V=(x+y)V$ et $ABV=xyV$. $x+y$ est racine de $\chi_{A+B}\in \Q[X]$ et $xy$ est racine de $\chi_{AB}\in \Q[X]$.\\\\
		On conclut que $\mathbb A$ est un sous-corps de $\C$.\\\\
		\textit{Remarque : Par la même démonstration on obtient que l'ensemble des entiers algébriques i.e l'ensemble des nombres complexes racines d'un polynôme unitaire à coefficients entiers non nul, est un sous-anneau de $\C$. Ce n'est cependant pas un corps car par exemple $\frac{1}{2}$ n'est pas un entier algébrique}
	\end{itemize}
	
	\subsection{Théorème de Kronecker \etoile{4}}
	\label{Théorème de Kronecker corrigé}
	\textcolor{blue}{\hyperref[Théorème de Kronecker]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On montre classiquement (cf. \ref{Matrice compagnon (1)}) que pour tout polynôme unitaire $Q=X^n+\displaystyle\sum_{k=0}^{n-1}a_kX^k$ on a $Q=\chi_{C_Q}$ avec $$C_Q=
		\begin{pmatrix}
			0&\dots&\dots&0&-a_0\\
			1&\ddots&\ &\vdots&-a_1\\
			0&\ddots&\ddots&\vdots&\vdots\\
			\vdots&\ddots&\ddots&0&-a_{n-2}\\
			0&\dots&0&1&-a_{n-1}
		\end{pmatrix}$$
		$C_P$ est trigonalisable dans $\M_n(\C)$ donc $\text{Sp}(C_P^k)=\{\lambda^k,\ \lambda\in \text{Sp}(C_p)\}=\{\lambda_1^k,\dots,\lambda_d^k\}$. $\lambda_1^k,\dots,\lambda_d^k$ sont donc les racines de $Q_k=\chi_{C_P^k}$ qui est unitaire.\\
		De plus, $C_P$ est à coefficients entiers donc $C_P^k$ est aussi à coefficients entiers. Par conséquent son polynôme caractéristique est aussi à coefficients entiers puisque ses coefficients sont polynomiaux en ceux de $C_P^k$ et puisque $\Z$ est un anneau.
		\item Notons $q_0,\dots,q_d$ les coefficients de $Q_k$. Par les relations coefficients racines on a :
		$$\forall n\in \crblanc{0}{d},\ q_{d-n}=(-1)^n\sum_{\substack{I\subset\crblanc{1}{d}\\\Card(I)=n}}\prod_{i\in I}\lambda_i^k$$
		Donc par inégalité triangulaire $\forall n\in \crblanc{0}{d},\ |q_{d-n}|\leq \displaystyle\sum_{\substack{I\subset\crblanc{1}{d}\\\Card(I)=n}}\prod_{i\in I}|\lambda_i|^k=\sum_{\substack{I\subset\crblanc{1}{d}\\\Card(I)=n}}1=\binom{d}{n}\leq d!$.\\
		\item Les coefficients de $Q_k$ étant des entiers bornées, il ne peuvent prendre qu'un nombre fini de valeurs. D'après le principe des tiroirs, il existe une infinité d'entiers naturels non nuls $k\ne k'$ pour lesquels $Q_k=Q_{k'}$. De plus, les racines de $Q_k$ étant en nombre fini quel que soit $k\in \N^*$ on en déduit finalement l'existence, quel que soit $i\in \crblanc{0}{d}$, de deux entiers naturel non nuls $k_i\ne k'_i$ pour lesquels $\lambda_i^{k_i}=\lambda_i^{k'_i}$.\\
		Enfin comme $\lambda_i\ne 0$, cela implique $\lambda_i\in \U_{|k_i-k'_i|}$.
	\end{enumerate}
	
	\subsection{Irrationalité de e (1)}
	\label{Irrationalité de e (1) corrigé}
	\textcolor{blue}{\hyperref[Irrationalité de e (1)]{[Enoncé]}}\\
	\subsection{Irrationalité de e (2)}
	\label{Irrationalité de e (2) corrigé}
	\textcolor{blue}{\hyperref[Irrationalité de e (2)]{[Enoncé]}}\\
	
	\subsection{Irrationalité de $\pi$}
	\label{Irrationalité de pi corrigé}
	\textcolor{blue}{\hyperref[Irrationalité de pi]{[Enoncé]}}\\
	
	\subsection{Critère de transcendance de Liouville \etoile{4}}
	\label{Critère de transcendance de Liouville corrigé}
	\textcolor{blue}{\hyperref[Critère de transcendance de Liouville]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On sait qu'il existe $P\in \Q[X]$ non nul annulant $\alpha$. Quitte à multiplier $P$ par le ppcm des dénominateurs de ses coefficients, on peut supposer $P\in \Z[X]$. Notons $P=\displaystyle\sum_{k=0}^da_kX^k$ où $d=\deg(P)$. $P$ n'est pas nul et il s'annule donc il ne peut pas être constant, ainsi $d\geq 1$. Fixons $n\in \N$.\\
		$\displaystyle P\left(\frac{p_n}{q_n}\right)=\sum_{k=0}^da_k\cdot\frac{p_n^k}{q_n^k}=\frac{1}{q_n^d}\sum_{k=0}^da_kp_n^kq_n^{n-k}$.\\
		Ainsi $\displaystyle q_n^dP\left(\frac{p_n}{q_n}\right)=\sum_{k=0}^da_kp_n^kq_n^{n-k}\in \Z$.\\
		De plus, si l'on considère $\varepsilon=\min\{|\alpha-z|\ |\ P(z)=0,z\ne \alpha\}$ qui est strictement positif, comme $\displaystyle\frac{p_n}{q_n}\unfty{\longrightarrow}\alpha$ on sait que $\exists N\in \N,\ \forall n\geq N,\ \left|\alpha-\displaystyle\frac{p_n}{q_n}\right|<\varepsilon$.\\
		Ainsi $\forall n\geq N,\ P\left(\displaystyle\frac{p_n}{q_n}\right)\ne 0$. Et donc si $n\geq N$, $q_n^dP\left(\displaystyle\frac{p_n}{q_n}\right)$ est un entier non nul, on peut donc affirmer que $\left|q_n^dP\left(\displaystyle\frac{p_n}{q_n}\right)\right|\geq 1$ i.e $\displaystyle \frac{1}{q_n^d}\leq\left|P\left(\frac{p_n}{q_n}\right)\right|$.\\\\
		Ensuite, fixons $n\geq N$. La fonction polynomiale associée à $P$ est $\mathcal C^1$ sur le segment $[\alpha-\varepsilon,\alpha+\varepsilon]$ auquel $\alpha$ et $\displaystyle\frac{p_n}{q_n}$ appartiennent, donc d'après l'inégalité de Taylor-Lagrange :\\\\
		$\displaystyle\left|P\left(\frac{p_n}{q_n}\right)\right|=\left|P\left(\frac{p_n}{q_n}\right)-P(\alpha)\right|\leq M\left|\alpha-\frac{p_n}{q_n}\right|$ avec $M=\sup\limits_{x\in[\alpha-\varepsilon,\alpha+\varepsilon]}|P'(x)|$.\\
		Finalement, $\forall n\geq N,\ \displaystyle\frac{1}{q_n^d}\leq M\left|\alpha-\frac{p_n}{q_n}\right|$ d'où $\displaystyle\frac{1}{q_n^d}\unfty{=}\bigO{\alpha-\frac{p_n}{q_n}}$.\\\\
		\textit{Remarque : ce critère traduit le fait que les nombres algébriques sont en quelque sorte "mal approximable" par les rationnels; dans le sens où, pour approximer un nombre algébrique $\alpha$ par des rationnels $\displaystyle\frac{p_n}{q_n}$, on va être obligé de prendre de grands dénominateurs pour nos rationnels de sorte qu'ils soient de l'ordre de la racine $d$-ième de $\left|\alpha-\displaystyle\frac{p_n}{q_n}\right|$.}
		\item Comme la série en question converge très rapidement on va l'approcher par ses sommes partielles. Posons pour $n\in \N^*,\ S_n=\displaystyle\sum_{k=1}^n\frac{1}{10^{k!}}$.\\
		$(S_n)_{n\in \N^*}$ est une suite de rationnels qui converge vers $S=\displaystyle\sum_{n=1}^{+\infty}\frac{1}{10^{n!}}$.\\
		$\forall n\in \N^*,\ S_n=\displaystyle\frac{\displaystyle\sum_{k=1}^n10^{n!-k!}}{10^{n!}}$.\\
		On cherche à montrer qu'il n'existe pas d'entier naturel non nul $d$ et de constante $C\in \R^*_+$ tels que APCR,\\
		$\displaystyle\frac{1}{10^{dn!}}\leq C|S-S_n|=C\sum_{k=n+1}^{+\infty}\frac{1}{10^{k!}}$. Supposons par l'absurde que c'est le cas.\\
		Fixons $d\in \N^*$ et $C\in \R^*_+$. Fixons $n\in \N^*$. Si $a$ et $b$ sont deux entiers supérieurs ou égaux à $2$ alors $ab\geq a+b$ (il suffit d'écrire, en supposant sans perte de généralité que $a\leq b$, que $a-1\geq 1\geq a/b$) donc,\\
		$\displaystyle\sum_{k=n+1}^{+\infty}\frac{1}{10^{k!}}\leq\frac{1}{10^{(n+1)!}}\sum_{k=n+1}^{+\infty}\frac{1}{10^{k(k-1)\dots(n+2)}}\leq \frac{1}{10^{(n+1)!}}\sum_{k=n+1}^{+\infty}\frac{1}{10^{2^{k-n}}}\leq \frac{1}{10^{(n+1)!}}\sum_{k=1}^{+\infty}\frac{1}{10^k}=\frac{1}{10^{(n+1)!}}\cdot\frac{\displaystyle\frac{1}{10}}{1-\displaystyle\frac{1}{10}}=\frac{1}{9\times10^{(n+1)!}}$.\\
		Donc $\exists N\in \N^*,\ \forall n\geq N,\ 1\leq \displaystyle\frac{C}{9}\cdot 10^{dn!-(n+1)!}$\\
		Or pour tout $n\geq d,\ (n+1)!=(n+1)n!>dn!$. Ainsi $\displaystyle\frac{C}{9}\cdot 10^{dn!-(n+1)!}\unfty{\longrightarrow}0$.\\
		On obtient une absurdité donc $S$ ne vérifie pas le critère d'où $\displaystyle\sum_{n=1}^{+\infty}\frac{1}{10^{n!}}$ est un nombre transcendant.
	\end{enumerate}
	
	\newpage
\section{Correction Arithmétique}
	\subsection{Infinité des nombres premiers \etoile{2}}
	\label{Infinité des nombres premiers corrigé}
	\textcolor{blue}{\hyperref[Infinité des nombres premiers]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Supposons par l'absurde qu'il existe un nombre fini de nombres premiers. On note alors $\mathbb P=\{p_1,\dots,p_n\}$ l'ensemble des nombres premiers et on pose $N=1+\displaystyle\prod_{k=1}^np_k$.\\
		$\forall k\in \crblanc{1}{n},\ N\equiv 1\not\equiv 0[p_k]$. Donc $N$ est un nombre premier qui n'appartient pas à $\mathbb P$ ce qui est absurde.\\
		On en déduit qu'il existe une infinité de nombres premiers.
		\item Supposons par l'absurde qu'il existe un nombre fini de nombres premiers congrus à $3$ modulo $4$. On note alors $\mathbb P_4^3=\{p_1,\dots,p_n\}$ l'ensemble des nombres premiers congrus à $3$ modulo $4$ et on pose $N=2+\displaystyle\prod_{k=1}^np_k$ et $M=4+\displaystyle\prod_{k=1}^np_k$.\\
		Pour tout $k\in \crblanc{1}{n},\ N\equiv 2[p_k]$ et $M\equiv 4[p_k]$. Or $2\notin \mathbb P_4^3$ donc $2\not\equiv 0[p_k]$ et $4\not\equiv 0[p_k]$. Donc $N$ et $M$ sont des nombres premiers. De plus, si $n=2r$ est pair alors $N\equiv 2+\displaystyle\prod_{k=1}^n3\equiv 2+9^r\equiv 2+1^r\equiv 3[4]$, et si $n=2r+1$ est impair alors $M\equiv 4+3\times 9^r\equiv 3[4]$. Dans tous les cas on peut construire un nouvel élément de $\mathbb P_4^3$ ce qui est absurde.\\
		On en déduit qu'il existe une infinité de nombres premiers congrus à $3$ modulo $4$.
	\end{enumerate}
	
	\subsection{Version faible du théorème de progression arithmétique de Dirichlet}
	\label{Version faible du théorème de progression arithmétique de Dirichlet corrigé}
	\textcolor{blue}{\hyperref[Version, faible du théorème de progression arithmétique de Dirichlet]{[Enoncé]}}\\
	
	\subsection{Racine carré d'un nombre premier \etoile{2}}
	\label{Racine carré d'un nombre premier corrigé}
		\textcolor{blue}{\hyperref[Racine carré d'un nombre premier]{[Enoncé]}}\\
	Supposons par l'absurde que $\sqrt{p}\in \Q$. Alors il existe un couple d'entiers $(a,b)\in \Z\times \N^*$ premiers entre eux tel que $\sqrt{p}=\displaystyle\frac{a}{b}$.\\
	Il suit que $pb^2=a^2$. Intéressons nous à la valuation $p$-adique de $a^2$.\\
	D'une part, si on écrit $a=\displaystyle\prod_{q\text{ premier}}q^{v_q(a)}$ la décomposition en facteur premier de $a$ alors $a^2=\displaystyle\prod_{q\text{ premier}}q^{2v_q(a)}$ d'où $v_p(a^2)=2v_p(a)$ est paire.\\
	D'autre part, par le même raisonnement la valuation $p$-adique de $b^2$ est paire mais alors $v_p(a^2)=v_p(pb^2)=1+v_p(b^2)$ est impaire.\\
	Ceci est absurde donc $\sqrt{p}$ n'est pas rationnel.
	
	\subsection{Une suite périodique}
	\label{Une suite périodique corrigé}
	\textcolor{blue}{\hyperref[Une suite périodique]{[Enoncé]}}\\
	
	\subsection{Racines de l'unité}
	\label{Racines de l'unité corrigé}
	\textcolor{blue}{\hyperref[Racines de l'unité]{[Enoncé]}}\\
	Soit $x\in\U_{m\wedge n}$.\\
	On sait que $m\wedge n$ divise $n$ par conséquent $x\in\U_n$.\\
	De même, on a $x\in\U_m$.\\
	Donc $x\in\U_n\cap\U_m$.\\
	Soit $x\in\U_n\cap\U_m$.\\
	D'après le théorème de Bézout, il existe $u,v\in\Z$ tels que $un+vm=m\wedge n$, ainsi :
	\[x^{m\wedge n}=x^{un}\times x^{vm}=1\times 1=1\]
	Donc $x\in\U_{n\wedge m}$.\\
	D'où \[\U_n\cap\U_m=\U_{m\wedge n}\]
	
	\subsection{Plus petit nombre premier ne divisant pas un entier donné}
	\label{Plus petit nombre premier ne divisant pas un entier donné corrigé}
	\textcolor{blue}{\hyperref[Plus petit nombre premier ne divisant pas un entier donné]{[Enoncé]}}\\
	
	\subsection{Théorème de Kurshak}
	\label{Théorème de Kurshak corrigé}
	\textcolor{blue}{\hyperref[Théorème de Kurshak]{[Enoncé]}}\\
	
	\subsection{Valuation $p$-adique de $\displaystyle\binom{p^n}{k}$}
	\label{Valuation padique de binom(p^n)(k) corrigé}
	\textcolor{blue}{\hyperref[Valuation padique de binom(p^n)(k)]{[Enoncé]}}\\
	
	\subsection{Une équation dans $\N$}
	\label{Une équation dans N corrigé}
	\textcolor{blue}{\hyperref[Une équation dans N]{[Enoncé]}}\\
	
	\subsection{Triplets pythagoriciens}
	\label{Triplets pythagoriciens corrigé}
	\textcolor{blue}{\hyperref[Triplets pythagoriciens]{[Enoncé]}}\\
	
	\subsection{Théorème de Sophie Germain \etoile{3}}
	\label{Théorème de Sophie Germain corrigé}
	\textcolor{blue}{\hyperref[Théorème de Sophie Germain]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $x^p+y^p+z^p=0$ Donc en divisant par $(x\wedge y\wedge z)^p$ on obtient $\displaystyle\left(\frac{x}{x\wedge y\wedge z}\right)^p+\left(\frac{y}{x\wedge y\wedge z}\right)^p+\left(\frac{z}{x\wedge y\wedge z}\right)^p=0$ avec $\displaystyle\frac{x}{x\wedge y\wedge z},\frac{y}{x\wedge y\wedge z}$ et $\displaystyle\frac{z}{x\wedge y\wedge z}$ des entiers premiers entre eux dans leur ensemble.\\
		On peut donc supposer, quitte à diviser par leur pgcd, que $x\wedge y\wedge z=1$. Soit $k$ un diviseur premier commun à $x$ et $y$. Alors $k|x^p+y^p=-z^p$. Or comme $k$ est premier, d'après le lemme d'Euclide $k|z$ et donc $k|x\wedge y\wedge z=1$ ce qui est absurde car $k\geq 2$. On en déduit que $x$ et $y$ sont premiers entre eux. Par un raisonnement analogue, $x\wedge z=y\wedge z=1$.
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $(a,b,c)\in \Z^2$ tel que $ab=c^k$ et $a\wedge b=1$. Considérons un nombre premier $r$. On sait que la valuation $r$-adique de $c^k$ est $v_r(c^k)=kv_r(c)$. D'autre part, $v_r(ab)=v_r(a)+v_r(b)$. Or comme $a\wedge b=1$, $r$ divise au plus un et un seul des deux entiers $a$ et $b$. Ainsi, on a toujours $v_r(ab)=v_r(a)$ ou $v_r(ab)=v_r(b)$. Par conséquent, $v_r(a)$ et $v_r(b)$ sont tous deux des multiples de $k$. On en déduit que $a$ et $b$ sont des puissances $k$-ième.
			\item On remarque que si $p=2$ alors $x^2+y^2+z^2=0\implies x=y=z=0$. Donc $p\ne 2$ et donc est impair. Ensuite, $y^p+z^p=-x^p\iff (x+y)\displaystyle\sum_{k=0}^{p-1}(-z)^{p-1-k}y^k=(-x)^p$.\\
			Or si $r$ est un diviseur premier commun à $y+z$ et $\displaystyle\sum_{k=0}^{p-1}(-z)^{p-1-k}y^k$ alors $y\equiv-z[r]$ et donc $\displaystyle\sum_{k=0}^{p-1}(-z)^{p-1-k}y^k\equiv \sum_{k=0}^{p-1}y^{p-1}\equiv py^{p-1}[r]$.\\
			Comme $r$ est premier on en déduit que $r|p$ ou $r|y^{p-1}$ c'est à dire comme $p$ est premier, $r=p$ ou, comme $r$ est premier, $r|y$. Or si $r|y$ alors comme $r|y+z$, on obtient que $y|z=y+z-y$ ce qui est absurde car $y$ et $z$ sont premiers entre eux.\\
			Donc $r=p$ mais alors $-x^p=(y+z)\displaystyle\sum_{k=0}^{p-1}(-z)^{p-1-k}y^k\equiv 0[p]$. Donc $p|-x^p$ puis $p|x$ et $xyz\equiv 0[p]$ ce qui est absurde.\\
			Ainsi $y+z$ et $\displaystyle\sum_{k=0}^{p-1}(-z)^{p-1-k}y^k$ n'ont pas de diviseurs premiers commun : ils sont donc premiers entre eux. La question précédente assure alors l'existence de deux entiers $a$ et $\alpha$ tels que $y+z=a^p$ et $\displaystyle\sum_{k=0}^{p-1}(-z)^{p-1-k}y^k=\alpha^p$.\\
			En réitérant le même raisonnement en isolant $z$ puis $y$ dans $x^p+y^p+z^p=0$ on obtient de même qu'il existe deux entiers $b$ et $c$ tels que $x+y=c^p$ et $x+z=b^p$.
		\end{enumerate}
		\item Soit $m$ un entier qui n'est pas un multiple de $q=2p+1$. D'après le petit théorème de Fermat $m^{q-1}=m^{2p}\equiv 1[q]$. Ceci se traduit dans $\Z/q\Z$ :
		$$\overline m^{2p}-\overline 1=\overline 0$$
		ou encore :
		$$\left(\overline m^p-\overline 1\right)\left(m^p+\overline 1\right)=\overline 0$$
		Or $\Z/q\Z$ est un corps car $q$ est premier. Il est donc intègre d'où :
		$$\overline m^p=\overline 1 \text{ ou } \overline m^p=\overline{-1}$$
		Ainsi, $m^p\equiv\pm 1[q]$.\\
		D'après ce qui vient d'être démontré, dans $\Z/q\Z,\ (\overline x^p,\overline y^p,\overline z^p)\in \{\overline{-1},\overline 0,\overline 1\}^3$.\\
		Si aucun des trois n'est nul alors $x^p+y^p+z^p\not \equiv 0[p]$ et donc $x^p+y^p+z^p\ne 0$ ce qui est absurde. Ainsi $x^py^pz^p\equiv 0[q]$ c'est à dire $xyz\equiv 0[q]$.\\
		Comme $x,y,z$ sont deux à deux premiers entre eux, $q$ ne divise qu'un seul d'entre eux.
		\item $b^p+c^p-a^p=x+z+x+y-y-z=2x\equiv 0[q]$.\\
		$c^p=x+y\equiv y[q]$. De même, $z\equiv b^p[q]$.\\ D'après la question précédente, $y\equiv\pm 1[q]$ et $z\equiv\pm 1[q]$. Si $y\equiv z[q]$ alors $a^p\equiv \pm 2$ ce qui est absurde. Donc $y\equiv-z[q]$ c'est à dire $q|a^p$ puis $q|a$ car $q$ est premier.\\
		Enfin, $y\equiv-z[q]\implies \alpha^p\equiv\displaystyle\sum_{k=0}^{p-1}y^{p-1}=py^{p-1}[q]$.\\
		On sait que $y\equiv c^p\equiv\pm 1[q]$. Donc comme $p-1$ est pair, $\alpha^p\equiv py^{p-1}\equiv p[q]$. De plus, $\alpha^p\equiv\pm 1[q]$ donc $p\equiv\pm 1[q]$.\\
		C'est à dire qu'il existe un entier $k$ tel que $p=kq\pm 1=k(2p+1)\pm 1$.\\
		Ceci est faux pour $k$ négatif ou nul puis si $k\geq 1,\ p<kq\pm 1$.\\
		On aboutit à une contradiction par conséquent il n'existe pas de triplet $(x,y,z)\in \Z^3$ tel que $xyz\not\equiv 0[p]$ et $x^p+y^p+z^p=0$.
	\end{enumerate}
	
	\subsection{Une équation diophantienne}
	\label{Une équation diophantienne corrigé}
		\textcolor{blue}{\hyperref[Une équation diophantienne]{[Enoncé]}}\\
	
	\subsection{Calcul d'une somme}
	\label{Calcul d'une somme corrigé}
		\textcolor{blue}{\hyperref[Calcul d'une somme]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Notons $q$ le quotient de cette division euclidienne. $a-bq$ est le reste de cette division euclidienne.\\
		Ainsi, \[0\leq a-bq < b\]
		d'où \[q\leq \frac{a}{b}<q+1\]
		Puisque $q$ est un entier, on a \[q=\left\lfloor \frac{a}{b}\right\rfloor\]
		\item Puisque $a\wedge b=1$, on en déduit que $a$ est inversible par conséquent $\varphi$ est clairement bijective d'inverse \[\fonction{\varphi^{-1}}{\Z/b\Z}{\Z/b\Z}{\overline{k}}{\overline{a^{-1}k}}\]
		\item On note $r_n$ le reste de la division euclidienne de $n$ par $b$.\\
		D'après la première question, $r_n=n-b\left\lfloor\frac{n}{b}\right\rfloor$. Par conséquent : 
		\[\sum_{k=1}^{b-1}\left\lfloor\frac{ka}{b}\right\rfloor=\sum_{k=1}^{n}\frac{ka}{b}-\sum_{k=1}^{b-1}\frac{r_{ka}}{b}\]
		De plus, d'après la question précédente $\sum_{k=1}^{b-1}\frac{r_{ka}}{b}=\sum_{k=1}^{b-1}\frac{k}{b}$.\\
		Ainsi, \[\sum_{k=1}^{b-1}=(a-1)\sum_{k=1}^{b-1}\frac{k}{b}=(a-1)\frac{b(b-1)}{2b}=\frac{(a-1)(b-1)}{2}\]
	\end{enumerate}
	\subsection{Nombres de Mersenne}
	\label{Nombres de Mersenne corrigé}
		\textcolor{blue}{\hyperref[Nombres de Mersenne]{[Enoncé]}}\\
	
	\subsection{Un exercice pour les années impaires}
	\label{Un exercice pour les années impaires corrigé}
			\textcolor{blue}{\hyperref[Un exercice pour les années impaires]{[Enoncé]}}\\
	
	\subsection{Equation du second degré dans $\Z/n\Z$}
	\label{Equation du second degré dans Z/nZ corrigé}
		\textcolor{blue}{\hyperref[Equation du second degré dans Z/nZ]{[Enoncé]}}\\
	
	\subsection{Un problème de congruence}
	\label{Un problème de congruence corrigé}
		\textcolor{blue}{\hyperref[Un problème de congruence]{[Enoncé]}}\\
	
	\subsection{Un multiple de 2026 qui ne s'écrit qu'avec des $2$}
	\label{Un multiple de 2026 corrigé}
		\textcolor{blue}{\hyperref[Un multiple de 2026]{[Enoncé]}}\\
	
	\subsection{Somme des puissances $k$-ièmes dans $\Z/p\Z$}
	\label{Somme des puissances k iemes dans Z/pZ corrigé}
		\textcolor{blue}{\hyperref[Somme des puissances k iemes dans Z/pZ]{[Enoncé]}}\\
	
	\subsection{Théorème de Wilson \etoile{2}}
		\textcolor{blue}{\hyperref[Théorème de Wilson]{[Enoncé]}}\\
	\label{Théorème de Wilson corrigé}
	On travaille dans $\Z/p\Z$.\\
	Si $p$ n'est pas premier alors $\exists (m,n)\in \crblanc{2}{p-1}^2,\ p=mn$. Par conséquent $(p-1)!=mn\times\displaystyle\prod_{k\in \crblanc{1}{p-1}\backslash\{m,n\}}k=\overline 0\ne \overline{-1}$.\\
	Supposons que $p$ est premier, autrement dit que $\Z/p\Z$ est un corps.\\
	Alors par commutativité puis intégrité de $\Z/p\Z$, $\forall k\in \Z/p\Z,\ k^2=\overline 1\iff (k-\overline 1)(k+\overline 1)=0\iff k\in \{\overline{-1},\overline 1\}$.\\
	Ceci montre que les seuls éléments de $\Z/p\Z$ qui sont égaux à leur inverse sont $\overline{-1}$ et $\overline 1=\overline{p-1}$.\\
	Par conséquent $(p-1)!=\displaystyle\prod_{k=1}^{p-1}\overline k=-\prod_{k=2}^{p-2}\overline k=-\overline 1$ en rassemblant dans le produit, par commutativité, les facteurs deux à deux avec leur inverse.
	
	\subsection{Problème Putnam (2011) \etoile{5}}
	\label{Problème Putnam corrigé}
		\textcolor{blue}{\hyperref[Problème Putnam]{[Enoncé]}}\\
	Tout d'abord, comme $p$ ne divise évidemment pas $\displaystyle\sum_{k=0}^{p-1}k!0^k=1$ il suffit de montrer qu'il existe au moins $\displaystyle\frac{p+1}{2}-1=\frac{p-1}{2}$ valeurs de $n$ dans $\crblanc{1}{p-1}$ telles que $p$ ne divise pas $\displaystyle\sum_{k=0}^{p-1}k!n^k$.\\
	Cela revient à montrer qu'il existe au plus $\displaystyle\frac{p-1}{2}$ valeurs de $n$ dans $\crblanc{1}{p-1}$ telles que $p$ divise $\displaystyle\sum_{k=0}^{p-1}k!n^k$. Donnons nous alors $n\in (\Z/p\Z)^*$ tel que $\displaystyle\sum_{k=0}^{p-1}k!n^k=\overline 0$. On calcule :\\
	\begin{align*}
		\sum_{k=0}^{p-1}k!n^k&=\sum_{k=0}^{p-1}(p-1-k)!n^{p-1-k}\\
		&=\sum_{k=0}^{p-1}\overline{(p-1)!}\cdot\overline{(p-1)^{-1}}\dots\overline{(p-k+1)^{-1}}n^{p-1-k}\\
		&=\sum_{k=0}^{p-1}-\overline{(p-1)^{-1}}\dots\overline{(p-k)^{-1}}n^{p-1-k} &\text{d'après le théorème de Wilson (cf. \ref{Théorème de Wilson})}\\
		&=\sum_{k=0}^{p-1}-\overline{(-1)^{-1}}\dots\overline{(-k)^{-1}}n^{-k} &\text{d'après le petit théorème de Fermat}\\
		&=\sum_{k=0}^{p-1}\overline{k!}^{-1}(-n)^k &\text{Car }x\mapsto x^{-1}\text{ est un automorphisme de }(\Z/p\Z)^*\text{ (cf. \ref{Automorphisme d'inversion})}\\
		&=\sum_{k=0}^{p-1}\overline{k!}^{-1}n^k &\text{Car }\overline x\mapsto \overline{p-x}=\overline{-x}\text{ est un automorphisme de }(\Z/p\Z)^*\\
	\end{align*}
	On pose le polynôme $P(X)=\displaystyle\sum_{k=0}^{p-1}\overline{k!}^{-1}X^k$ de $\Z/p\Z$. On cherche à montrer que $P(X)$ a au plus $\displaystyle\frac{p-1}{2}$ racines. D'après l'indication on sait déjà qu'il en a au plus $p-1$. Une idée peut alors être d'essayer de montrer que toutes ses racines sont doubles.\\
	On va en fait montrer que les racines, dans $\Z/p\Z$, du polynôme $Q(X)=\displaystyle\sum_{k=0}^{p-1}\frac{X^k}{k!}-X+X^p$ de $\R[X]$ sont doubles.\\
	$P$ et $Q$ coïncident sur $\Z/p\Z$ grâce au petit théorème de Fermat. De plus, $0$ n'est pas racine de $Q$ dans $\Z/p\Z$ et, si $x$ n'est pas un multiple de $p$ alors $Q'(x)=\displaystyle\sum_{k=1}^{p-1}\frac{x^{k-1}}{(k-1)!}-1+px^{p-1}\equiv \sum_{k=0}^{p-1}(k!)^{-1}x^k-((p-1)!)^{-1}x^{p-1}-1\equiv \sum_{k=0}^{p-1}(k!)^{-1}x^k\equiv Q(x)[p]$.\\
	Ainsi, si $\overline n$ est racine de $P$ alors $n$ est racine de $Q$ et de $Q'$ dans $\Z/p\Z$. Toutes les racines du polynôme $Q$ dans $\Z/p\Z$ sont doubles donc celui-ci a au plus $\displaystyle\frac{p-1}{2}$ racines dans $\Z/p\Z$.\\
	Ainsi $P$ a au plus $\displaystyle\frac{p-1}{2}$ racines.
	
	\subsection{Critère d'Euler}
	\label{Critère d'Euler corrigé}
		\textcolor{blue}{\hyperref[Critère d'Euler]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $x,y\in \mathbb F_p^*$.\\
		\[x^2=y^2\Leftrightarrow (x+y)(x-y)=0\Leftrightarrow x=y \text{ ou } x=-y\]
		Comme $p\ne 2$, $y\ne -y$, ainsi tout élément de $\mathcal{C}$ admet exactement deux antécédent par l'application $x\mapsto x^2$.\\
		Ainsi puisque $\mathcal{C}$ est l'image de cette application et que $\Card(\mathbb F_p^*)=p-1$, on obtient : \[\Card(\mathcal{C})=\frac{p-1}{2}\]
		\item Puisque $\Card(\mathbb F_p^*)=p-1$, on a $\left(a^{\frac{p-1}{2}}\right)^2=1$.\\
		Ainsi par intégrité du corps $\mathbb F_p^*$, $a^{\frac{p-1}{2}}\in\{\pm\overline{1}\}$.
		\item On rappelle que les polynômes de Lagrange $L_i=\prod_{j\ne i}\frac{X-a_j}{a_i-a_j}$ forme une base de $\R[X]$.\\
		Par conséquent, on peut écrire \[P=\sum_{i=1}^{d}P(a_d)L_i\]
		Il existe donc des entiers $n_1,\dots,n_d$ tels que 
		\[\left(\prod_{1\leq i<j\leq n}a_j-a_i\right)P=\sum_{i=1}^{n}P(a_i)\prod_{j\ne i}(X-a_j)\]
		Ainsi pour tout $n\in\Z$, \[\left(\prod_{1\leq i<j\leq n}a_j-a_i\right)P(n)=\sum_{i=1}^{n}P(a_i)\prod_{j\ne i}(n-a_j)\]
		On sait que $p|P(a_i)$ pour tout $i\in\crblanc{1}{n}$, ainsi $p$ divise le membre de droite.\\
		Puisque que les $a_i$ sont distincts modulo $p$, $p$ ne divise pas $\left(\prod_{1\leq i<j\leq n}a_j-a_i\right)$ donc d'après le lemme d'Euclide $p|P(n)$.
		\item D'après le théorème de Lagrange, pour tout $a\in\mathcal{C}$, $a^{\frac{p-1}{2}}=\overline{1}$.
		Donc \[\mathcal{C}\subset\{a\in\mathbb F_p^*,a^{\frac{p-1}{2}}=\overline{1}\}\]
		On pose $P=X^{\frac{p-1}{2}}-\overline{1}\in \R_{\frac{p-1}{2}}[X]$.\\
		Supposons qu'il existe $a\in\mathbb F_p^*\setminus\mathcal{C}$ tel que $a^{\frac{p-1}{2}}=\overline{1}$.\\
		Ainsi, $P$ admet $\frac{p-1}{2}+1$ racines donc $p$ divise toutes les racines de $P$ ainsi d'après la question précédente pour tout $n\in\Z$, $p|P(n)$ ce qui est absurde car $p\not\mid P(0)=-\overline{1}$.\\
		Ainsi, $\mathcal{C}=\{a\in\mathbb F_p^*,a^{\frac{p-1}{2}}=\overline{1}\}$.
	\end{enumerate}
	
	\subsection{Indicatrice d'Euler}
	\label{Indicatrice d'Euler corrigé}
	\textcolor{blue}{\hyperref[Indicatrice d'Euler]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On montre que $\Z/n\Z$ possède un unique sous-groupe d'ordre $d$ et qu'il est cyclique (cf.\ref{Sous-groupe d'un groupe cyclique}). Notons $H$ ce sous-groupe et $x$ un générateur.\\
		Si $y\in \Z/n\Z$ est d'ordre $d$ alors $\langle y\rangle$ est un sous-groupe de $\Z/n\Z$ d'ordre $d$. Donc $\langle y\rangle=H$ et en particulier $y\in H$.\\
		On montre ensuite que pour $k\in \Z$ l'ordre de $kx$ est $\dfrac{d}{k\wedge d}$.\\
		Ainsi les éléments de $\Z/n\Z$ d'ordre $d$ sont les $kx$ avec $1\leq k\leq d$ et $k\wedge d=1$, il y en a donc $\varphi(d)$.
		\item On partitionne $\Z/n\Z$ par les ensembles suivants :
		\[
		H_d=\{x\in\Z/n\Z,\ o(x)=d\} \text{ pour }d\mid n
		\]
		D'après la question précédente, $|H_d|=\varphi(d)$.
		Ces ensembles sont bien disjoints deux à deux et chaque élément de $\Z/n\Z$ appartient bien à un de ces ensembles d'après le théorème de Lagrange.\\
		Ainsi : \[\Z/n\Z=\bigsqcup_{d\mid n}H_d\]
		Donc par passage au cardinal : \[n=\sum_{d\mid n}\varphi(d)\]
		\item On remarque ainsi que \[\varphi(n)=n-\sum_{d\mid n\T{ et } d\ne n} \varphi(d)\]
		La traduction directe de cela peut s'écrire :
		\begin{lstlisting}
			def ind_euler(n):
			 s=0
			 for i in range(1,n) : 
			  if n%i==0:
			   s+=ind_euler(i)
			 return n-s
		\end{lstlisting}
		On peut gagner du temps de calcul en gardant en mémoire les valeurs calculées (puisque les diviseurs d'un diviseur sont des diviseurs) :
		\begin{lstlisting}
			def auxiliaire(n,S) :
			 # S est la liste des images
			 # de l'indicatrice d'Euler
			 s=0
			 for i in range(1,n) :
			  if n%i==0 :
			   if S[i]!=0 :
			    s+=S[i]
			   else :
			    S[i]=auxiliaire(i,S)[i]
			    s+=S[i]
			 S[n]=n-s
			 return S
			def ind_euler_bis(n) :
			return auxiliaire(n,[0]*(n+1))[-1]
		\end{lstlisting}
		Voici une comparaison du temps de calcul (en seconde) entre les deux pour les calculs des 10000 premières valeurs :
		\begin{center}
			\includegraphics[scale=0.8]{Comparaison indicatrice d'Euler.PNG}
		\end{center}
		Et avec la formule $\varphi(n)=n\displaystyle\prod_{\substack{p|n\\p\T{ premier}}}\left(1-\frac{1}{p}\right)$, en implémentant le crible d'Eratostène sur les 5000 premières valeurs :
		\begin{center}
			\includegraphics[scale=0.8]{Comparaison Crible d'Eratostène}
		\end{center}
	\end{enumerate}
	
	\subsection{Une minoration de l'indicatrice d'Euler}
	\label{Une minoration de l'indicatrice d'Euler corrigé}
	\textcolor{blue}{\hyperref[Une minoration de l'indicatrice d'Euler]{[Enoncé]}}\\
	\begin{enumerate}
		\item 
		Sans perte de généralités, supposons que les $n_i$ sont rangées dans l'ordre décroissant. Ainsi puisque les $(n_i)$ sont des entiers ont en déduit les inégalités suivantes: $$\forall i\in\crblanc{2}{k}\, ,\quad n_i\geq n_{1}+i-1$$
		\\Par conséquent : $$\displaystyle\prod_{i=1}^k\left(1-\frac{1}{n_i}\right)\geq\prod_{i=1}^k\frac{n_{1}+i-2}{n_1+i-1}=\frac{n_1-1}{n_1+k-1}.$$
		L'étude de la fonction: $x\mapsto\dfrac{x-1}{x+k-1}$ sur $[2,+\infty[$ donne : $$\prod_{i=1}^k\left(1-\frac{1}{n_i}\right)\geq\frac{1}{k+1}.$$
		\item Soit $n\in\N^*$.
		\\ On décompose $n$ en facteurs premiers : $\displaystyle n=\prod_{i=1}^kp_i^{\alpha_i}$.
		\\D'après le cours : $\displaystyle\varphi(n)=n\prod_{i=1}^k\left(1-\frac{1}{p_i}\right)$.
		\\Ainsi d'après la question précédente, $\displaystyle\varphi(n)\geq\frac{n}{k+1}$.
		\\De plus, $\displaystyle n\geq\prod_{i=1}^kp_i\geq2^k$ donc $\displaystyle 2n\geq2^{k+1}$ et donc $\displaystyle\ln(2n)\geq(k+1)\ln(2)$.
		\\Par conséquent, $\displaystyle \varphi(n)\geq \frac{n\ln(2)}{\ln(2n)}$
	\end{enumerate}
	
	\subsection{Formule d'inversion de Möbius}
	\label{Formule d'inversion de Möbius corrigé}
	\textcolor{blue}{\hyperref[Formule d'inversion de Möbius]{[Enoncé]}}\\
	\begin{enumerate}
		\item $1$ est le produit de $0$ nombre premiers distincts, donc : $\mu(1)=(-1)^0=1\ne 0$.
		Soit $(m,n)\in(\N^*)^2$ tel que $m\wedge n=1$.
		Si $m$ ou $n$ est divisible par le carré d'un nombre premier, alors $mn$ l'est également. Donc :\[\mu(mn)=0=\mu(m)\mu(n)\]
		Sinon, si $m$ et $n$ n'est pas divisible par le carré d'un nombre premier, alors $mn$ n'est pas divisible par le carré d'un nombre premier car $m$ et $n$ sont premiers entre eux i.e. $m$ $n$ n'ont pas de facteurs premiers communs.\\
		En notant $k$ (resp. $l$) le nombre de facteur premiers distincts intervenant dans la décomposition en facteurs premiers de $m$ (resp. $n$), on en déduit que $mn$ est le produit de $k+l$ nombres premiers.
		Ainsi : \[\mu(mn)=(-1)^{k+l}=(-1)^k(-1)^l=\mu(m)\mu(n)\]
		Donc $\mu$ est multiplicative.
		\item Il s'agit de montrer que : \[\sum_{d|n}\mu(d)=\begin{cases}
			1&\mbox{si }n=1\\
			0&\mbox{si }n\geq 2
		\end{cases}\]
		Soient $p_1,\dots,p_r$ les diviseurs premiers deux à deux distincts de l'entier $n$.
		Par définition de $\mu$, les seuls diviseurs positifs qui de $n$ qui vont intervenir dans la somme sont ceux de la forme :\[p_I=p_{i_1}\dots p_{i_r}\]
		où $I=\{i_1,\dots, i_r\}$ est une partie de $\crblanc{1}{r}$.
		On a alors :\[\sum_{d|n}\mu(d)=\sum_{I}(-1)^{|I|}\]
		Or pour tout $s\in\crblanc{0}{r}$, il y a $\displaystyle\binom{r}{s}$ parties $I$ de $\crblanc{1}{r}$ à $s$ éléments.\\
		Ainsi, \[\sum_{d|n}\mu(d)=\sum_{s=0}^{r}\binom{r}{s}(-1)^s=(1-1)^r\]
		Finalement, on en déduit bien que :
		\[\sum_{d|n}\mu(d)=\begin{cases}
			1 &\mbox{si }n=1\\
			0&\mbox{si }n\geq 2
		\end{cases}\]
		\item Soit $n\in\N^*$ 
		Remarquons l'équivalence suivante :
		\[\forall d,d'\geq 1, \; d|n \text{ et } d'|\frac{n}{d} \Longleftrightarrow d'|n \text{ et } d|\frac{n}{d}\]
		\begin{align*}
			\sum_{d|n}\mu(d)F\!\left(\frac{n}{d}\right)
			&= \sum_{d|n}\mu(d)\sum_{d'\mid\frac{n}{d}}f(d')\\
			&= \sum_{d|n}\sum_{d'\mid\frac{n}{d}}\mu(d)f(d')\\
			&= \sum_{d'\mid n}\sum_{d\mid\frac{n}{d'}}\mu(d)f(d') 
			&& \text{(d'après l'équivalence précédente)}\\
			&= \sum_{d'\mid n} f(d') \sum_{d\mid\frac{n}{d'}}\mu(d)
		\end{align*}
		D'après la question 2, \[\sum_{d|\frac{n}{d'}}\mu(d)\ne 0 \Longleftrightarrow d'=n\]
		Par conséquent, on en déduit bien l'égalité souhaité : \[f(n)=\sum_{d|n}\mu(d)F(\frac{n}{d})\]
		\item D'après l'exercice \ref{Indicatrice d'Euler}, on a : \[I(n)=n=\sum_{d|n}\varphi(d)\]
		Ainsi d'après la formule d'inversion de Möbius, on a :
		\[\varphi(n)=\sum_{d|n}\mu(d)\frac{n}{d}\]
		i.e \[\varphi=\mu*I\]
	\end{enumerate}
	
	
	\subsection{Probabilité que deux entiers soient premiers entre eux}
	\label{Probabilité que deux entiers soient premiers entre eux corrigé}
	\textcolor{blue}{\hyperref[Probabilité que deux entiers soient premiers entre eux]{[Enoncé]}}\\
	
	\subsection{Limite d'une fonction arithmétique multiplicative}
	\label{Limite d'une fonction arithmétique multiplicative corrigé}
	\textcolor{blue}{\hyperref[Limite d'une fonction arithmétique multiplicative]{[Enoncé]}}\\
	
	\subsection{Fonctions arithmétiques réelles additives}
	\label{Fonctions arithmétiques réelles additives corrigé}
	\textcolor{blue}{\hyperref[Fonctions arithmétiques réelles addtives]{[Enoncé]}}\\
	
	\subsection{Une majoration de la somme des diviseurs d'un entier}
	\label{Une majoration de la somme des diviseurs d'un entier corrigé}
	\textcolor{blue}{\hyperref[Une majorattion de la somme des diviseurs d'un entier]{[Enoncé]}}\\
	
	\subsection{Entiers algébriques}
	\label{Entiers algébriques corrigé}
	\textcolor{blue}{\hyperref[Entiers algébriques]{[Enoncé]}}\\
	
	\subsection{Majoration de la primorielle}
	\label{Majoration de la primorielle corrigé}
	\textcolor{blue}{\hyperref[Majoration de la primorielle]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item \begin{itemize}
			\item Pour $n=1$ $\displaystyle\prod_{\underset{p \text{premier}}{p\leq 1}}p=1\leq 4^1$.
			\item Pour $n=2$, $\displaystyle\prod_{\underset{p \text{premier}}{p\leq 2}}p=2\leq 4^2=16$.
			\item Pour $n=3$, $\displaystyle\prod_{\underset{p \text{premier}}{p\leq 3}}p=6\leq 4^3=64$.
		\end{itemize}
		\item Soit $n\geq 4$ un entier pair.\\
		Puisque $n$ est pair, il ne peut être premier, ainsi : 
		\[\prod_{\underset{p \text{premier}}{p\leq n}}p=\prod_{\underset{p \text{premier}}{p\leq n-1}}p\]
		Ainsi, d'après l'hypothèse de récurrence, le résultat est vrai.*
		\item Soit \(p\) un nombre premier tel que \(m+1<p\le 2m+1\). Alors \(p\) apparaît dans la décomposition en facteurs premiers de \((2m+1)!\) (au moins une fois), tandis que ni \(m!\) ni \((m+1)!\) ne contiennent \(p\) (car tous leurs facteurs sont \(\le m+1<p\)). Par conséquent la puissance de \(p\) dans le numérateur est strictement supérieure à sa puissance dans le dénominateur, et donc \(p\) divise
		\(\binom{2m+1}{m}\).
		
		Ainsi le produit des nombres premiers situés dans l'intervalle \(m+1<p\le 2m+1\) divise \(\binom{2m+1}{m}\). En particulier
		\[
		\prod_{\substack{p\ \text{premier}\\ m+1<p\le 2m+1}} p \ \Bigm|\ \binom{2m+1}{m}
		\quad\Longrightarrow\quad
		\prod_{\substack{p\ \text{premier}\\ m+1<p\le 2m+1}} p \le \binom{2m+1}{m}.
		\]
		Les coefficients binomiaux de l'expansion de \((1+1)^{2m+1}=2^{2m+1}\) satisfont, par symétrie,
		\[
		\binom{2m+1}{m}=\binom{2m+1}{m+1},
		\]
		donc
		\[
		2\binom{2m+1}{m}=\binom{2m+1}{m}+\binom{2m+1}{m+1}\le\sum_{k=0}^{2m+1}\binom{2m+1}{k}=2^{2m+1}.
		\]
		D'où
		\[
		\binom{2m+1}{m}\le 2^{2m}=4^{m}.
		\]
		
		En combinant les deux observations on obtient
		\[
		\prod_{\substack{p\ \text{premier}\\ m+1<p\le 2m+1}} p \le \binom{2m+1}{m} \le 4^{m}.
		\]
		\item 
		Par hypothèse de récurrence (valable pour tous les entiers strictement inférieurs à \(n\)), on a en particulier
		\[
		\prod_{\substack{p\ \text{premier}\\ p\le m}} p \le 4^{m}.
		\]
		Donc le produit de tous les nombres premiers \(\le n=2m+1\) se factorise en
		\[
		\prod_{\substack{p\ \text{premier}\\ p\le 2m+1}} p
		=
		\left(\prod_{p\le m} p\right)\cdot
		\left(\prod_{\substack{p\ \text{premier}\\ m+1<p\le 2m+1}} p\right)
		\le 4^{m}\cdot 4^{m}=4^{2m}=4^{n-1}.
		\]
		En particulier on obtient bien la majoration cherchée
		\[
		\prod_{\substack{p\ \text{premier}\\ p\le n}} p \le 4^{n}.
		\]
	\end{enumerate}
	
	\subsection{Théorèmes de Mertens}
	\label{Théorèmes de Mertens corrigé}
	\textcolor{blue}{\hyperref[Théorème de Mertens]{[Enoncé]}}\\
	
	\subsection{Théorèmes de Tchebychev}
	\label{Théorèmes de Tchebychev corrigé}
	\textcolor{blue}{\hyperref[Théorèmes de Tchebychev]{[Enoncé]}}\\
	
	
	\newpage
\section{Correction Dénombrement}
	\subsection{Identité de Vandermonde}
	\label{Identité de Vandermonde corrigé}
	\textcolor{blue}{\hyperref[Identité de Vandermonde]{[Enoncé]}}\\
	
	\subsection{Nombre de Fibonacci}
	\label{Nombre de Fibonacci corrigé}
	\textcolor{blue}{\hyperref[Nombre de Fibonacci]{[Enoncé]}}\\

	\subsection{Matrices orthogonales à coefficients entiers}
	\label{Matrices orthogonales à coefficients entiers corrigé}
	\textcolor{blue}{\hyperref[Matrices orthogonales à coefficients entiers]{[Enoncé]}}\\
	
	\subsection{Dérangement}
	\label{Dérangement corrigé}
	\textcolor{blue}{\hyperref[Dérangement]{[Enoncé]}}\\
	
	\subsection{Dérangement partiel}
	\label{Dérangement partiel corrigé}
	\textcolor{blue}{\hyperref[Dérangement partiel]{[Enoncé]}}\\
	
	\subsection{Nombres de Bell}
	\label{Nombres de Bell corrigé}
	\textcolor{blue}{\hyperref[Nombres de Bell]{[Enoncé]}}\\
	
	\subsection{Nombres de Catalan}
	\label{Nombres de Catalan corrigé}
	\textcolor{blue}{\hyperref[Nombres de Catalan]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Un mot bien parenthésé de longueur $2(n+1)$ est de la forme $(m)m'$ où $m$ est un mot de longueur $k$ et $m'$ est un mot de longueur $2(n-k)$ avec $k\in\crblanc{0}{n}$.\\
		Ainsi pour $k\in\crblanc{0}{n}$, on a $C_kC_{n-k}$ choix possibles.\\
		On en déduit ainsi que : \[C_{n+1}=\sum_{k=0}^{n}C_kC_{n-k}\]
		\item Si on compte l'ensemble des mot parenthésé (sans compter si chaque parenthèse ouvrante est associé à une parenthèse fermée), il en existe $2^{2n}$.\\
		Puisque l'ensemble des mots bien parenthésés est un sous-ensemble des mots parenthésés, on a \[C_n\leq 2^{2n}\]
		On a par conséquent que le rayon de la série $\displaystyle \sum_{n\in\N}C_nx^n$ est supérieur à $\displaystyle \frac{1}{4}$.\\
		\item Soit $x\in\left]-\frac{1}{4},\frac{1}{4}\right[$.\\
		\[(F(x))^2=\sum_{n=0}^{+\infty}\sum_{k=0}^{n}C_kC_{n-k}x^n=\sum_{n=0}^{+\infty}C_{n+1}x^n\]
		D'où en changeant d'indice, on a 
		\[1+x(F(x))^2=C_0+\sum_{n=0}^{+\infty}C_{k+1}x^{k+1}=F(x)\]
		\item Supposons que $f$ s'annule sur $\left]-\frac{1}{4},\frac{1}{4}\right[$.\\
		Soit $x\in\left]-\frac{1}{4},\frac{1}{4}\right[$ tel que $f(x)=0$.\\
		D'après la question précédente, $F(x)=1+\frac{F(x)}{2}$ donc $F(x)=2$.\\
		Cela implique que $x=\frac{1}{4}$ ce qui est absurbe.\\
		Donc $f$ ne s'annule pas $\left]-\frac{1}{4},\frac{1}{4}\right[$.
		\item Si $x=0$, d'après la question 3, $F(0)=1$.\\
		Sinon, toujours d'après la question 3, $F(x)$ est solution de l'équation $xX^2-X+1=0$ d'inconnue $X$.\\
		Les deux solutions de l'équation de degré 2 sont : 
		\[X_1=\frac{1+\sqrt{1-4x}}{2x} \quad \text{et}\quad X_2=\frac{1-\sqrt{1-4x}}{2x}\]
		Puisque $F$ est continue sur $\left]-\frac{1}{4},\frac{1}{4}\right[$, $f$ l'est également.
		Ainsi d'après la contraposée du théorème des valeurs intermédiaires, $f$ ne change pas de signe sur $\left]-\frac{1}{4},\frac{1}{4}\right[$ et puisque $f(0)=-1$, on a \[2xF(x)<1\]
		Or $2xX_1\geq 1$ donc \[F(x)=\begin{cases}
			\frac{1-\sqrt{1-4x}}{2x}&\mbox{si } x\ne 0\\
			1 &\mbox{sinon}
		\end{cases}\]
		\item On sait que pour tout $u\in]-1,1[$, \[\sqrt{1-u}=\sum_{n=0}^{+\infty}\frac{\prod_{i=0}^{n-1}(\frac{1}{2}-i)}{i!}(-u)^i=\sum_{n=0}^{+\infty}\frac{(-1)^i\left(\prod_{i=0}^{n-1}(2i-1)\right)\left(\prod_{i=0}^{n-1}2i\right)}{2^i\left(\prod_{i=0}^{n-1}2i\right)i!}(-u)^n\]
		Donc \[\sqrt{1-u}=1-\sum_{n=1}^{+\infty}\frac{(2n-2)!}{2^{2n-1}(n-1)!n!}u^n\]
		\item Soit $x\in\left]-\frac{1}{4},\frac{1}{4}\right[$.\\
		On a d'après ce qui précède \[1-\sqrt{1-4x}=\sum_{n=1}^{+\infty}\frac{2(2n-2)!}{4^n(n-1)!n!}(4x)^n=\sum_{n=1}^{+\infty}\frac{2(2n-2)!}{(n-1)!n!}x^n\]
		Par conséquent, on en déduit que si $x\ne 0$
		\[F(x)=\sum_{n=0}^{+\infty}\frac{(2n)!}{n!(n+1)!}x^n\]
		On remarque que l'égalité reste vraie pour $x=0$.
		Finalement, par unicité du développement en série entière, on en conclut que \[C_n=\frac{(2n)!}{n!(n+1)!}\]
	\end{enumerate}
	\subsection{Nombres de parties}
	\label{Nombres de parties corrigé}
	\textcolor{blue}{\hyperref[Nombres de parties]{[Enoncé]}}\\
	
	\subsection{Nombre de surjection}
	\label{Nombre de surjection corrigé}
		\textcolor{blue}{\hyperref[Nombre de surjection]{[Enoncé]}}\\
	
	\subsection{Formule de Legendre}
	\label{Formule de Legendre corrigé}
		\textcolor{blue}{\hyperref[Formule de Legendre]{[Enoncé]}}\\
	
	\subsection{Théorème de Hall \etoile{4}}
		\textcolor{blue}{\hyperref[Théorème de Hall]{[Enoncé]}}\\
	\label{Théorème de Hall corrigé}
	(i)$\implies$(ii) :\\
	Supposons que l'on dispose de $x_1\in A_1,\dots,x_n\in A_n$ tous distincts et donnons nous une partie $I$ de $\crblanc{1}{n}$.\\
	$\{x_i,\ i\in I\}\subset \displaystyle\bigcup_{i\in I}A_i$ donc $\displaystyle\Card\left(\bigcup_{i\in I}A_i\right)\geq \Card(\{x_i,\ i\in I\})=\Card(I)$.\\\\
	(ii)$\implies$(i) :\\
	Par récurrence forte sur $n\in \N^*$.\\
	$n=1$:\\
	Si l'on dispose de $A_1\subset E$ tel que $\Card(A_1)\geq \Card(\crblanc{1}{1})=1$ alors il existe $x_1\in A_1$ et la preuve est terminée.\\
	Supposons le résultat vrai, pour tout $k\in \crblanc{1}{n}$, pour un certain $n\in \N^*$ et donnons nous $A_1,\dots,A_{n+1}\subset E$ pour lesquels on ai (ii).\\
	Dans la suite on appellera \textit{système de représentant} de la famille $(A_i)_{i\in I}$ toute famille $(x_i)_{i\in I}\in \displaystyle\prod_{i\in I}A_i$ dont les éléments sont tous distincts. Considérons $\mathcal I=\{I\in\mathcal P(\crblanc{1}{n+1})\backslash\{\emptyset,\crblanc{1}{n+1}\},\ \Card(A_I)=\Card(I)\}$ où pour $I\subset \crblanc{1}{n+1},\ A_I$ désigne la réunion $\displaystyle\bigcup_{i\in I}A_i$.\\
	Dans un premier temps supposons que $\mathcal I$ est vide, autrement dit que $\forall I\subset \crblanc{1}{n+1},\ 1\leq\Card(I)\leq n\implies \Card(A_I)>\Card(I)$.\\
	Considérons $x_{n+1}\in A_{n+1}$ puis posons pour $i\in \crblanc{1}{n},\ B_i=A_i\backslash\{x_{n+1}\}$. Montrons que la famille $(B_i)_{1\leq i\leq n}$ satisfait (ii).\\
	Soit $I\subset \crblanc{1}{n}$ non vide.\\
	$\displaystyle\Card\left(\bigcup_{i\in I}B_i\right)=\Card\left(\bigcup_{i\in I}A_i\backslash\{x_{n+1}\}\right)=\Card\left(\left(\bigcup_{i\in I}A_i\right)\backslash\{x_{n+1}\}\right)\geq \Card\left(\bigcup_{i\in I}A_i\right)-1>\Card(I)-1$.\\
	Donc $\displaystyle\Card\left(\bigcup_{i\in I}B_i\right)\geq \Card(I)$.\\
	On en déduit par hypothèse de récurrence qu'il existe un système de représentant $(x_i)_{1\leq i\leq n}$ de $(B_i)_{1\leq i\leq n}$. Mais alors par construction, $x_{n+1}\in A_{n+1}$ et pour tout $(i,j)\in \crblanc{1}{n}^2$ distincts, $x_i\in A_i$ et $x_i\ne x_{n+1}$ et $x_i\ne x_j$. Donc $(x_i)_{1\leq i\leq n+1}$ est un système de représentant de $(A_i)_{1\leq i\leq n+1}$.\\\\
	Supposons maintenant que $\mathcal I$ n'est pas vide et fixons $I\in \mathcal I$. $1\leq\Card(I)\leq n$ donc en appliquant l'hypothèse de récurrence à $(A_i)_{i\in I}$, qui vérifie bien (ii), il existe un système de représentant $(y_i)_{i\in I}$ de $(A_i)_{i\in I}$.\\
	On pose alors $K=\crblanc{1}{n+1}\backslash I$ puis pour tout $k\in K,\ C_k=A_k\backslash A_I$. Montrons $(C_k)_{k\in K}$ vérifie (ii).\\
	Soit $J\subset K$.\\
	$\displaystyle\Card\left(\bigcup_{j\in J}C_j\right)=\Card\left(\bigcup_{j\in J}A_j\backslash A_I\right)=\Card\left(\left(\bigcup_{j\in J}A_j\right)\backslash A_I\right)=\Card\left(\left(\bigcup_{i\in J\cup I}A_j\right)\backslash A_I\right)\geq \Card\left(\bigcup_{j\in J\cup I}A_j\right)-\Card(A_I)$.\\
	Or $I\in \mathcal I$ donc $\Card(A_I)=\Card(I)$ et par (ii) $\Card\left(\bigcup_{j\in J\cup I}A_j\right)\geq \Card(J\cup I)$. De plus, l'union $J\cup I$ est disjointe puisque $J$ est inclus dans le complémentaire de $I$.\\
	On en déduit que $\displaystyle\Card\left(\bigcup_{j\in J}C_j\right)\geq \Card(J\cup I)-\Card(I)=\Card(J)$.\\
	Enfin $1\leq\Card(K)\leq n$ donc par hypothèse de récurrence il existe un système de représentant $(y_i)_{i\in K}$ de $(C_k)_{k\in K}$.\\
	Mais alors par construction si $k\in K$, $y_k\in A_k$ n'est pas un élément de la famille $(y_i)_{i\in I\cup K\backslash\{k\}}$. Ainsi $(y_i)_{i\in I\cup K}$ est un système de représentant de $(A_i)_{1\leq i\leq n+1}$ ce qui termine l'hérédité.
	
	\subsection{Formule du Crible}
		\textcolor{blue}{\hyperref[Formule du crible]{[Enoncé]}}\\
	\label{Formule du crible corrigé}
	
	\subsection{Cardinal de $GL_n(K)$ et $SL_n(K)$}
	\textcolor{blue}{\hyperref[Cardinal de GL_n et SL_n]{[Enoncé]}}\\
	\label{Cardinal de Gl_n et Sl_n corrigé}
	
	\subsection{$\Q$ est dénombrable}
	\label{Q est dénombrable corrigé}
	\textcolor{blue}{\hyperref[Q est dénombrable]{[Enoncé]}}\\
	
	\subsection{$\R$ n'est pas dénombrable}
	\textcolor{blue}{\hyperref[R n'est pas dénombrable]{[Enoncé]}}\\
	\label{R n'est pas dénombrable corrigé}
	
	\subsection{Dénombrabilité des nombres algébriques}
	\textcolor{blue}{\hyperref[Dénombrabilité des nombres algébriques]{[Enoncé]}}\\
	\label{Dénombrabilité des nombres algébriques corrigé}
	
	\subsection{Théorème de Cantor}
	\textcolor{blue}{\hyperref[Théorème de Cantor]{[Enoncé]}}\\
	\label{Théorème de Cantor corrigé}
	
	\subsection{Fonction qui intervertit rationnels et irrationnels}
	\textcolor{blue}{\hyperref[Fonction qui intervertit rationnels et irrationnels]{[Enoncé]}}\\
	\label{Fonction qui intervertit rationnels et irrationnels corrigé}
	
	\subsection{Support d'une famille sommable}
	\textcolor{blue}{\hyperref[Support d'une famille sommable]{[Enoncé]}}\\
	\label{Support d'une famille sommable corrigé}
	
	\subsection{Théorème de Froda}
	\textcolor{blue}{\hyperref[Théorème de Froda]{[Enoncé]}}\\
	\label{Théorème de Froda corrigé}
	
	\subsection{Ouverts de $\R$ \xens{4}}
	\label{Ouverts de R corrigé}
	\textcolor{blue}{\hyperref[Ouverts de R]{[Enoncé]}}\\
	Soit $\mathcal U$ un ouvert de $\R$. $\forall x>0,\ \exists \varepsilon_x>0,\ I_x:=]x-\varepsilon_x,x+\varepsilon_x[\subset\mathcal U$. Alors on a clairement $\mathcal U=\displaystyle\bigcup_{x\in \mathcal U}I_x$. Or par densité de $\Q$ dans $\R$, chaque $I_x$ contient un rationnel $q_x$. Donc $\exists r_{q_x}=\min(|x-q_x|,q_x-x+\varepsilon_x,x+\varepsilon_x-q_x)>0,\ x\in J_{q_x}:=]q_x-r_{q_x},q_x+r_{q_x}[\subset \mathcal U$.\\
	Par construction, $\mathcal U=\displaystyle\bigcup_{x\in \mathcal U}J_{q_x}$. Or $\Q$ étant dénombrable, l'ensemble $Q:=\{q_x,\ x\in \mathcal U\}\subset\Q$ est au plus dénombrable. Ainsi $\mathcal U=\displaystyle\bigcup_{q\in Q} J_q$ avec $Q$ au plus dénombrable.\\
	Pour réaliser une union disjointe on va utiliser les composantes connexes par arcs de $\mathcal U$, qui partitionnent $\mathcal U$ par définition.\\
	On rappelle que les connexes par arcs de $\R$ sont exactement les intervalles. Comme on a écrit $\mathcal U$ comme une réunion au plus dénombrable de connexes par arcs, $\mathcal U$ a un nombre au plus dénombrable de composantes connexe par arcs. De plus les composantes connexes par arcs d'un ouvert sont ouvertes. En effet, donnons nous $J$ une composante connexe par arcs de $\mathcal U$. Si $J=\R$ alors c'est bon. Sinon $J$ a une borne finie, disons que c'est son sup $s$. Si $s\in J$ alors par caractère ouvert de $\mathcal U,\ \exists \eta>0,\ [s,s+\eta[\subset \mathcal U$. Mais alors $J\subsetneq J\cup[s,s+\eta[\subset \mathcal U$ avec $J\cup[s,s+\eta[$ connexe par arcs ce qui contredit la maximalité de $J$ en tant que composante connexe par arcs. Donc $s\notin J$, càd $J$ est un intervalle ouvert. Enfin
	$$\mathcal U=\bigsqcup_{J\T{ composante connexe par arcs de }\mathcal U}J$$
	
	\subsection{Ensemble discret}
	\textcolor{blue}{\hyperref[Ensemble discret]{[Enoncé]}}\\
	\label{Ensemble discret corrigé}
	
	\subsection{Ensemble parfait}
	\textcolor{blue}{\hyperref[Ensemble parfait]{[Enoncé]}}\\
	\label{Ensemble parfait corrigé}
	\subsubsection{Poussière de Cantor}
	\subsubsection{Parfaits de $\R$}
	\subsubsection{Théorème de Cantor-Berdixson}
	
	\newpage
\section{Correction Probabilités}
	\subsection{Somme de variables de Bernoulli indépendantes \etoile{2}}
	\label{Somme de variables de Bernoulli indépendantes corrigé}
		\textcolor{blue}{\hyperref[Somme de variables de Bernoulli indépedantes]{[Enoncé]}}\\
	Tout d'abord, $X(\Omega)=\crblanc{0}{n}$. Notons, pour $k\in \crblanc{0}{n},\ E_k$ l'ensemble des parties de $\crblanc{1}{n}$ a $k$ éléments. Fixons $k\in \crblanc{0}{n}$.\\
	Pour que $X=k$ il est nécessaire et suffisant de choisir $k$ indices $i$ dans $\crblanc{1}{n}$ pour lesquels on ait $X_i=1$ et $X_j=0$ pour les $n-k$ autres indices $j$:
	$$\{X=k\}=\displaystyle\bigsqcup\limits_{I\in E_k}\left(\bigcap\limits_{i\in I}\{X_i=1\}\bigcap\limits_{j\in \crblanc{1}{n}\backslash I}\{X_j=0\}\right)$$
	Ainsi,
	\begin{align*}
		\p(X=k)&=\displaystyle\sum\limits_{I\in E_k}\p\left(\bigcap\limits_{i\in I}\{X_i=1\}\bigcap\limits_{j\in \crblanc{1}{n}\backslash I}\{X_j=0\}\right)\\
		\text{(par indépendance) }&=\displaystyle\sum\limits_{I\in E_k}\left(\prod\limits_{i\in I}\p(X_i=1)\prod\limits_{j\in \crblanc{1}{n}\backslash I}\p(X_j=0)\right)\\
		&=\displaystyle\sum\limits_{I\in E_k}\left(\prod\limits_{i\in I}p\prod\limits_{j\in \crblanc{1}{n}\backslash I}(1-p)\right)\\
		&=\displaystyle\sum\limits_{I\in E_k}p^{\text{Card}(I)}(1-p)^{\text{Card}(\crblanc{1}{n}\backslash I)}\\
		&=\displaystyle\sum\limits_{I\in E_k}p^k(1-p)^{n-k}\\
		&=\text{Card}(E_k)p^k(1-p)^{n-k}\\
		&=\displaystyle\binom{n}{k}p^k(1-p)^{n-k}
	\end{align*}
	$X\sim\B(n,p)$.
	
	\subsection{Approximation de la loi de Poisson par des lois binomiales \etoile{1}}
	\label{Approximation de la loi de Poisson par des lois binomiales corrigé}
		\textcolor{blue}{\hyperref[Approximation d'une loi de Poisson par des lois binomiales]{[Enoncé]}}\\
	Soit $k\in \N$.\\
	$\forall n\in \N,\ \p(X_n=k)=\displaystyle\binom{n}{k}p_n^k(1-p_n)^{n-k}=\frac{n!}{k!(n-k)!}p_n^k(1-p_n)^{n-k}$.\\
	Donc d'après la formule de Stirling,
	\begin{align*}
		\p(X_n=k)&\unfty{\sim}\displaystyle\frac{p_n^k(1-p_n)^{n-k}}{k!}\cdot\frac{\left(\displaystyle\frac{n}{e}\right)^n\displaystyle\sqrt{2\pi n}}{\left(\displaystyle\frac{n-k}{e}\right)^{n-k}\displaystyle\sqrt{2\pi(n-k)}}\\
		&\unfty{\sim}\displaystyle\frac{p_n^k(1-p_n)^{n-k}}{k!}\left(\frac{n}{n-k}\right)^ne^k(n-k)^k\\
		&\unfty{\sim}\displaystyle\frac{(np_n)^k(1-p_n)^{n-k}}{k!}e^{n(\ln(n)-\ln(n-k))}e^k\\
		&\unfty{\sim}\displaystyle\frac{\lambda^ke^{(n-k)\ln(1-p_n)}}{k!}e^{-n\ln(1-k/n)}e^k
	\end{align*}
	Or $np_n\unfty{=}\bigO{1}$ donc $p_n\unfty{\longrightarrow}0$ d'où $(n-k)\ln(1-p_n)\unfty{\sim}(n-k)(-p_n)\unfty{\sim}-\lambda$ et puis $e^{(n-k)\ln(1-p_n)}\unfty{\sim}e^{-\lambda}$.\\
	De plus, $-n\ln\left(1-\displaystyle\frac{k}{n}\right)\unfty{\longrightarrow}-k$ d'où $e^{-n\ln(1-k/n)}\unfty{\sim}e^{-k}$.\\
	Ainsi, $\p(X_n=k)\unfty{\longrightarrow}\displaystyle\frac{\lambda^ke^{-\lambda}}{k!}$.
	
	\subsection{Inégalité de Markov et inégalité de Bienaymé-Tchébychev \etoile{1}}
	\label{Inégalité de Markov et inégalité de Bienaymé-Tchébychev corrigé}
		\textcolor{blue}{\hyperref[Inégalité de Markov et inégalité de Bienaymé-Tchebychev]{[Enoncé]}}\\
	\underline{Inégalité de Markov} :\\
	Si $X\in L^1$ est une variable aléatoire réelle positive et $a\in \R^*_+$ alors,
	$$\p(X\geq a)\leq \displaystyle\frac{\E(X)}{a}$$
	Soient $X\in L^1$ est une variable aléatoire réelle positive et $a\in \R^*_+$.\\
	$\p(X\geq a)=\displaystyle\sum\limits_{\substack{x\in X(\Omega)\\x\geq a}}\p(X=x)\leq \displaystyle\sum\limits_{\substack{x\in X(\Omega)\\x\geq a}}\frac{x}{a}\p(X=x)\leq \displaystyle\frac{1}{a}\sum\limits_{x\in X(\Omega)}x\p(X=x)=\displaystyle\frac{\E(X)}{a}$.\\\\\\
	\underline{Inégalité de Bienaymé-Tchébychev} :\\
	Si $X\in L^2$ et $\varepsilon\in \R^*_+$ alors,
	$$\p(|X-\E(X)|\geq\varepsilon)\leq\displaystyle\frac{\V(X)}{\varepsilon^2}$$
	Soient $X\in L^2$ et $\varepsilon\in \R^*_+$.\\
	$Y=(X-\E(X))^2$ est une variable aléatoire réelle positive d'espérance finie, en effet son espérance est $\V(X)<+\infty$.\\
	D'après l'inégalité de Markov,
	$$\p(|X-\E(X)|\geq\varepsilon)=\p(Y\geq \varepsilon^2)\leq\displaystyle\frac{\E(Y)}{\varepsilon^2}=\frac{\V(X)}{\varepsilon^2}$$
	
	\subsection{Paradoxe des anniversaires \etoile{2}}
	\label{Paradoxe des anniversaires corrigé}
		\textcolor{blue}{\hyperref[Paradoxe des anniversaires]{[Enoncé]}}\\
	Notons $A_n$ l'évènement "Aucun des $n$ élèves de la classe ne sont nés le même jour" de sorte que $\p(A_n)=1-p_n$.\\
	Notons pour $k\in \crblanc{1}{n},\ X_k$ la variable aléatoire qui associe au $k$-ième élève associe son jour de naissance $(X_k(\Omega)=\crblanc{1}{365})$.\\
	L'énoncé nous invite à munir $\Omega$ de la probabilité uniforme pour laquelle on a $\forall (k,j)\in \crblanc{1}{n}\times\crblanc{1}{365},\ \p(X_k=j)=\displaystyle\frac{1}{365}$.\\
	Alors $A_n=\displaystyle\bigcap\limits_{k=1}^n\left(\bigcap\limits_{p=1}^{k-1}\{X_k\ne X_p\}\right)$ et en supposant l'indépendance des $X_k$ (ce qui revient à ne pas prendre en compte les situations du type jumeaux par exemple) :
	\begin{align*}
		p_n&=1-\displaystyle\prod\limits_{k=1}^n\prod\limits_{p=1}^{k-1}\p(X_k\ne X_p)\\
		&=1-\prod\limits_{k=1}^n\prod\limits_{p=1}^{k-1}\sum\limits_{j=1}^{365}\p(X_k=j,X_p\ne j)\\
		\text{(par indépendance)}&=1-\prod\limits_{k=1}^n\prod\limits_{p=1}^{k-1}\sum\limits_{j=1}^{365}\p(X_k=j)\p(X_p\ne j)\\
		&=1-\prod\limits_{k=1}^n\prod\limits_{p=1}^{k-1}\sum\limits_{j=1}^{365}\frac{1}{365}\cdot\frac{364}{365}\\
		&=1-\prod\limits_{k=1}^n\prod\limits_{p=1}^{k-1}\frac{364}{365}\\
		&=1-\prod\limits_{k=1}^n\left(\frac{364}{365}\right)^{k-1}\\
		&=1-\left(\displaystyle\frac{364}{365}\right)^{\displaystyle\sum\limits_{k=1}^n(k-1)}\\
		&=1-\left(\displaystyle\frac{364}{365}\right)^{\frac{n(n-1)}{2}}
	\end{align*}
	Ainsi, $p_n>0,5\iff \left(\displaystyle\frac{364}{365}\right)^{\frac{n(n-1)}{2}}<0,5\iff \displaystyle\frac{n(n-1)}{2}\ln\left(\displaystyle\frac{364}{365}\right)<\ln(0,5)\iff n(n-1)>\displaystyle\frac{2\ln(2)}{\ln\left(\displaystyle\frac{365}{364}\right)}\approx505,3$.\\
	On calcule $23\times 22=506$ donc à partir de $n=23$ élèves, il y a plus d'une chance sur $2$ pour que deux élèves aient la même date d'anniversaire.\\
	Pour $n=50,\ p_n\approx0,965$.
	
	\subsection{Variable aléatoire presque sûrement nulle/constante}
	\label{Variable aléatoire presque sûrement nulle/constante corrigé}
	\textcolor{blue}{\hyperref[Variable aléatoire presque sûrement nulle/constante]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item D'après la formule de transfert, on a : \[\E(|X|)=\sum_{x\in X(\Omega)}|x|\p(X=x)\]
		Il s'agit d'une somme à termes positifs, ainsi on en déduit que \[\forall x\in X(\Omega), |x|\p(X=x)=0\]
		Par conséquent, \[\forall x\in X(\Omega), |x|=0 \text{ ou } \p(X=x)=0\]
		Donc pour tout $x\in X(\Omega)$ non nul, on a $\p(X=x)=0$.
		Par conséquent, \[\p(X=0)=1\] 
		\item On sait que $\V(X)=\E((X-\E(X))^2)$, donc en posant $Y=(X-\E(X))^2$, d'après la question 1, on a que \[Y \text{ est presque surement nulle}\]
		Donc \[\p(X-\E(X)=0)=1\]
		D'où le résultat en posant $a=\E(X)$.
	\end{enumerate}
	\subsection{Loi de Pascal \etoile{2}}
	\label{Loi de Pascal corrigé}
		\textcolor{blue}{\hyperref[Loi de Pascal]{[Enoncé]}}\\
	\begin{enumerate}
		\item La variable aléatoire $T_r$ peut être interprétée comme celle qui indique le premier rang d'obtention du $r$-ième succès d'une expérience de Bernoulli que l'on répète indéfiniment. Pour $r=1$ on reconnaît l'interprétation d'une loi géométrique. C'est donc ce que l'on va montrer.\\
		Il est clair que $T_1(\Omega)=\N^*\cup\{+\infty\}$. Fixons $k\in \N^*$.\\
		$\{T_1=k\}=\{X_k=1\}\displaystyle\cap\bigcap\limits_{i=1}^{k-1}\{X_i=0\}$.\\
		Donc par indépendance, $\p(T_1=k)=\p(X_k=1)\displaystyle\prod\limits_{i=1}^{k-1}\p(X_i=0)=p(1-p)^{k-1}$.\\
		Ainsi, $T_1\sim \mathcal{G}(p)$.
		\item Soit $n\in \N^*$.\\
		D'après le cours, $S_n=\displaystyle\sum\limits_{k=1}^nX_k\sim \B(n,p)$ donc $\p(S_n=r-1)=\displaystyle\binom{n}{r-1}p^{r-1}(1-p)^{n-r+1}$.\\
		Ensuite, $\{T_r=n\}=\{S_n=r\}\cap\{S_{n-1}=r-1\}$.\\
		Donc $\p(T_r=n)=\p(S_{n-1}=r-1)\p(S_n=r\ |S_{n-1}=r-1)=\p(S_{n-1}=r-1)\p(X_n=1)=\displaystyle\binom{n-1}{r-1}p^r(1-p)^{n-r}$.\\
		On remarque d'ailleurs que cette expression reste valide pour $r<n$.
		\item $\overline{\{T_r=+\infty\}}=\displaystyle\bigsqcup\limits_{n\in \N^*}\{T_r=n\}$ donc
		\begin{align*}
			\p(\overline{T_r=+\infty})&=\displaystyle\sum\limits_{n=1}^{+\infty}\p(T_r=n)\\
			&=\sum\limits_{n=1}^{+\infty}\binom{n-1}{r-1}p^r(1-p)^{n-r}\\
			&=\sum\limits_{n=r}^{+\infty}\binom{n-1}{r-1}p^r(1-p)^{n-r}\\
			&=\frac{p^r}{(r-1)!}\sum\limits_{n=r}^{+\infty}\frac{(n-1)!}{(n-r)!}(1-p)^{n-r}\\
			&=\frac{p^r}{(r-1)!}\sum\limits_{n=r-1}^{+\infty}\frac{n!}{(n+1-r)!}(1-p)^{n+1-r}
		\end{align*}
		On reconnaît la somme de la dérivée $r-1$-ième de la série géométrique en $1-p\in ]-1,1[$ :\\
		$\displaystyle\sum\limits_{n=r-1}^{+\infty}\frac{n!}{(n+1-r)!}x^{n+1-r}=\frac{d^{r-1}}{dx^{r-1}}\left(\sum\limits_{n=0}^{+\infty}x^n\right)=\displaystyle\frac{d^{r-1}}{dx^{r-1}}\left(\frac{1}{1-x}\right)=\frac{(r-1)!}{(1-x)^r}$.\\
		D'où $\p(\overline{T_r=+\infty})=\displaystyle\frac{p^r}{(r-1)!}\cdot\frac{(r-1)!}{p^r}=1$ et $\p(T_r=+\infty)=0$.
	\end{enumerate}
	
	\subsection{Lemme de Borel-Cantelli et loi du zéro-un de Borel \etoile{3}}
	\label{Lemme de Borel-Cantelli et loi du zéro-un de Borel corrigé}
		\textcolor{blue}{\hyperref[Lemme de Borel-Cantelli et loi du zéro-un de Borel]{[Enoncé]}}\\
	\begin{enumerate}
		\item \begin{enumerate}[label=\alph*.]
			\item $\forall n\in \N,\ B_{n+1}\subset B_n$ donc par continuité décroissante, $\p(A)=\p\left(\displaystyle\bigcap_{n\in\N}B_n\right)=\unfty{\lim}\p(B_n)$.
			\item $\forall n\in \N,\ \p(B_n)\leq \displaystyle\sum\limits_{k=n}^{+\infty}\p(A_k)$. Or comme la série $\displaystyle\sum\limits_{n\in\N}\p(A_n)$ converge, on sait que $\unfty{\lim}\displaystyle\sum\limits_{k=n}^{+\infty}\p(A_k)=0$ d'où $\p(A)=0$.
		\end{enumerate}
		\item Rappelons que $\ln$ est concave sur $\R^*_+$ et donc que $\forall x\in \R^*_+,\ \ln(x)\leq \ln'(1)(x-1)+\ln(1)=x-1$. De plus, par indépendance, $\p\left(\displaystyle\bigcap\limits_{k=n}^{n+p}\overline{A_k}\right)=\displaystyle\prod\limits_{k=n}^{n+p}\p(\overline{A_k})$.\\
		Si $\p\left(\displaystyle\bigcap\limits_{k=n}^{n+p}\overline{A_k}\right)=0$ alors l'inégalité est trivialement vérifiée et si ce n'est pas le cas alors,\\
		$\ln\left(\p\left(\displaystyle\bigcap\limits_{k=n}^{n+p}\overline{A_k}\right)\right)=\displaystyle\sum\limits_{k=n}^{n+p}\ln(\p(\overline{A_k}))\leq\displaystyle\sum\limits_{k=n}^{n+p}\left(\p(\overline{A_k})-1\right)=-\displaystyle\sum\limits_{k=n}^{n+p}\p(A_k)$.\\
		Donc par passage à l'exponentielle qui est une fonction croissante sur $\R$,\\
		$\p\left(\displaystyle\bigcap\limits_{k=n}^{n+p}\overline{A_k}\right)\leq \exp\left(-\displaystyle\sum\limits_{k=n}^{n+p}\p(A_k)\right)$.
		\item $\displaystyle\sum\limits_{n\in\N}\p(A_n)$ étant une série à termes positifs divergente, sa somme vaut $+\infty$. Ainsi en faisant tendre $p$ vers l'infini dans l'inégalité précédente, $\upfty{\lim}\p\left(\displaystyle\bigcap\limits_{k=n}^{n+p}\overline{A_k}\right)\leq \exp\left(-\displaystyle\sum\limits_{k=n}^{+\infty}\p(A_k)\right)=0$. De plus par continuité décroissante, $\upfty{\lim}\p\left(\displaystyle\bigcap\limits_{k=n}^{n+p}\overline{A_k}\right)=\p\left(\displaystyle\bigcap\limits_{k=n}^{+\infty}\overline{A_k}\right)=\p(\overline{B_n})$. Donc $\p(\overline{B_n})=0$.\\
		Ainsi comme $\p(\overline{A})=\p\left(\displaystyle\bigcup\limits_{n\in \N}\overline{B_n}\right)\leq \displaystyle\sum\limits_{n=0}^{+\infty}\p(\overline{B_n})=0$, $\p(\overline{A})=0$ c'est à dire $\p(A)=1$.
	\end{enumerate}
	
	\subsection{Formule d'antirépartition \etoile{3}}
	\label{Formule d'antirépartition corrigé}
		\textcolor{blue}{\hyperref[Formule d'antirépartition]{[Enoncé]}}\\
	Soit $N\in \N^*$.
	\begin{align*}
		\sum\limits_{n=0}^{N}n\p(X=n)&=\sum\limits_{n=1}^{N}n(\p(X>n-1)-\p(X>n))\\
		&=\sum\limits_{n=1}^{N}((n-1)\p(X>n-1)-n\p(X>n))+\sum\limits_{n=1}^{N}\p(X>n-1)\\
		&=\sum\limits_{n=1}^{N}\p(X>n-1)-N\p(X>N)\\
		&=\sum\limits_{n=0}^{N-1}\p(X>n)-N\p(X>N)\\
	\end{align*}
	Si $\displaystyle\sum\limits_{n\in \N}\p(X>n)$ converge alors $\forall N\in \N^*,\ \displaystyle\sum\limits_{n=0}^{N}n\p(X=n)\leq \displaystyle\sum\limits_{n=0}^{N-1}\p(X>n)\leq \sum\limits_{n=0}^{+\infty}\p(X>n)<+\infty$ d'où $X$ est d'espérance finie.\\
	Réciproquement, supposons que $X$ soit d'espérance finie.\\
	Alors le reste $\displaystyle\sum\limits_{n=N}^{+\infty}n\p(X=n)$ est défini et converge vers $0$. De plus,\\
	$\forall N\in \N,\ \displaystyle\sum\limits_{n=N}^{+\infty}n\p(X=n)\geq N\displaystyle\sum\limits_{n=N}^{+\infty}\p(X=n)=N\p(X>N)\geq 0$.\\
	Donc $N\p(X>N)\underset{N\to +\infty}{\longrightarrow}0$ et donc $\displaystyle\sum\limits_{n=0}^{N-1}\p(X>n)\underset{\N\to +\infty}{\longrightarrow}\E(X)<+\infty$.
	
	\subsection{Loi de Poisson \etoile{2}}
	\label{Loi de Poisson corrigé}
		\textcolor{blue}{\hyperref[Loi de Poisson]{[Enoncé]}}\\
	\begin{enumerate}
		\item Soit $n\in \N$. Le choix du guichet pour une voiture étant aléatoire et indépendant des autres voitures, on suppose que la loi de $X$ conditionnée par l'évènement $\{N=n\}$ est une loi binomiale de paramètre $n$ et $p=\displaystyle\frac{1}{m}$.\\
		C'est à dire $\forall k\in \crblanc{0}{n},\ \p(X=k|N=n)=\displaystyle\binom{n}{k}\frac{1}{m^k}\left(1-\frac{1}{m}\right)^{n-k}$.
		\item $N(\Omega)=\N$ donc d'après la formule des probabilités totales,\\
		$\p(X=k)=\displaystyle\sum\limits_{n=0}^{+\infty}\p(N=n)\p(X=k|N=n)=\sum\limits_{n=0}^{+\infty}e^{-\lambda}\frac{\lambda^n}{n!}\binom{n}{k}\frac{1}{m^k}\left(1-\frac{1}{m}\right)^{n-k}=e^{-\lambda}\left(\frac{\lambda}{m}\right)^k\frac{1}{k!}\sum\limits_{n=k}^{+\infty}\frac{\lambda^{n-k}}{(n-k)!}\left(1-\frac{1}{m}\right)^{n-k}$\\
		D'où $\p(X=k)=\displaystyle\frac{e^{-\lambda}}{k!}\left(\frac{\lambda}{m}\right)^k\sum\limits_{n=0}^{+\infty}\frac{\lambda^n}{n!}\left(1-\frac{1}{m}\right)^n$.
		\item $X(\Omega)=\N$ et $\forall k\in \N,\ \p(X=k)=\displaystyle\frac{e^{-\lambda}}{k!}\left(\frac{\lambda}{m}\right)^ke^{\lambda\left(1-\frac{1}{m}\right)}=e^{\frac{\lambda}{m}}\displaystyle\frac{\left(\frac{\lambda}{m}\right)^k}{k!}$ : $X\sim\mathcal{P}\left(\displaystyle\frac{\lambda}{m}\right)$.
		\item D'après le cours, $\E(X)=\V(X)=\displaystyle\frac{\lambda}{m}$.
	\end{enumerate}
	
	\subsection{Maximum de deux lois géométriques indépendantes \etoile{1}}
	\label{Maximum de deux lois géométriques indépendantes corrigé}
		\textcolor{blue}{\hyperref[Maximum de deux lois géométriques indépendantes]{[Enoncé]}}\\
	\begin{enumerate}
		\item $M(\Omega)=\N^*\cup\{+\infty\}$. Fixons $k\in \N^*$.\\
		$\{M>k\}=\{X>k\}\cap\{Y>k\}$. Donc par indépendance $\p(M>k)=\p(X>k)\p(Y>k)$\\
		De plus $\p(X>k)=\displaystyle\sum\limits_{i=k+1}^{+\infty}\p(X=i)=\sum\limits_{i=k+1}^{+\infty}p(1-p)^{i-1}=p\cdot\frac{(1-p)^k}{1-(1-p)}=(1-p)^k$. De même, $\p(Y>k)=(1-q)^k$.\\
		D'après la formule d'antirépartition,
		\begin{align*}
			\E(M)&=\sum\limits_{k=0}^{+\infty}\p(M>k)\\
			&=\sum\limits_{k=0}^{+\infty}\p(X>k)\p(Y>k)\\
			&=\sum\limits_{k=0}^{+\infty}[(1-p)(1-q)]^k\\
			&=\frac{1}{1-(1-p)(1-q)}\\
			&=\frac{1}{p+q-pq}
		\end{align*}
		\item \underline{$1^{ère}$ méthode :} On utilise la question $1$ :\\\\
		On a $Z+M=X+Y$ donc $\E(Z)=\E(X)+\E(Y)-\E(M)=\displaystyle\frac{1}{p}+\frac{1}{q}-\frac{1}{p+q-pq}$.\\\\
		\underline{$2^{nd}$ méthode :} On peut faire comme pour la question $1$ :\\\\
		$Z(\Omega)=\N^*\cup\{+\infty\}$. Fixons $k\in \N^*$.\\
		$\{Z\leq k\}=\{X\leq k\}\cap\{Y\leq k\}$. Donc par indépendance $\p(Z\leq k)=\p(X\leq k)\p(Y\leq k)$\\
		De plus $\p(X\leq k)=\displaystyle\sum\limits_{i=1}^k\p(X=i)=\sum\limits_{i=1}^kp(1-p)^{i-1}=p\cdot\frac{1-(1-p)^k}{1-(1-p)}=1-(1-p)^k$. De même, $\p(Y\leq k)=1-(1-q)^k$.\\
		D'après la formule d'antirépartition,
		\begin{align*}
			\E(Z)&=\sum\limits_{k=0}^{+\infty}\p(Z>k)\\
			&=\sum\limits_{k=0}^{+\infty}\left(1-\p(Z\leq k)\right)\\
			&=\sum\limits_{k=0}^{+\infty}\left(1-\p(X\leq k)\p(Y\leq k)\right)\\
			&=\sum\limits_{k=0}^{+\infty}\left(1-(1-(1-p)^k)(1-(1-q)^k)\right)\\
			&=\sum\limits_{k=0}^{+\infty}\left((1-p)^k+(1-q)^k-[(1-p)(1-q)]^k\right)\\
			&=\sum\limits_{k=0}^{+\infty}(1-p)^k+\sum\limits_{k=0}^{+\infty}(1-q)^k-\sum\limits_{k=0}^{+\infty}\left((1-p)(1-q)\right)^k\\
			&=\frac{1}{p}+\frac{1}{q}-\frac{1}{1-(1-p)(1-q)}\\
			&=\frac{1}{p}+\frac{1}{q}-\frac{1}{p+q-pq}
		\end{align*}
	\end{enumerate}
	
	
	\subsection{Max et min de lois géométriques iid \etoile{4}}
	\label{Max et min de lois géométriques iid corrigé}
		\textcolor{blue}{\hyperref[Max et min de lois géométriques iid]{[Enoncé]}}\\
	\begin{enumerate}
		\item Fixons $n\in \N^*$.\\
		Tout d'abord, $Y_n(\Omega)=\N^*\cup\{+\infty\}$.\\
		$\forall k\in \N^*,\ \{Y_n>k\}=\displaystyle\bigcap\limits_{i=1}^n\{X_i>k\}$.\\
		Donc $\forall k\in \N^*, \p(Y_n>k)=\displaystyle\prod\limits_{i=1}^n\p(X_i>k)=\prod\limits_{i=1}^n(1-p)^k=(1-p)^{kn}$. Ainsi d'après la formule d'antirépartition,\\
		$\E(Y_n)=\displaystyle\sum\limits_{k=0}^{+\infty}\p(Y_n>k)=\sum\limits_{k=0}^{+\infty}(1-p)^{kn}=\frac{1}{1-(1-p)^n}$.
		\item Fixons $n\in \N^*$.\\
		Tout d'abord, $Z_n(\Omega)=\N^*\cup\{+\infty\}$.\\
		D'après la formule d'antirépartition,\begin{align*}
			\E(Z_n)&=\sum\limits_{k=0}^{+\infty}\p(Z_n>k)\\
			&=\sum\limits_{k=0}^{+\infty}1-\p(Z_n\leq k)\\
			&=\sum\limits_{k=0}^{+\infty}1-\p\left(\bigcap\limits_{i=1}^n\{X_i\leq k\}\right)\\
			\text{(par indépendance) }&=\sum\limits_{k=0}^{+\infty}\left(1-\prod\limits_{i=1}^n\p(X_i\leq k)\right)\\
			&=\sum\limits_{k=0}^{+\infty}\left(1-\prod\limits_{i=1}^n(1-\p(X_i>k))\right)\\
			\text{avec }q=1-p\ &=\sum\limits_{k=0}^{+\infty}\left(1-\prod\limits_{i=1}^n(1-q^k)\right)\\
			&=\sum\limits_{k=0}^{+\infty}\left(1-(1-q^k)^n\right)\\
		\end{align*}
		Posons $\fonction{f_n}{\R_+}{\R}{x}{1-(1-q^x)^n}$. $f_n$ est continue décroissante et positive sur $\R_+$.
		\begin{align*}
			\forall k\in \N^*\ &(x\in [k,k+1]\implies f_n(k+1)\leq f_n(x)\leq f_n(k))\\
			\implies &\sum\limits_{k=0}^{+\infty}f_n(k+1)\leq\int_0^{+\infty}f(x)dx\leq\sum\limits_{k=0}^{+\infty}f_n(k)\\
			\implies &\E(Z_n)-f_n(0)\leq \int_0^{+\infty}f_n(x)dx\leq \E(Z_n)\\
			\implies &\int_0^{+\infty}f_n(x)dx\leq \E(Z_n)\leq 1+\int_0^{+\infty}f_n(x)dx
		\end{align*}
		Calcul de $\displaystyle\int_0^{+\infty}f_n(x)dx$ :\\
		On pose $u=1-q^x$. Ce changement de variable est légitime car la fonction $x\mapsto 1-q^x$ est de classe $\mathcal{C}^1$ et strictement décroissante sur $\R_+$.\\
		$\displaystyle\int_0^{+\infty}f_n(x)dx=\int_0^1(1-u^n)\frac{-du}{\ln(q)(1-u)}=-\frac{1}{\ln(q)}\int_0^1\frac{1-u^n}{1-u}du=-\frac{1}{\ln(q)}\int_0^1\sum\limits_{k=0}^{n-1}u^kdu=-\frac{1}{\ln(q)}\sum\limits_{k=1}^n\frac{1}{k}$
		Ainsi, $\E(Z_n)\unfty{\sim}-\displaystyle\frac{\ln(n)}{\ln(q)}=-\log_q(n)$.
	\end{enumerate}
	
	\subsection{Formule de Wald \etoile{3}}
	\label{Formule de Wald corrigé}
		\textcolor{blue}{\hyperref[Formule de Wald]{[Enoncé]}}\\
	\begin{enumerate}
		\item $S(\Omega)=\N$ est dénombrable. Soit $n\in \N$.\\
		$\{S=n\}=\displaystyle\bigsqcup\limits_{p\in \N^*}\left(\{N=p\}\cap\left\{\sum\limits_{k=1}^pX_k=n\right\}\right)$.\\
		Par opération, pour tout $p\in \N^*\ S_p=\displaystyle\sum\limits_{k=1}^pX_k$ est une variable aléatoire donc $\forall p,\in \N^*\ \{S_p=n\}\in \A$.\\
		$\forall p\in \N^*,\ \{N=p\}\in \A$. La tribu est stable par intersection finie donc $\forall p\in \N^*,\ \{N=p\}\cap\{S_p=n\}\in \A$.\\
		Enfin, la tribu est stable par union dénombrable donc $\{S=n\}=\displaystyle\bigsqcup\limits_{p\in \N^*}(\{N=p\}\cap\{S_p=n\})\in \A$.\\
		$S$ est bien une variable aléatoire discrète.
		\item Soit $t\in [0,1]$. Si $p\in \N^*,\ N,X_1,\dots,X_p$ sont mutuellement indépendantes donc d'après le lemme des coalitions, $N$ et $S_p$ sont indépendantes. On en déduit que $\forall n\in \N,\ \p(S=n)=\displaystyle\sum\limits_{p=1}^{+\infty}\p(N=p)\p(S_p=n)$:\\
		$G_S(t)=\displaystyle\sum\limits_{n=0}^{+\infty}\p(S=n)t^n=\sum\limits_{n=0}^{+\infty}\sum\limits_{p=1}^{+\infty}\p(N=p)\p(S_p=n)t^n$.\\
		$(\p(N=p)\p(S_p=n))_{(n,p)\in \N\times \N^*}$ est sommable puisque\\
		$\displaystyle\sum\limits_{(n,p)\in \N\times \N^*}|\p(N=p)\p(S_p=n)t^n|=\sum\limits_{n=0}^{+\infty}\sum\limits_{p=1}^{+\infty}\p(N=p)\p(S_p=n)|t|^n=\sum\limits_{n=0}^{+\infty}\p(S=n)|t|^n=G_S(|t|)<+\infty$.\\\\
		Alors d'après le théorème de Fubini,\\
		$G_S(t)=\displaystyle\sum\limits_{p=1}^{+\infty}\sum\limits_{n=0}^{+\infty}\p(N=p)\p(S_p=n)t^n=\sum\limits_{p=1}^{+\infty}\p(N=p)\left(\sum\limits_{n=0}^{+\infty}\p(S_p=n)t^n\right)=\sum\limits_{p=1}^{+\infty}\p(N=p)G_{S_p}(t)$.\\
		Or pour tout $p\in \N^*,\ X_1,\dots,X_p$ sont mutuellement indépendantes. Donc $\forall p\in \N^*,\ G_{S_p}(t)=\displaystyle\prod\limits_{k=1}^pG_{X_k}(t)=\prod\limits_{k=1}^pG_X(t)=G_x(t)^p$.\\
		Ainsi, $G_S(t)=\displaystyle\sum\limits_{p=1}^{+\infty}\p(N=p)G_X(t)^p=G_N\circ G_X(t)$.
		\item $G_X$ est dérivable en $1$ et $G_N$ est dérivable en $G_X(1)=1$ donc $G_S$ est dérivable en $1$ et,\\
		$\E(S)=G_S'(1)=G_X'(1)\cdot G_N'\circ G_X(1)=\E(X)\E(N)$
		\item $G_X$ est deux fois dérivable en $1$ et $G_N$ est deux fois dérivable en $G_X(1)=1$ donc $G_S$ est deux fois dérivable en $1$ et,
		\begin{align*}
			\V(S)&=G_S''(1)+G_S'(1)-G_S'(1)^2\\
			&=G_X''(1)\cdot G_N'\circ G_X(1)+G_X'(1)^2\cdot G_N''\circ G_X(1)+\E(S)-\E(S)^2\\
			&=(\V(X)+\E(X)^2-\E(X))\E(N)+\E(X)^2(\V(N)+\E(N)^2-\E(N))+\E(X)\E(N)-\E(X)^2\E(N)^2\\
			&=\V(X)\E(N)+\E(X)^2\V(N)
		\end{align*}
	\end{enumerate}
	
	\subsection{Loi binomiale aléatoire \etoile{3}}
	\label{Loi binomiale aléatoire corrigé}
		\textcolor{blue}{\hyperref[Loi binomiale aléatoire]{[Enoncé]}}\\
	Tout d'abord, $Y(\Omega)=\N$ est dénombrable. Fixons $k\in \N$.\\
	$\{Y=k\}=\displaystyle\bigsqcup\limits_{n\in \N}(\{N=n\}\cap\{X_n=k\})$.\\
	$\forall n\in \N,\ (\{N=n\},\{X_n=k\})\in \A^2$. La tribu est stable par intersection finie et par réunion dénombrable donc $\{Y=k\}\in \A$.\\
	Ainsi $Y$ est bien une variable aléatoire discrète.\\
	De plus par indépendance, $\forall n\in \N,\ \p(N=n,X_n=k)=\p(N+1=n+1)\p(X_n=k)=q(1-q)^n\displaystyle\binom{n}{k}p^k(1-p)^{n-k}$.\\
	Donc,
	\begin{align*}
		\p(Y=k)&=\sum\limits_{n=0}^{+\infty}\p(N=n,X_n=k)\\
		&=q(p(1-q))^k\sum\limits_{n=0}^{+\infty}\binom{n}{k}((1-p)(1-q))^{n-k}\\
		&=\frac{q(p(1-q))^k}{k!}\sum\limits_{n=k}^{+\infty}\frac{n!}{(n-k)!}((1-p)(1-q))^{n-k}
	\end{align*}
	On reconnaît la somme de la dérivée $k$-ième de la série géométrique en $(1-p)(1-q)\in ]-1,1[$ :\\
	$\displaystyle\sum\limits_{n=k}^{+\infty}\frac{n!}{(n-k)!}x^{n-k}=\frac{d^k}{dx^k}\left(\sum\limits_{n=0}^{+\infty}x^n\right)=\frac{d^k}{dx^k}\left(\frac{1}{1-x}\right)=\frac{k!}{(1-x)^{k+1}}$\\
	Donc, $\p(Y=k)=\displaystyle\frac{q(p(1-q))^k}{k!}\sum\limits_{n=k}^{+\infty}\frac{n!}{(n-k)!}((1-p)(1-q))^{n-k}=q\frac{(p(1-q))^k}{(p+q-pq)^{k+1}}=\frac{q}{p+q-pq}\left(1-\frac{q}{p+q-pq}\right)^k$.\\
	$Y\sim \mathcal{G}\left(\displaystyle\frac{q}{p+q-pq}\right)$.
	
	\subsection{Somme de lois de Poisson \etoile{1}}
	\label{Somme de lois de Poisson corrigé}
		\textcolor{blue}{\hyperref[Somme de lois de Poisson]{[Enoncé]}}\\
	$\forall t\in [-1,1],\ G_S(t)=\E(t^S)=\displaystyle\E\left(\prod_{i=1}^nt^{X_i}\right)$.\\
	Et par indépendance, $\forall t\in [-1,1],\ \displaystyle\E\left(\prod_{i=1}^nt^{X_i}\right)=\prod_{i=1}^n\E\left(t^{X_i}\right)=\prod_{i=1}^nG_{X_i}(t)=\prod_{i=1}^ne^{(t-1)\lambda_i}=e^{(t-1)\displaystyle\sum_{i=1}^n\lambda_i}$.\\
	Ainsi $S\sim \mathcal P\left(\displaystyle\sum_{i=1}^n\lambda_i\right)$.
	
	\subsection{Obtenir trois pile consécutifs}
	\label{Obtenir trois pile consécutifs corrigé}
	\textcolor{blue}{\hyperref[Obtenir trois pile consécutifs]{[Enoncé]}}\\
	
	\subsection{Lancer de dés équitable \etoile{3}}
	\label{Lancer de dés équitable corrigé}
		\textcolor{blue}{\hyperref[Lancer de dés équitable]{[Enoncé]}}\\
	Supposons que $X\sim\mathcal U(\crblanc{2}{12})$. Notons $D_1$ et $D_2$ les variables aléatoire qui donnent le résultat des dés $1$ et $2$ respectivement de sorte que $X=D_1+D_2$. D'après l'énoncé, $D_1$ et $D_2$ sont indépendantes. Fixons $t\in]-1,1[$.\\
	$G_X(t)=\displaystyle\sum_{k=2}^{12}\frac{1}{11}t^k=\frac{t^2}{11}\cdot\frac{t^{11}-1}{t-1}$. Or par indépendance, $G_X(t)=G_{D_1}(t)G_{D_2}(t)$.\\
	Notons $G_{D_1}(t)=\displaystyle\sum_{k=1}^6p_kt^k$ et $G_{D_1}(t)=\displaystyle\sum_{k=1}^6q_kt^k$.\\
	$\forall t\in ]-1,1[\backslash\{0\},\ G_X(t)=G_{D_1}(t)G_{D_2}(t)\implies \displaystyle\frac{1}{11}\cdot \frac{t^{11}-1}{t-1}=\left(\sum_{k=0}^5p_{k+1}t^k\right)\left(\sum_{k=0}^5q_{k+1}t^k\right)$.\\
	$\displaystyle\sum_{k=0}^5p_{k+1}t^k$ est un polynôme à coefficients réels de degré impair, il admet donc une racine réelle. Cependant, les racines de $G_X$ sont les racines $11$-ième de l'unité hormis $1$, il n'a donc aucune racine réelle.\\
	Ceci est absurde, par conséquent il est impossible de truquer les deux dés de manière à ce que $X\sim\mathcal U(\crblanc{2}{12})$.
	
	\subsection{Espérance conditionnelle \etoile{3}}
	\label{Espérance conditionnelle corrigé}
		\textcolor{blue}{\hyperref[Espérance conditionnelle]{[Enoncé]}}\\
	\begin{enumerate}
		\item $\forall x\in X(\Omega),\ \{X=x\}\cap A\subset \{X=x\}$.\\
		Donc $\forall x\in X(\Omega),\ |\p(\{X=x\}\cap A)|=\p(\{X=x\}\cap A)\leq \p(X=x)=|\p(X=x)|$.\\
		La famille $(x\p(X=x))_{x\in X(\Omega)}$ est sommable par hypothèse donc $(x\p_A(X=x))_{x\in X(\Omega)}=\left(\displaystyle\frac{x\p(X=x)}{\p(A)}\right)_{x\in X(\Omega)}$ est sommable.\\
		Fixons $x\in X(\Omega)\backslash\{0\}$. On sait que $\mathbbm{1}_A\sim\B(\p(A))$ donc $\forall \omega\in \Omega,\ (\mathbbm{1}_A\cdot X)(\omega)=1\iff \mathbbm{1}_A(\omega)X(\omega)=1\iff (\mathbbm{1}_A(\omega)=1\land X(\omega)=x)\iff \omega\in A\cap\{X=x\}$.\\
		Ainsi, $\forall x\in X(\Omega)\backslash\{0\},\ \p_A(X=x)=\displaystyle\frac{\p(\{X=x\}\cap A)}{\p(A)}=\frac{\p(\mathbbm{1}_A\cdot X=x)}{\p(A)}$.\\
		Et $\E(\mathbbm{1}_A\cdot X)=\displaystyle\sum\limits_{x\in X(\Omega)\cup\{0\}}x\p(\mathbbm{1}_A\cdot X=x)=\sum\limits_{x\in X(\Omega)\backslash\{0\}}x\p(\mathbbm{1}_A\cdot X=x)=\p(A)\sum\limits_{x\in X(\Omega)\backslash\{0\}}x\p_A(X=x)=\p(A)\E_A(X)$.\\\\
		C'est à dire, $\E_A(X)=\displaystyle\frac{\E(\mathbbm{1}_A\cdot X)}{\p(A)}$.
		\item D'après la formule des probabilités totales, $\forall x\in X(\Omega),\ \p(X=x)=\displaystyle\sum\limits_{i\in I}\p_{A_i}(X=x)\p(A_i)$.\\
		La famille $(x\p_{A_i}(X=x)\p(A_i))_{(x,i)\in X(\Omega)\times I}$ est sommable. En effet,\\
		$\displaystyle\sum\limits_{(x,i)\in X(\Omega)\times I}|x\p_{A_i}(X=x)\p(A_i)|=\sum\limits_{x\in X(\Omega)}|x|\sum\limits_{i\in I}\p_{A_i}(X=x)\p(A_i)=\sum\limits_{x\in X(\Omega)}|x\p(X=x)|<+\infty$ par hypothèse.\\\\
		Par conséquent d'après le théorème de Fubini,\\
		$\E(X)=\displaystyle\sum\limits_{x\in X(\Omega)}x\p(X=x)=\sum\limits_{x\in X(\Omega)}\sum\limits_{i\in I}x\p_{A_i}(X=x)\p(A_i)=\sum\limits_{i\in I}\p(A_i)\sum\limits_{x\in X(\Omega)}x\p_{A_i}(X=x)=\sum\limits_{i\in I}\E_{A_i}(X)\p(A_i)$.
	\end{enumerate}
	
	\subsection{Problème du collectionneur \etoile{2}}
	\label{Problème du collectionneur corrigé}
		\textcolor{blue}{\hyperref[Problème du collectionneur]{[Enoncé]}}\\
	\begin{enumerate}
		\item $X_{n,i}(\Omega)=\N^*$.\\
		$X_{n,i}$ est la variable aléatoire qui donne le rang du premier succès de l'expérience de Bernoulli "acheter une carte et regarder si on l'a déjà" lorsque que l'on la répète de manière indépendante. On suppose que la répartition des $n$ cartes est uniforme au cours de tous les achats donc la probabilité du succès de cette expérience aléatoire est $\displaystyle\frac{n-i+1}{n}$. Alors $X_{n,i}$ suit une loi géométrique de paramètre $\displaystyle\frac{n-i+1}{n}$.\\ On sait alors que \[\E(X_{n,i})=\displaystyle\frac{n}{n-i+1} \text{ et } \V(X)=\displaystyle\frac{1-\displaystyle\frac{n-i+1}{n}}{\left(\displaystyle\frac{n-i+1}{n}\right)^2}=\frac{n(i-1)}{(n-i+1)^2}\]. Ces expressions sont encore valables pour $i=1$.
		\item Pour tout $i\in \crblanc{1}{n}$, l'expérience aléatoire associée à $X_{n,i}$ n'est pas impactée par les valeurs prises par $X_{n,1},\dots,X_{n,{i-1}}$. On peut donc supposer que $X_{n,1},\dots,X_{n,n}$ sont mutuellement indépendantes.
		\item Par linéarité de l'espérance,\\
		$\E(T_n)=\displaystyle\sum\limits_{i=1}^n\E(X_{n,i})=n\sum\limits_{i=1}^n\frac{1}{n-i+1}=n\sum\limits_{i=1}^{n}\frac{1}{i}=nH_n$.\\
		Comme $X_{n,1},\dots,X_{n,n}$ sont mutuellement indépendantes,\\
		$\V(T_n)=\displaystyle\sum\limits_{i=1}^n\V(X_{n,i})=n\sum\limits_{i=1}^n\left(\frac{n}{(n-i+1)^2}-\frac{n-i+1}{(n-i+1)^2}\right)=n^2\sum\limits_{i=1}^n\frac{1}{i^2}-n\sum\limits_{i=1}^n\frac{1}{i}=n^2S_n-nH_n$.\\
		$\displaystyle\frac{1}{n}\unfty{\sim}\ln\left(1+\frac{1}{n}\right)=\ln(n+1)-\ln(n)$.\\
		$\displaystyle\sum\limits_{n\in \N^*}\frac{1}{n}$ diverge et $\forall n\in \N^*,\ \displaystyle\frac{1}{n}\geq 0$ donc d'après les théorèmes de comparaison pour des séries divergentes,\\
		$H_n\unfty{\sim}\displaystyle\sum\limits_{k=1}^{n}\left(\ln(k+1)-\ln(k)\right)=\ln(n+1)=\ln(n)+\ln\left(1+\frac{1}{n}\right)\unfty{\sim}\ln(n)$.\\
		$\displaystyle\sum\limits_{n=1}^{+\infty}\frac{1}{n^2}$ converge donc sa somme partielle est équivalente à sa somme $S=\displaystyle\frac{\pi^2}{6}$. (Il n'est pas nécessaire d'avoir la valeur de $S$)\\
		Ainsi, $\E(T_n)\unfty{\sim}n\ln n$ et $\V(T_n)\unfty{\sim} \displaystyle\frac{\pi^2}{6}n^2-n\ln n\unfty{\sim}\frac{\pi^2}{6}n^2$.
	\end{enumerate}
	
	\subsubsection{Généralisation}
	\begin{enumerate}[leftmargin=*]
		\item $\E(R_n)=\displaystyle\sum_{(x_1,\dots,x_n)\in X(\Omega)^n}\Card(\{x_1,\dots,x_n\})\p(X=x_1,\dots,X_n=x_n)=\sum_{(x_1,\dots,x_n)\in X(\Omega)^n}\Card(\{x_1,\dots,x_n\})\prod_{k=1}^n\p(X=x_k)\leq $
		\begin{align*}
			\E(R_{n+1})&=\displaystyle\sum_{(x_1,\dots,x_n,x_{n+1})\in \N^{n+1}}\Card(\{x_1,\dots,x_n,x_{n+1}\})\p(X_1=x_1,\dots,X_n=x_n,X_{n+1}=x_{n+1})\\
			&\leq \sum_{k=0}^{+\infty}\sum_{(x_1,\dots,x_n)\in \N^n}(\Card(\{x_1,\dots,x_n\})+1)\p(X_1=x_1,\dots,X_n=x_n)\p(X_{n+1}=k)\\
			&=\sum_{k=0}^{+\infty}\p(X_{n+1}=k)(\E(R_n)+1)\\
			&=\E(R_n)+1\\
			&\leq a+1+n\p(X\geq a)
		\end{align*}
		
	\end{enumerate}
	
	\subsection{Problème de la ruine du joueur}
	\label{Problème de la ruine du joueur corrigé}
	\textcolor{blue}{\hyperref[Problème de la ruine du joueur]{[Enoncé]}}\\
	
	\subsection{Passager d'un avion}
	\label{Passager d'un avion corrigé}
	\textcolor{blue}{\hyperref[Passager d'un avion]{[Enoncé]}}\\
	
	\subsection{Fonction de répartition \etoile{4}}
	\label{Fonction de répartition corrigé}
		\textcolor{blue}{\hyperref[Fonction de répartition]{[Enoncé]}}\\
	On sait que $X(\Omega)$ est au plus dénombrable.\\
	S'il est fini on note $X(\Omega)=\{x_1,\dots,x_n\}$.\\ 
	S'il est dénombrable on pose une bijection $\fonction{S}{\N}{X(\Omega)}{n}{s_n}$.
	\begin{enumerate}
		\item Soient $x,y\in \R$ tels que $x\leq y$.\\
		$\{X\leq x\}\subset \{X\leq y\}$ donc $F_X(x)=\p(X\leq x)\leq \p(X\leq y)=F_X(y)$.\\
		De plus, $\forall x\in \R,\ 0\leq F_X(x)\leq 1$. Donc d'après le théorème de la limite monotone $F_X$ admet des limites en $\pm\infty$.\\
		Déterminons les limites de $F_X$.\\
		\textbf{Si $X(\Omega)$ est fini :}\\
		On note $m=\min(X(\Omega))$ et $M=\max(X(\Omega))$.
		$(\forall x<m,\ F_X(x)=0)\implies \underset{x\to -\infty}{\lim}F_X(x)=0$.\\
		$(\forall x\geq M,\ F_X(x)=1)\implies \uxfty{\lim}F_X(x)=1$.\\
		\textbf{Si $X(\Omega)$ est dénombrable :}\\
		\underline{$1^{ère}$ méthode :} Continuité (dé)croissante.\\
		Notons pour $k\in \Z,\ A_k=\{X\leq k\}$. $(A_k)_{k\in \N}$ est une suite croissante d'évènement. Par continuité croissante,\\
		$\uxfty{\lim}F_X(x)=\unfty{\lim}\p(A_k)=\p\left(\displaystyle\bigcup\limits_{k\in \N}A_k\right)=\p(X\leq +\infty)=1$.\\
		Similairement, $(A_{-k})_{k\in \N}$ est une suite décroissante d'évènement. Par continuité décroissante,\\
		$\underset{x\to -\infty}{\lim}F_X(x)=\unfty{\lim}\p(A_{-k})=\p\left(\displaystyle\bigcap\limits_{k\in \N}A_{-k}\right)=\p(X\leq -\infty)=0$.\\\\
		\underline{$2^{nd}$ méthode :} Série de fonctions.\\
		On remarque que $\forall x\in \R,\ F_X(x)=\displaystyle\sum\limits_{s\in X(\Omega)}\p(X=s)\mathbbm{1}_{[s,+\infty[}(x)$. Si $x\in \R$, comme $(\p(X=s)\mathbbm{1}_{[s,+\infty[}(x))_{s\in X(\Omega)}$ est une famille positive on a $\displaystyle\sum\limits_{s\in X(\Omega)}\p(X=s)\mathbbm{1}_{[s,+\infty[}(x)=\sum\limits_{n\in \N}\p(X=s_n)\mathbbm{1}_{[s_n,+\infty[}(x)$.\\
		Posons pour $n\in \N,\ f_n=\p(X=s_n)\mathbbm{1}_{[s_n,+\infty[}$.\\
		\begin{itemize}
			\item $\displaystyle\sum\limits_{n\in \N}f_n$ converge normalement, a fortiori uniformément sur $\R$.\\
			En effet, $\displaystyle\sum\limits_{n=0}^{+\infty}{||f_n||}_\infty=\sum\limits_{n=0}^{+\infty}\p(X=s_n)=1$;
			\item $\forall n\in \N,\ f_n(x)\uxfty{\longrightarrow}\p(X=s_n)\in \R$;
			\item $\forall n\in \N,\ f_n(x)\underset{x\to -\infty}{\longrightarrow}0\in \R$.
		\end{itemize}
		Donc d'après le théorème d'interversion série-limite, $\uxfty{\lim}F_X(x)=\displaystyle\sum\limits_{n=0}^{+\infty}\p(X=s_n)=1$ et $\underset{x\to -\infty}{\lim}F_X(x)=\displaystyle\sum\limits_{n=0}^{+\infty}0=0$.\\\\
		\underline{$3^{ème}$ méthode :} Par la définition\\
		On remarque que $\forall x\in \R,\ F_X(x)=\displaystyle\sum\limits_{s\in X(\Omega)}\p(X=s)\mathbbm{1}_{[s,+\infty[}(x)$. Soit $\varepsilon\in \R^*_+$.\\
		$\displaystyle\sum\limits_{n=0}^{+\infty}\p(X=s_n)=1$ et $\left(\displaystyle\sum\limits_{n=0}^{N}\p(X=s_n)\right)_{N\in \N}$ est croissante donc $\exists N\in \N,\ \forall n>N,\ \displaystyle\sum\limits_{n=0}^{N}\p(X=s_n)\geq 1-\varepsilon$.\\\\
		Soit $x\geq \max\{s_n,\ n\in \crblanc{0}{N}\}$.\\
		$(\forall n\in \crblanc{0}{N},\ \mathbbm{1}_{[s_n,+\infty[}(x)=1)\implies\displaystyle\sum\limits_{n=0}^{+\infty}\p(X=s_n)\mathbbm{1}_{[s_n,+\infty[}(x)\geq \sum\limits_{n=0}^{N}\p(X=s_n)\geq 1-\varepsilon$\\
		De plus, $\forall x\in \R,\ \displaystyle\sum\limits_{n=0}^{+\infty}\p(X=s_n)\mathbbm{1}_{[s_n,+\infty[}(x)\leq \sum\limits_{n=0}^{+\infty}\p(X=s_n)=1$\\
		Ainsi, $\uxfty{\lim}F_X(x)=1$.\\
		Ensuite, $\displaystyle\sum\limits_{n\in \N}\p(X=s_n)$ converge donc la suite des restes $(R_p)_{p\in \N}=\left(\displaystyle\sum\limits_{n=p+1}^{+\infty}\p(X=s_n)\right)_{p\in \N}$ converge vers $0$. Alors, $\exists M\in \N,\ \forall n>M,\ |R_p|=R_p\leq \varepsilon$.\\\\
		Soit $x\leq \min\{s_n,\ n\in \crblanc{0}{M}\}$.\\
		$(\forall n\in \crblanc{0}{M},\ \mathbbm{1}_{[s_n,+\infty[}(x)=0)\implies\displaystyle\sum\limits_{n=0}^{+\infty}\p(X=s_n)\mathbbm{1}_{[s_n,+\infty[}(x)\leq R_M\leq \varepsilon$.\\
		Ainsi, $\underset{x\to -\infty}{\lim}F_X(x)=0$.
		\item \textbf{Si $X(\Omega)$ est fini :}\\
		$F_X=\displaystyle\sum\limits_{k=1}^n\p(X=x_k)\mathbbm{1}_{[x_k,+\infty[}$ donc $F_X$ est continue à droite en tout point comme somme (finie) de fonctions continues à droite en tout point.\\
		\textbf{Si $X(\Omega)$ est dénombrable :}\\
		\underline{$1^{ère}$ méthode :} Continuité (dé)croissante.\\
		Soient $x\in \R$ et $(x_n)\in (]x,+\infty[)^\N$ convergeant vers $x$.\\
		Montrons le lemme suivant : \underline{D'une suite réelle peut toujours être extraite une suite monotone}.\\\\
		Fixons $(u_n)\in \R^\N$ et posons $A=\{n\in \N|\forall k>n,\ u_k\geq u_n\}$. Alors,\\
		soit $A$ est infini et la suite $(u_n)_{n\in A}$ est une suite extraite de $(u_n)$ croissante,\\
		soit $A$ est fini et il existe donc $N\in \N,\ \forall n\geq N,\ n\notin A$. On construit alors l'extractrice par récurrence : $\varphi(0)=N$ puis, si $N=\varphi(0)<\dots<\varphi(n)$ sont bien définis alors $\varphi(n)\notin A$ donne l'existence de $k>\varphi(n)$ tel que $u_k<u_{\varphi(n)}$. On pose donc $\varphi(n+1)=k$. Par définition, $(u_{\varphi(n)})_{n\in \N}$ est décroissante.\\\\
		On applique le lemme a $(x_n)$ pour extraire une suite $(y_n)$ de $(x_n)$ monotone. On remarque que si $(y_n)$ est croissante alors comme elle converge vers $x$ en tant que suite extraite, $\forall n\in \N,\ y_n\leq x$. Or par définition de $(x_n)$, $\forall n\in \N,\ y_n>x$. Ceci est absurde par conséquent $(y_n)$ est décroissante.\\
		Mais alors $(\{X\leq y_n\})_{n\in \N}$ est une suite décroissante d'évènements d'où par continuité décroissante,\\
		$\unfty{\lim}F_X(x_n)=\unfty{\lim}F_X(y_n)=\unfty{\lim}\p(\{X\leq y_n\})=\p\left(\displaystyle\bigcap\limits_{k\in \N}\{X\leq y_k\}\right)=\p(X\leq x)=F_X(x)$.\\
		$F_X$ est continue à droite en $x$ par caractérisation séquentielle.\\\\
		\underline{$2^{nd}$ méthode :} Série de fonctions.\\
		On réutilise l'expression $F_X=\displaystyle\sum\limits_{n\in \N}\p(X=s_n)\mathbbm{1}_{[s_n,+\infty[}$. On a déjà montré la convergence normale, et donc uniforme, sur $\R$ de la série $\displaystyle\sum\limits_{n\in \N}\p(X=s_n)\mathbbm{1}_{[s_n,+\infty[}$. De plus, quel que soit $n\in \N,\ \p(X=s_n)\mathbbm{1}_{[s_n,+\infty[}$ est continue à droite en tout point de $\R$. Donc par théorème de transfert, $F_X$ est continue à droite en tout point de $\R$.
		\item \textbf{Si $X(\Omega)$ est fini :}\\
		$F_X=\displaystyle\sum\limits_{k=1}^n\p(X=x_k)\mathbbm{1}_{[x_k,+\infty[}$ donc $F_X$ est continue sur $\R\backslash X(\Omega)$ comme somme (finie) de fonctions continues sur $\R\backslash X(\Omega)$, et $F_X$ est continue en un des points $x_k,\ k\in\crblanc{1}{n}$ si et seulement si $\p(X=x=k)=0$.\\
		\textbf{Si $X(\Omega)$ est dénombrable :}\\
		\underline{$1^{ère}$ méthode :} Continuité (dé)croissante.\\
		Soient $a\in \R$ et $(a_n)\in (]-\infty,x[)^\N$ convergeant vers $a$.\\
		On applique le lemme démontré précédemment à $(a_n)$ pour extraire une suite $(b_n)$ de $(a_n)$ monotone. On remarque pour des raisons analogues à celles de la question précédente que $(b_n)$ est croissante.\\
		Mais alors $(\{X\leq b_n\})_{n\in \N}$ est une suite croissante d'évènements d'où par continuité croissante,\\
		$\unfty{\lim}F_X(a_n)=\unfty{\lim}F_X(b_n)=\unfty{\lim}\p(\{X\leq b_n\})=\p\left(\displaystyle\bigcup\limits_{k\in \N}\{X\leq b_k\}\right)=\p(X<a)=F_X(a)-\p(X=a)$.\\
		Ainsi, $F_X$ est continue en $a$ si et seulement si $\p(X=a)=0$ c'est à dire si et seulement si l'évènement $\{X=a\}$ est négligeable.
	\end{enumerate}
	
	\subsection{Loi sans mémoire \etoile{2}}
	\label{Loi sans mémoire corrigé}
		\textcolor{blue}{\hyperref[Loi sans mémoire]{[Enoncé]}}\\
	Supposons que $X\sim \mathcal{G}(p)$ pour un certain $p\in ]0,1[$. On note $q=1-p$.\\ Alors $\forall m\in \N,\ \p(X>m)=\displaystyle\sum\limits_{i=m+1}^{+\infty}pq^{i-1}=p\cdot\frac{q^m}{1-q}=q^m$.\\
	Fixons $(n,k)\in \N^2$.\\
	$\p(X>n+k|X>n)=\displaystyle\frac{\p(X>n+k,X>n)}{\p(X>n)}=\frac{\p(X>n+k)}{\p(X>n)}=\frac{q^{n+k}}{q^m}=q^k=\p(X>k)$.\\
	Réciproquement, supposons que $\forall (n,k)\in\N^2, \ \p(X>n+k|X>n)=\p(X>k)$. Fixons $n\in \N$.\\
	$\p(X>n+1)=\p(X>n+1,X>n)=\p(X>n)\p(X>n+1|X>n)=\p(X>n)\p(X>1)$.\\
	Ainsi, $(\p(X>n))_{n\in \N^*}$ est une suite géométrique de raison $\p(X>1)$ : $X\sim \mathcal{G}(\p(X>1))$.
	
	\subsection{Caractérisation de la loi de Poisson par l'espérance}
	\label{Caractérisation de la loi de Poisson par l'espérance corrigé}
	\textcolor{blue}{\hyperref[Caractérisation de la loi de Poisson par l'espérance]{[Enoncé]}}\\
	
	\subsection{Loi Zéta \etoile{2}}
	\label{Loi Zéta corrigé}
		\textcolor{blue}{\hyperref[Loi Zéta]{[Enoncé]}}\\
	\begin{enumerate}
		\item On doit avoir $\displaystyle\sum\limits_{n=1}^{+\infty}\p(X=n)=1$ donc $\lambda\displaystyle\sum\limits_{n=1}^{+\infty}\frac{1}{n^s}=1$ c'est à dire $\lambda=\displaystyle\frac{1}{\zeta(s)}$.
		\item Soit $n\in \N^*$.\\
		$\p(n|X)=\p(X\in n\N^*)=\displaystyle\p\left(X\in \bigsqcup\limits_{m\in \N^*}\{mn\}\right)=\sum\limits_{m=1}^{+\infty}\p(X=mn)=\zeta(s)^{-1}\sum\limits_{m=1}^{+\infty}\frac{1}{(mn)^s}=\frac{\zeta(s)^{-1}}{n^s}\sum\limits_{m=1}^{+\infty}\frac{1}{m^s}=\frac{1}{n^s}$.
		\item Fixons $q\in \N^*$. Soient $i_1,\dots,i_q\in \N^*$ distincts.\\
		D'après le lemme d'Euclide, $\forall k\in \N^*,\ (\forall j\in \crblanc{1}{q},\ p_{i_j}^{\alpha_{i_j}}|k)\iff \displaystyle\prod\limits_{j=1}^qp_{i_j}^{\alpha_{i_j}}|k$.\\
		Donc $\displaystyle\bigcap\limits_{j=1}^q\left\{p_{i_j}^{\alpha_{i_j}}|X\right\}=\left\{\prod\limits_{j=1}^qp_{i_j}^{\alpha_{i_j}}|X\right\}$.\\
		On en déduit d'après la question précédente que $\displaystyle\p\left(\bigcap\limits_{j=1}^q\left\{p_{i_j}^{\alpha_{i_j}}|X\right\}\right)=\left(\prod\limits_{j=1}^qp_{i_j}^{\alpha_{i_j}}\right)^{-s}=\prod\limits_{j=1}^q\left(p_{i_j}^{\alpha_{i_j}}\right)^{-s}=\prod\limits_{j=1}^q\p\left(p_{i_j}^{\alpha_{i_j}}|X\right)$.
		\item Les évènements $\{p_1|X\},\dots,\{p_r|X\}$ sont mutuellement indépendants donc,\\
		$\displaystyle\p\left(\bigcap\limits_{i=1}^r\{p_i\nmid X\}\right)=\prod\limits_{i=1}^r\p\left(\overline{p_i|X}\right)=\prod\limits_{i=1}^r\left(1-p_i^{-s}\right)$.
		\item Par continuité décroissante $\zeta(s)^{-1}=\p(X=1)=\displaystyle\p\left(\bigcap\limits_{k=1}^{+\infty}\{p_k\nmid X\}\right)=\unfty{\lim}\p\left(\bigcap\limits_{k=1}^{n}\{p_k\nmid X\}\right)$.\\
		Et donc d'après la question précédente, $\zeta(s)^{-1}=\unfty{\lim}\displaystyle\prod\limits_{k=1}^{n}\p\left(p_k\nmid X\right)=\unfty{\lim}\prod\limits_{k=1}^{n}\left(1-p_k^{-s}\right)$.
		\item La famille $\left(\displaystyle\frac{1}{p}\right)_{p\in \mathcal{P}}$ est positive donc, dans $\R\cup\{+\infty\}$, $\displaystyle\sum\limits_{p\in \mathcal{P}}\frac{1}{p}=\sum\limits_{n=1}^{+\infty}\frac{1}{p_n}$.\\
		De plus, comme $p_n\unfty{\longrightarrow}+\infty,\ \displaystyle\frac{1}{p_n}\unfty{\sim}-\ln\left(1-\frac{1}{p_n}\right)$ et $\left(\displaystyle\frac{1}{p_n}\right)_{n\in \N^*}$ est positive APCR. Donc $\displaystyle\sum\limits_{n\in \N^*}\frac{1}{p_n}$ et $\displaystyle\sum\limits_{n\in \N^*}-\ln\left(1-\frac{1}{p_n}\right)$ sont de même nature.\\
		Supposons que $\left(\displaystyle\frac{1}{p}\right)_{p\in \mathcal{P}}$ est sommable. Alors d'après ce qui a été fait précédemment, $\displaystyle\sum\limits_{n\in \N^*}-\ln\left(1-\frac{1}{p_n}\right)$ converge.\\
		$\forall N\in \N^*,\ \displaystyle\sum\limits_{n=1}^N\ln\left(1-\frac{1}{p_n}\right)=\ln\left(\prod\limits_{n=1}^{N}\left(1-\frac{1}{p_n}\right)\right)$.\\
		Donc la suite $(u_N)_{N\in \N^*}$ définit par $\forall N\in \N^*,\ u_n=\displaystyle\prod\limits_{n=1}^{N}\left(1-\frac{1}{p_n}\right)$ converge. Notons $l$ sa limite et fixons $N\in \N^*$. $\forall n\in \crblanc{1}{N},\ 0\leq \displaystyle\frac{1}{p_n}<1$. Donc, $s>1\implies \forall n\in \crblanc{1}{N},\ 1>\displaystyle\frac{1}{p_n}>\frac{1}{p_n^s}\geq 0\implies 0<1-\frac{1}{p_n}<1-p_n^{-s}\implies 0<u_N<\prod\limits_{n=1}^N\left(1-p_n^{-s}\right)$.\\
		Ainsi, par passage à la limite, $0\leq l\leq \zeta(s)^{-1}$.\\
		On montre alors classiquement que $\underset{s\to 1^+}{\lim}\zeta(s)=+\infty$ pour conclure que $l=0$ ce qui est absurde car alors $(\ln(u_n))_{n\in \N^*}=\displaystyle\sum\limits_{n\in \N}\ln\left(1-\frac{1}{p_n}\right)$ diverge :\\
		$t\mapsto t^{-s}$ est continue positive, décroissante sur $[1,+\infty[$ et intégrable en $+\infty$ donc,\\
		$$\left(\forall n\in \N^*,\ \int_n^{n+1}t^{-s}dt\leq \frac{1}{n^s}\right)\land\left(\forall n\in \llbracket2;+\infty\llbracket,\ \frac{1}{n^s}\leq \int_{n-1}^n t^{-s}dt\right)$$
		puis en sommant dans les deux inégalités,\\
		$$\int_1^{+\infty}t^{-s}dt\leq \zeta(s)\leq 1+\int_{1}^{+\infty}t^{-s}dt$$
		c'est à dire,\\
		$$\frac{1}{s-1}\leq \zeta(s)\leq 1+\frac{1}{s-1}$$
		Or, $\displaystyle\frac{1}{s-1}\underset{s\to 1^+}{\sim}1+\frac{1}{s-1}$ donc $\zeta(s)\underset{s\to 1^+}{\sim}\displaystyle\frac{1}{s-1}$ d'où $\underset{s\to 1^+}{\lim}\zeta(s)=+\infty$.
	\end{enumerate}
	
	\subsection{Taux de panne \etoile{2}}
	\label{Taux de panne corrigé}
		\textcolor{blue}{\hyperref[Taux de panne]{[Enoncé]}}\\
	\begin{enumerate}
		\item $\displaystyle\sum\limits_{n=1}^{+\infty}\frac{1}{n(n+1)}=\sum\limits_{n=1}^{+\infty}\left(\frac{1}{n}-\frac{1}{n+1}\right)=1$ et $\forall n\in \N^*,\ \displaystyle\frac{1}{n(n+1)}\geq 0$ : $\left(\displaystyle\frac{1}{n(n+1)}\right)_{n\in \N^*}$ définit une loi de probabilité. Fixons $n\in \N^*$.\\
		$\p(Y=n|Y\geq n)=\displaystyle\frac{\p(Y=n,Y\geq n)}{\p(Y\geq n)}=\frac{\p(Y=n)}{\displaystyle\sum\limits_{k=n}^{+\infty}\p(Y=k)}=\frac{\displaystyle\frac{1}{n(n+1)}}{\displaystyle\frac{1}{n}}=\frac{1}{n+1}$.
		\item Soit $n\in \llbracket2;+\infty\llbracket$. D'après la formule des probabilités totales,\\
		$\forall k\in \crblanc{1}{n-1},\ 1-x_k=1-\displaystyle\frac{\p(X=k)}{\p(X\geq k)}=\frac{\p(X\geq k+1)}{\p(X\geq k)}$.\\
		Par conséquent, $\p(X\geq n)=\displaystyle\frac{\p(X\geq n)}{\p(X\geq 1)}=\prod\limits_{k=1}^{n-1}(1-x_k)$.
		\item $\forall n\in \llbracket2;+\infty\llbracket,\ \p(X=n)=\p(X\geq n)-\p(X\geq n+1)=\displaystyle\prod\limits_{k=1}^{n-1}(1-x_k)-\prod\limits_{k=1}^n(1-x_k)=(1-(1-x_n))\prod\limits_{k=1}^{n-1}(1-x_k)=x_n\prod\limits_{k=1}^{n-1}(1-x_k)$.\\
		Cette expression est encore valable pour $n=1$ puisque $\p(X=1)=\p(X=1|X\geq 1)=x_1$.
		\item Soit $X$ une variable aléatoire à valeur dans $\N^*$ à taux de panne constant : $\forall n\in \N^*,\ x_n=x\in ]0,1[$.\\
		Alors, $\forall n\in \N^*,\ p(X=n)=x(1-x)^{n-1} : X\sim\mathcal{G}(x)$.\\
		Réciproquement, supposons que $X\sim \mathcal{G}(x)$ pour un certain $x\in ]0,1[$. Fixons $n\in \N^*$\\
		Alors $\p(X\geq n)=\displaystyle\sum\limits_{k=n}^{+\infty}x(1-x)^{k-1}=x\cdot\frac{(1-x)^{n-1}}{1-(1-x)}=(1-x)^{n-1}$.\\
		On en déduit $x_n=\displaystyle\frac{\p(X=n)}{\p(X\geq n)}=\frac{x(1-x)^{n-1}}{(1-x)^{n-1}}=x$.
	\end{enumerate}
	
	\subsection{Matrice aléatoire (1) \etoile{2}}
	\label{Matrice aléatoire 1 corrigé}
		\textcolor{blue}{\hyperref[Matrice aléatoire 1]{[Enoncé]}}\\
	\begin{enumerate}
		\item $\det(A)=X_1X_2$.\\
		Ainsi, $\p(\det(A)=0)=\p(X_1X_2=0)=\p(\{X_1=0\}\cup\{X_2=0\})\leq \p(X_1=0)+\p(X_2=0)=0$.\\
		D'où $\p(\det(A)=0)=0 : A$ est presque sûrement inversible.
		\item Soit $\omega\in \Omega$. Si $X_1(\omega)=X_2(\omega)=x$ alors $A(\omega)=\begin{pmatrix}
			x &1\\
			0 & x
		\end{pmatrix}$ n'est pas diagonalisable puisque que son spectre ne contient que $x$ et qu'elle est différente de $x\text{I}_2$.\\
		Et si $X_1(\omega)\ne X_2(\omega)$ alors $A(\omega)$ est diagonalisable puisque à spectre simple : $\operatorname{Sp}(A(\omega))=\{X_1(\omega),X_2(\omega)\}$.\\
		Ainsi si l'on note $D$ l'évènement "$A$ est diagonalisable" on a,\\
		$\p(\overline{D})=\p(X_1=X_2)=\displaystyle\sum\limits_{n=1}^{+\infty}\p(X_1=n,X_2=n)=\sum\limits_{n=1}^{+\infty}\p(X_1=n)\p(X_2=n)=p^2\sum\limits_{n=1}^{+\infty}\left((1-p)^2\right)^{n-1}=\frac{p^2}{1-(1-p)^2}$\\
		Et donc $\p(D)=1-\p(\overline{D})=1-\displaystyle\frac{p^2}{p(2-p)}=\frac{2p(1-p)}{p(2-p)}=\frac{2(1-p)}{2-p}$.
	\end{enumerate}
	
	\subsection{Matrice aléatoire (2)}
		\label{Matrice aléatoire 2 corrigé}
	\textcolor{blue}{\hyperref[Matrice aléatoire 2]{[Enoncé]}}\\
	\subsection{Matrice aléatoire (3)}
		\label{Matrice aléatoire 3 corrigé}
	\textcolor{blue}{\hyperref[Matrice aléatoire 3]{[Enoncé]}}\\
	\subsection{Matrice aléatoire (4)}
		\label{Matrice aléatoire 4 corrigé}
	\textcolor{blue}{\hyperref[Matrice aléatoire 4]{[Enoncé]}}\\
	\subsection{Matrice aléatoire (5)}
		\label{Matrice aléatoire 5 corrigé}
	\textcolor{blue}{\hyperref[Matrice aléatoire 5]{[Enoncé]}}\\
	\subsection{Matrice aléatoire (6)}
		\label{Matrice aléatoire 6 corrigé}
	\textcolor{blue}{\hyperref[Matrice aléatoire 6]{[Enoncé]}}\\
	\subsection{Matrice aléatoire (7)}
		\label{Matrice aléatoire 7 corrigé}
	\textcolor{blue}{\hyperref[Matrice aléatoire 7]{[Enoncé]}}\\
	
	\subsection{Matrice de Rademacher}
	\label{Matrice de Rademacher corrigé}
	\subsection{Vecteur propre aléatoire}
	\label{Vecteur propre aléatoire corrigé}
	\subsection{Equation différentielle à coefficients aléatoires}
	\label{Equation différentielle à coefficients aléatoires corrigé}
	\textcolor{blue}{\hyperref[Equation différentielle à coefficients aléatoires]{[Enoncé]}}\\
	\subsection{Série entière aléatoire}
	\label{Série entière aléatoire corrigé}
	\textcolor{blue}{\hyperref[Série entière aléatoire]{[Enoncé]}}\\
	\subsection{Permutation aléatoire}
	\label{Permutation aléatoire corrigé}
	\textcolor{blue}{\hyperref[Permutation aléatoire]{[Enoncé]}}\\
	\subsection{Permutations composées d'un grand cycle}
	\label{Permutations composées d'un grand cycle corrigé}
	\textcolor{blue}{\hyperref[Permutations composées d'un grand cycle]{[Enoncé]}}\\
	\subsection{Loi conjointe (1) \etoile{2}}
	\label{Loi conjointe 1 corrigé}
		\textcolor{blue}{\hyperref[Loi conjointe 1]{[Enoncé]}}\\
	\begin{enumerate}
		\item On doit avoir $\displaystyle\sum\limits_{(n,k)\in \N^2}\p(X=k,Y=n)=1$.\\
		On applique le théorème de Fubini pour une famille positive,\\
		$\displaystyle\sum\limits_{(n,k)\in \N^2}\p(X=k,Y=n)=\sum\limits_{n=0}^{+\infty}\sum\limits_{k=0}^{+\infty}\p(X=k,Y=n)=\sum\limits_{n=0}^{+\infty}a^np(1-p)^n\sum\limits_{k=0}^{n}\binom{n}{k}1^k\cdot1^{n-k}=p\sum\limits_{n=0}^{+\infty}(2a(1-p))^n=\frac{p}{1-2a(1-p)}$.\\\\
		Donc $a=\displaystyle\frac{1}{2}$.
		\item Soit $n\in \N$.\\
		D'après la formule des probabilités totales,\\
		$\p(Y=n)=\displaystyle\sum\limits_{k=0}^{+\infty}\p(X=k,Y=n)=p(1-p)^n\frac{1}{2^n}\sum\limits_{k=0}^{n}\binom{n}{k}=p(1-p)^n$.\\
		$Y+1\sim\mathcal{G}(p)$.
		\item Soit $k\in \N$.\\
		D'après la formule des probabilités totales,\\
		$\p(X=k)=\displaystyle\sum\limits_{n=0}^{+\infty}\p(X=k,Y=n)=p\sum\limits_{n=k}^{+\infty}\binom{n}{k}\left(\frac{1-p}{2}\right)^n=p\left(\frac{1-p}{2}\right)^k\sum\limits_{n=k}^{+\infty}\binom{n}{k}\left(\frac{1-p}{2}\right)^{n-k}=\frac{p\left(\displaystyle\frac{1-p}{2}\right)^k}{\left(1-\displaystyle\frac{1-p}{2}\right)^{k+1}}$.\\
		C'est à dire, $\p(X=k)=\displaystyle\frac{2p}{1+p}\left(\frac{1-p}{1+p}\right)^k=\frac{2p}{1+p}\left(1-\frac{2p}{1+p}\right)^k$.\\
		$X+1\sim\mathcal{G}\left(\displaystyle\frac{2p}{1+p}\right)$.
		\item $X$ et $Y$ ne sont pas indépendantes. En effet,\\
		$\p(X=1,Y=0)=0\ne \p(X=1)\p(Y=0)$.
	\end{enumerate}
	
	\subsection{Loi conjointe (2) \etoile{3}}
	\label{Loi conjointe 2 corrigé}
		\textcolor{blue}{\hyperref[Loi conjointe 2]{[Enoncé]}}\\
	\begin{enumerate}
		\item On doit avoir $\displaystyle\sum\limits_{0\leq i,j\leq n}\p(X=i,Y=j)=1$.\\
		Or, $\displaystyle\sum\limits_{0\leq i,j\leq n}\p(X=i,Y=j)=\lambda\sum\limits_{i=0}^n\binom{n}{i}\sum\limits_{j=0}^n\binom{n}{j}=\lambda\left(\sum\limits_{k=0}^n\binom{n}{k}1^k\cdot 1^{n-k}\right)^2=\lambda\left((1+1)^n\right)^2=\lambda 4^n$.\\
		Donc $\lambda=\displaystyle\frac{1}{4^n}$.
		\item Soit $i\in \crblanc{0}{n}$. D'après la formule des probabilités totales,\\
		$\p(X=i)=\displaystyle\sum\limits_{j=0}^n\p(X=i,Y=j)=\frac{1}{4^n}\binom{n}{i}\sum\limits_{j=0}^n\binom{n}{j}=\frac{1}{2^n}\binom{n}{i}$.\\
		Par symétrie, $\forall j\in \crblanc{0}{n},\ \p(Y=j)=\displaystyle\frac{1}{2^n}\binom{n}{j}$.
		\item $\forall (i,j)\in \crblanc{0}{n}^2,\ \p(X=i,Y=j)=\p(X=i)\p(Y=j)\implies X\independent Y$.
		\item Notons $C=\left(\displaystyle\frac{1}{2^n}\binom{n}{j}\right)_{0\leq j\leq n}\in \M_{n,1}(\R)$.\\\\\\
		$B=
		\begin{pmatrix}
			\displaystyle\frac{1}{2^n}\binom{n}{0}C&\left|\displaystyle\frac{1}{2^n}\binom{n}{1}C\right|&\dots&\left|\displaystyle\frac{1}{2^n}\binom{n}{i}C\right|&\dots&\left|\displaystyle\frac{1}{2^n}\binom{n}{n-1}C\right|&\displaystyle\frac{1}{2^n}\binom{n}{n}C   
		\end{pmatrix}=CC^\top$.\\\\
		Donc $\forall p\in \N^*,\ B^p=\left(CC^\top\right)^p=C\left(C^\top C\right)^{p-1}C^\top=\left(C^\top C\right)^{p-1}CC^\top=\left(C^\top C\right)^{p-1}B$. $\left(\left(C^\top C\right)^{p-1}\text{ est un scalaire}\right)$.
		Et on calcule $C^\top C=\Tr(C^\top C)=\Tr(CC^\top)=\Tr(B)=\displaystyle\frac{1}{4^n}\sum\limits_{k=0}^n\binom{n}{k}^2=\frac{1}{4^n}\binom{2n}{n}$. \textit{(la dernière égalité provient de l'identité de Vandermonde et n'est pas nécessaire)}\\
		Ainsi, $B^p=\Tr(B)^{p-1}B$.
		\item $X^2-\Tr(B)X=X(X-\Tr(B))$ annule $B$ et est scindé à racines simples donc $B$ est diagonalisable et $\operatorname{Sp}(B)=\{0,\Tr(B)\}$.\\
		$\rg(B)=1$ donc on sait que $\dim(\text{Ker}(B))=n-1$ et $\dim(\text{Ker}(B-\Tr(B)\text{I}_n))=1$.\\
		On remarque que $BC=C\left(C^\top C\right)=\Tr(B)C$ et que pour tout $j\in \crblanc{2}{n},\ BX_j=0$ avec $X_j=\begin{pmatrix} 1\\ 0\\ \vdots\\ 0\\ -1\\ 0\\ \vdots\\ 0\end{pmatrix}$ où le coefficient $-1$ est en $j$-ième position.\\
		Ainsi, $\text{Ker}(B)=\text{Vect}(C)$ et $\text{Ker}(B-\Tr(B)I_n)=\text{Vect}(X_2,\dots,X_n)$.
	\end{enumerate}
	
	\subsection{Fonction caractéristique \etoile{2}}
	\label{Fonction caractéristique corrigé}
		\textcolor{blue}{\hyperref[Fonction caractéristique]{[Enoncé]}}\\
	\begin{enumerate}
		\item Si $t\in \R$, la famille $\left(\p(X=k)e^{itk}\right)_{k\in \Z}$ est sommable puisque $\displaystyle\sum\limits_{k\in \Z}\left|\p(X=k)e^{itk}\right|=\sum\limits_{k\in \Z}\p(X=k)=1<+\infty$. Ainsi $\varphi_X$ est définie sur $\R$. $\varphi_X$ est clairement $2\pi$-périodique.\\
		Notons pour $k\in \Z,\ f_k:t\in \R\mapsto \p(X=k)e^{itk}$\\
		Les série de fonctions $\displaystyle\sum\limits_{k\in \N}f_k$ et $\displaystyle\sum\limits_{k\in \N^*}f_{-k}$ convergent normalement, a fortiori uniformément, sur $\R$ puisque $\displaystyle\sum\limits_{k\in \N}{||f_k||}_\infty=\displaystyle\sum\limits_{k\in \N}\p(X=k)\leq 1<+\infty$ et $\displaystyle\sum\limits_{k\in \N}{||f_{-k}||}_\infty=\displaystyle\sum\limits_{k\in \N^*}\p(X=-k)\leq 1<+\infty$.\\
		Les fonctions $f_k,\ k\in \Z$ étant toutes continues sur $\R$, par transfert de continuité, $\varphi_X=\displaystyle\sum\limits_{k\in \N}f_k+\displaystyle\sum\limits_{k\in \N^*}f_{-k}$ est continue sur $\R$.
		\item Pour tout $k\in \Z\ f_k$ est $\mathcal{C}^1$ sur $\R$ et, $\forall t\in \R,\ f_k'(t)=ik\p(X=k)e^{itk}$.\\
		Encore une fois les séries $\displaystyle\sum\limits_{k\in \N}f_k'$ et $\displaystyle\sum\limits_{k\in \N^*}f_{-k}'$ convergent normalement, a fortiori uniformément, sur $\R$ puisque $\displaystyle\sum\limits_{k\in \N}{||f_k'||}_\infty=\displaystyle\sum\limits_{k\in \N}k\p(X=k)\leq \E(X)<+\infty$ et $\displaystyle\sum\limits_{k\in \N}{||f_{-k}'||}_\infty=\displaystyle\sum\limits_{k\in \N^*}-k\p(X=-k)\leq \E(X)<+\infty$.\\
		Donc par théorème de transfert $\mathcal{C}^1$, $\varphi_X$ est $\mathcal{C}^1$ sur $\R$ et,\\
		$\forall t\in \R,\ \varphi_X'(t)=\displaystyle\sum\limits_{k\in \N}f_k'(t)+\displaystyle\sum\limits_{k\in \N^*}f_{-k}'(t)=\sum\limits_{k\in \Z}ik\p(X=k)e^{itk}$.\\\\\\
		On en déduit que $\E(X)=\displaystyle\frac{\varphi_X'(0)}{i}=-i\varphi_X'(0)$.
		\item Pour tout $k\in \Z\ f_k$ est $\mathcal{C}^2$ sur $\R$ et, $\forall t\in \R\ f_k''(t)=-k^2\p(X=k)e^{itk}$.\\
		Encore une fois les séries $\displaystyle\sum\limits_{k\in \N}f_k''$ et $\displaystyle\sum\limits_{k\in \N^*}f_{-k}''$ convergent normalement, a fortiori uniformément, sur $\R$ puisque $\displaystyle\sum\limits_{k\in \N}{||f_k''||}_\infty=\displaystyle\sum\limits_{k\in \N}k^2\p(X=k)\leq \E(X^2)<+\infty$ et $\displaystyle\sum\limits_{k\in \N}{||f_{-k}''||}_\infty=\displaystyle\sum\limits_{k\in \N^*}(-k)^2\p(X=-k)\leq \E(X^2)<+\infty$.\\
		Donc par théorème de transfert $\mathcal{C}^2$, $\varphi_X$ est $\mathcal{C}^2$ sur $\R$ et,\\
		$\forall t\in \R,\ \varphi_X''(t)=\displaystyle\sum\limits_{k\in \N}f_k''(t)+\displaystyle\sum\limits_{k\in \N^*}f_{-k}''(t)=\sum\limits_{k\in \Z}-k^2\p(X=k)e^{itk}$.\\\\\\
		On en déduit que $\V(X)=\E(X^2)-\E(X)^2=\displaystyle\sum\limits_{k\in \Z}k^2\p(X=k)-\left(\frac{\varphi_X'(0)}{i}\right)^2=\varphi_X'(0)^2-\varphi_X''(0)$.
	\end{enumerate}
	
	\subsection{Fonction génératrice des moments \etoile{5}}
	\label{Fonction génératrice des moments corrigé}
		\textcolor{blue}{\hyperref[Fonction génératrice des moments]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $\E(e^{0\cdot X})=\E(1)=1<+\infty$. Donc $0\in I_X$. Si $I_X=\{0\}$ alors c'est un intervalle. Supposons que $I_X$ ne soit pas réduit à $\{0\}$.\\
		Soient $a,b\in I_X$ tels que $a<b$ et soit $t\in [a,b]$.\\
		$\forall \omega\in \Omega,\ X(\omega)\geq 0\implies aX(\omega)\leq tX(\omega)\leq bX(\omega)\implies e^{tX(\omega)}\leq e^{bX(\omega)}$.\\
		Et $\forall \omega\in \Omega,\ X(\omega)<0\implies aX(\omega)\geq tX(\omega)\geq bX(\omega)\implies e^{tX(\omega)}\leq e^{aX(\omega)}$.\\
		Ainsi, \begin{align*}
			\sum\limits_{x\in X(\Omega)}e^{tx}\p(X=x)&=\sum\limits_{x\in X(\Omega)\cap\R^*_-}e^{tx}\p(X=x)+\sum\limits_{x\in X(\Omega)\cap\R^*_+}e^{tx}\p(X=x)\\
			&\leq \sum\limits_{x\in X(\Omega)\cap\R^*_-}e^{ax}\p(X=x)+\sum\limits_{x\in X(\Omega)\cap\R^*_+}e^{bx}\p(X=x)\\
			&\leq \sum\limits_{x\in X(\Omega)}e^{ax}\p(X=x)+\sum\limits_{x\in X(\Omega)}e^{bx}\p(X=x)\\
			&=\E\left(e^{aX}\right)+\E\left(e^{bX}\right)\\
			&<+\infty
		\end{align*}\\
		Donc $e^{tX}\in L^1$ c'est à dire $t\in I_X$.\\
		$I_X$ est bien un intervalle.
		\item Par hypothèse, $\exists a\in \R^*_+,\ ]-a,a[\subset I_X$.\\
		On sait que $X(\Omega)$ est au plus dénombrable.\\
		Si $X(\Omega)=\{x_0,\dots,x_p\}$ est fini alors il n'y a pas de difficulté,\\
		$\forall t\in ]-a,a[,\ M_X(t)=\displaystyle\sum\limits_{k=0}^pe^{tx_k}\p(X=x_k)=\sum\limits_{k=0}^p\sum\limits_{n=0}^{+\infty}\frac{(tx)^n}{n!}\p(X=x_k)=\sum\limits_{n=0}^{+\infty}\frac{t^n}{n!}\sum\limits_{k=0}^px_k^n\p(X=x_k)=\sum\limits_{n=0}^{+\infty}\frac{\E(X^n)}{n!}t^n$\\
		Supposons que $X(\Omega)$ est dénombrable. On note alors $X(\Omega)=\{x_n,\ n\in\N\}$. Montrons que $M_X$ est $\mathcal{C}^\infty$ sur $]-a,a[$ et que $\forall p\in \N,\ M_X^{(p)}(0)=\E(X^p)$.\\
		Posons, pour $n\in \N,\ f_n:t\in ]-a,a[\ \mapsto \p(X=x_n)e^{tx_n}$.\\
		Pour tout $n\in \N,\ f_n$ est $\mathcal{C}^\infty$ sur $]-a,a[$ et $\forall t\in ]-a,a[,\ \forall p\in \N,\ f_n^{(p)}(t)=\p(X=x_n)x_n^pe^{tx_n}$.\\\\
		Fixons maintenant $\alpha\in ]0,a[$ et $\rho\in ]\alpha,a[$.\\
		On peut écrire $\forall n\in \N,\ \forall t\in [-\alpha,\alpha],\ |f_n^{(p)}(t)|\leq |x_n^p|\p(X=x_n)e^{\alpha|x_n|}$.\\
		Ensuite, on écrit $\forall n\in \N,\ |x_n^p|\p(X=x_n)e^{\alpha|x_n|}=|x_n^p|e^{(\alpha-\rho)|x_n|}\times \p(X=x_n)e^{\rho|x_n|}$.\\
		D'une part la fonction $u\mapsto u^pe^{(\alpha-a)u}$ est continue sur $\R_+$ et de limite nulle en $+\infty$, elle est donc bornée sur $\R_+$. On note alors $M_p\in \R_+$ tel que $\forall n\in \N,\ |x_n|^pe^{(\alpha-a)|x_n|}\leq M_p$.\\
		Donc, $\forall n\in \N,\ \forall p\in \N,\ {||f_n^{(p)}||}_{\infty,[-\alpha,\alpha]}\leq M_p\p(X=x_n)e^{\rho|x_n|}$.\\
		D'autre part, $\p(X=x_n)e^{\rho|x_n|}\leq \p(X=x_n)e^{\rho x_n}+\p(X=x_n)e^{-\rho x_n}$\\
		Finalement, $\displaystyle\sum\limits_{n=0}^{+\infty}{||f_n||}_{\infty,[-\alpha,\alpha]}\leq M_p\left(\sum\limits_{n=0}^{+\infty}f_n(\rho)+\sum\limits_{n=0}^{+\infty}f_n(-\rho)\right)<+\infty$ (car $(-\rho,\rho)\in I_X^2$).\\
		On en déduit que $\displaystyle\sum\limits_{n\in\N}f_n$ converge uniformément sur $[-\alpha,\alpha]$. Ceci étant vrai pour tout segment $[-\alpha,\alpha]$ de $]-a,a[$, $\displaystyle\sum\limits_{n\in\N}f_n$ converge uniformément sur $]-a,a[$.\\
		D'après le théorème de transfert $\mathcal{C}^\infty$, $M_X$ est $\mathcal{C}^\infty$ sur $]-a,a[$ et $\forall p\in \N,\ \forall t\in ]-a,a[,\ M_X^{(p)}(t)=\displaystyle\sum\limits_{n=0}^{+\infty}x_n^p\p(X=x_n)e^{tx_n}$.\\
		Et par conséquent, $\forall p\in \N,\ M_X^{(p)}(0)=\displaystyle\sum\limits_{n=0}^{+\infty}x_n^p\p(X=x_n)=\E(X^p)$.\\\\
		Soit $t\in ]-a,a[$.
		$M_X(t)=\displaystyle\sum\limits_{k=0}^{+\infty}\p(X=x_k)e^{tx_k}=\sum\limits_{k=0}^{+\infty}\sum\limits_{n=0}^{+\infty}\p(X=x_k)\frac{(tx_k)^n}{n!}=\sum\limits_{k=0}^{+\infty}\sum\limits_{n=0}^{+\infty}x_k^n\p(X=x_k)\frac{t^n}{n!}$.\\
		Or la famille $\left(x_k^n\p(X=x_k)\displaystyle\frac{t^n}{n!}\right)_{(n,k)\in \N^2}$ est sommable. En effet, d'après le théorème de Fubini pour les familles positives,\\
		$\displaystyle\sum\limits_{k=0}^{+\infty}\sum\limits_{n=0}^{+\infty}\left|x_k^n\p(X=x_k)\frac{t^n}{n!}\right|=\sum\limits_{n=0}^{+\infty}\frac{|t|^n}{n!}\sum\limits_{k=0}^{+\infty}|x_k|^n\p(X=x_k)=\sum\limits_{n=0}^{+\infty}\frac{\E\left(|X|^n\right)}{n!}|t|^n$.
	\end{enumerate}
	
	\subsection{Inégalité de Jensen \etoile{1}}
	\label{Inégalité de Jensen corrigé}
		\textcolor{blue}{\hyperref[Inégalité de Jensen]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $\E(X)\in \R$ donc comme $f$ est dérivable et convexe sur $\R$,\\
		$\forall x\in \R,\ f(x)\geq f'(\E(X))(x-\E(X))+f(\E(X))$.\\
		$X(\Omega)\subset \R$ donc l'inégalité est vraie en termes de variables aléatoires :
		$$f(X)\geq f'(\E(X))(X-\E(X))+f(\E(X))$$
		\item Supposons que $f(X)\in L^1$.\\
		Alors par croissance de l'espérance,
		\begin{align*}
			\E(f(X))&\geq \E[f'(\E(X))(X-\E(X))+f(\E(X))]\\
			(f'(\E(X))\text{ et } \E(X)\text{ sont des constantes})&=f'(\E(X))(\E(X)-\E(X))+\E(f(\E(X)))\\
			(f(\E(X))\text{ est une constante})&=f(\E(X))
		\end{align*}
	\end{enumerate}
	
	\subsection{Inégalité de Hölder \etoile{2}}
	\label{Inégalité de Hölder corrigé}
		\textcolor{blue}{\hyperref[Inégalité de Hölder]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item L'inégalité est trivialement vérifiée si $x$ ou $y$ est nul. Soient $x,y\in \R^*_+$.\\
		La fonction $\ln$ est deux fois dérivable sur $\R^*_+$ et $\forall t>0,\ \ln''(t)=\displaystyle-\frac{1}{t^2}\leq 0$. Donc $\ln$ est concave sur $\R^*_+$.\\
		Par conséquent d'après l'inégalité de Jensen $\forall u,v>0,\ \displaystyle\frac{1}{p}\ln(u)+\frac{1}{q}\ln(v)\leq \ln\left(\frac{u}{p}+\frac{v}{p}\right)$.\\
		En appliquant cette inégalité pour $u=x^p$ et $v=y^q$ on obtient :
		$$\ln(x)+\ln(y)\leq \ln\left(\frac{x^p}{p}+\frac{y^q}{q}\right)$$
		Puis en passant à l'exponentielle qui est une fonction strictement croissante :
		$$xy\leq \frac{x^p}{p}+\frac{y^q}{q}$$
		\item Supposons que $\E(X^p)=\E(Y^q)=1$.\\
		Soit $\omega\in \Omega$. $X(\omega),Y(\omega)\in \R_+$ donc, $X(\omega)Y(\omega)\leq \displaystyle\frac{X(\omega)^p}{p}+\frac{Y(\omega)^q}{q}$ i.e $(XY)(\omega)\leq \displaystyle\frac{X^p(\omega)}{p}+\frac{Y^q(\omega)}{q}$. Alors en passant à l'espérance,
		$$\E(XY)\leq \frac{\E(X^p)}{p}+\frac{\E(Y^q)}{q}=\frac{1}{p}+\frac{1}{q}=1=\E(X^p)^{1/p}\E(Y^q)^{1/q}$$
		Revenons au cas général. Tout d'abord si $\E^p(X)=0$ alors $X^p$ est presque sûrement nulle puisque c'est une variable aléatoire positive. Donc $X$ est presque sûrement nulle et par suite $\E(X)=0$. De même, $\E(Y^q)=0\implies \E(Y)=0$. L'inégalité est donc trivialement vérifiée dans ces cas.\\
		On suppose maintenant $\E(X^p)\ne 0$ et $\E(Y^q)\ne 0$. On pose $X_0=\displaystyle\frac{X}{\E(X^p)^{1/p}}$ et $Y_0=\displaystyle\frac{Y}{\E(Y^q)^{1/q}}$. $X_0$ et $Y_0$ sont deux variables aléatoires réelles positives vérifiant $\E(X_0^p)=\E(Y_0^q)=1$. D'après ce qui a été fait précédemment on sait que
		$$\E(X_0Y_0)\leq \E(X_0^p)^{1/p}\E(Y_0^q)^{1/q}$$
		C'est à dire $\displaystyle\frac{\E(XY)}{\E(X^p)^{1/p}\E(Y^q)^{1/q}}\leq 1$ ou encore $\E(XY)\leq \E(X^p)^{1/p}\E(Y^q)^{1/q}$.
		\item Pour $p=q=2$ on a $\E(XY)\leq \sqrt{\E(X^2)\E(Y^2)}$. Il s'agit de l'inégalité de Cauchy-Schwarz.
	\end{enumerate}
	
	\subsection{Modes de convergences}
	\label{Modes de convergences corrigé}
	\textcolor{blue}{\hyperref[Modes de convergences]{[Enoncé]}}\\
		
	\subsection{Marche aléatoire sur $\Z^d$ \etoile{3}}
	\label{Marche aléatoire sur Z^d corrigé}
		\textcolor{blue}{\hyperref[Marche aléatoire sur Z^d]{[Enoncé]}}\\
	\subsubsection{Le cas $d=1$ \etoile{2}}
	\begin{enumerate}
		\item Soit $t\in\N^*$. $Y_t(\Omega)=\{0,1\} : Y_t\sim\B(\p(Y_t=1))=\B(\p(X_t=1))=\B(p)$.\\
		Soit $n\in \N^*$.\\
		$X_1,\dots,X_n$ sont mutuellement indépendantes donc d'après le lemme des coalitions, $Y_1,\dots,Y_n$ sont mutuellement indépendantes. Ainsi d'après le cours, $\displaystyle\sum\limits_{t=1}^nY_t\sim\B(n,p)$.
		\item Soit $n\in \N^*$.\\
		$S_n\displaystyle\sum_{t=1}^nX_t=\sum\limits_{t=1}^n(2Y_t-1)=2\sum\limits_{t=1}^nY_t-n$.\\
		Ainsi, si $n$ est impair alors $S_n(\Omega)\subset \Z\backslash 2\Z$ et donc $\{S_n=0\}=\emptyset$ d'où $\p(S_n=0)=0$.\\
		Et si $n\in 2\N$ alors, $\p(S_n=0)=\displaystyle\p\left(\sum\limits_{t=1}^nY_t=\frac{n}{2}\right)=\binom{n}{n/2}(p(1-p))^{n/2}$.
		\item On utilise la formule de Stirling :\\
		$\p(S_{2n}=0)=\displaystyle\frac{(2n)!}{(n!)^2}(p(1-p))^n\unfty{\sim}(p(1-p))^n\frac{\displaystyle\left(\frac{2n}{e}\right)^{2n}\displaystyle\sqrt{4\pi n}}{\displaystyle\left(\left(\frac{n}{e}\right)^n\displaystyle\sqrt{2\pi n}\right)^2}=\frac{(4p(1-p))^n}{\displaystyle\sqrt{\pi n}}$.\\
		Donc, $\unfty{\lim}\p(S_{2n}=0)\in \{0,+\infty\}$ et,\\
		$\unfty{\lim}\p(S_{2n}=0)=+\infty\iff 4p(1-p)>1\iff p(1-p)>\displaystyle\frac{1}{4}$.\\
		Or la fonction $x\in ]0,1[\ \mapsto x(1-x)$ admet un maximum en $x=\displaystyle\frac{1}{2}$ qui vaut $\displaystyle\frac{1}{4}$.\\
		Donc $\unfty{\lim}\p(S_{2n}=0)=0$.\\
		On peut interpréter cela comme le fait que la puce ne retourne presque sûrement plus en l'origine après un certain temps.
		\item $O_{2j}(\Omega)=\{0,1\}\text{ et }\{O_{2j}=1\}=\{S_{2j}=0\}$ donc $O_{2j}\sim\B(\p(S_{2j}=0))=\B\left(\displaystyle\binom{2j}{j}(p(1-p))^j\right)$.\\
		Donc par linéarité de l'espérance, $\E(T_n)=\displaystyle\sum\limits_{j=0}^n\E(O_{2j})=\sum\limits_{j=0}^n\binom{2j}{j}(p(1-p))^j$
		\item Soit $x\in ]-1,1[$.\\
		On sait que $\displaystyle\frac{1}{\displaystyle\sqrt{1-x}}=\sum\limits_{n=0}^{+\infty}\binom{-1/2}{n}(-x)^n$ avec,\\
		$\displaystyle\binom{-1/2}{n}=\frac{1}{n!}\prod\limits_{k=0}^{n-1}\left(-\frac{1}{2}-k\right)=\frac{(-1)^n}{2^nn!}\prod\limits_{k=0}^{n-1}(2k+1)=\frac{(-1)^n}{2^nn!}\cdot\frac{\displaystyle\prod\limits_{k=1}^{2n}k}{\displaystyle\prod\limits_{k=1}^n2k}=\frac{(-1)^n(2n)!}{2^{2n}(n!)^2}=\frac{(-1)^n}{4^n}\binom{2n}{n}$.\\
		Donc $\displaystyle\frac{1}{\displaystyle\sqrt{1-x}}=\sum\limits_{n=0}^{+\infty}\binom{2n}{n}\left(\frac{x}{4}\right)^n$.\\
		$p\ne \displaystyle\frac{1}{2}\implies 0<4p(1-p)<1$ donc,\\
		$\unfty{\lim}\E(T_n)=\displaystyle\sum\limits_{n=0}^{+\infty}\binom{2n}{n}\left(\frac{4p(1-p)}{4}\right)^n=\frac{1}{\displaystyle\sqrt{1-4p(1-p)}}$.\\
		On peut interpréter qu'en moyenne, la puce est repassée $\displaystyle\frac{1}{\displaystyle\sqrt{1-4p(1-p)}}$ fois par l'origine lors de son parcours.
		\item Soit $n\in \N$.\\
		$\E(T_n)=\displaystyle\sum\limits_{j=0}^{n}\binom{2j}{j}\displaystyle\frac{1}{4^n}$ donc $\E(T_0)=0=\displaystyle\frac{2\times 0+1}{2^{2\times 0}}\binom{0}{0}$.\\
		Supposons que $\E(T_n)=\displaystyle\frac{2n+1}{2^{2n}}\binom{2n}{n}$.\\
		Alors, $\E(T_{n+1})=\E(T_n)+\p(S_{2n+2}=0)=\displaystyle\frac{2n+1}{4^n}\binom{2n}{n}+\binom{2n+2}{n+1}\frac{1}{4^{n+1}}$\\\\\\
		Et $\displaystyle\binom{2n}{n}=\frac{(2n)!}{(n!)^2}=\frac{(n+1)^2}{(2n+1)(2n+2)}\cdot \frac{(2n+2)!}{((n+1)!)^2}=\frac{2(n+1)}{4(2n+1)}\binom{2n+2}{n+1}$.\\\\\\
		Donc, $\E(T_{n+1})=\displaystyle\frac{1}{4^{n+1}}\binom{2n+2}{n+1}(2n+2+1)=\frac{2(n+1)+1}{4^{n+1}}\binom{2(n+1)}{n+1}$.\\
		Ainsi par récurrence, $\forall n\in \N,\ \E(T_n)=\displaystyle\frac{2n+1}{4^n}\binom{2n}{n}$.\\
		On en déduit que $\E(T_n)\unfty{\sim}\displaystyle\frac{2n+1}{\displaystyle\sqrt{\pi n}}\unfty{\sim}2\displaystyle\sqrt{\frac{n}{\pi}}$ d'où $\E(T_n)\unfty{\longrightarrow}+\infty$.\\
		La puce repasse, en moyenne, une infinité de fois par l'origine lors de son parcours. Ceci paraît cohérent pour une puce qui a autant de chance de se déplacer à gauche qu'à droite.
		\underline{Remarque :} On aurait aussi pu utiliser l'équivalent $\p(S_{2n}=0)\unfty{\sim}\displaystyle\frac{(4p(1-p))^n}{\displaystyle\sqrt{\pi n}}=\frac{1}{\displaystyle\sqrt{\pi n}}=\frac{2}{\displaystyle\sqrt{\pi}}\left(\displaystyle\sqrt{n+1}-\displaystyle\sqrt{n}\right)$ pour avoir l'équivalent, et donc la limite, de $(\E(T_n))_{n\in \N}$.
	\end{enumerate}
	
	\subsubsection{Le cas $d=2$}
	
	
	\subsubsection{Le cas général}
	
	
	\subsubsection{Marche aléatoire auto-évitante}
	
	
	\subsection{Matrice de covariance \etoile{2}}
	\label{Matrice de covariance corrigé}
		\textcolor{blue}{\hyperref[Matrice de covariances]{[Enoncé]}}\\
	\begin{enumerate}
		\item $A^\top X=\displaystyle\sum\limits_{k=1}^na_kX_k$.\\
		$L^2$ est un espace vectoriel donc $A^\top X\in L^2$ et par bilinéarité de la covariance,
		\begin{align*}
			\V(A^\top X)&=\operatorname{Cov}(A^\top X,A^\top X)\\
			&=\operatorname{Cov}\left(\sum\limits_{i=1}^na_iX_i,\sum\limits_{j=1}^na_jX_j\right)\\
			&=\sum\limits_{1\leq i,j\leq n}a_ia_j\operatorname{Cov}(X_i,X_j)\\
			&=A^\top\Sigma A
		\end{align*}
		\item La covariance étant symétrique, $\Sigma$ est symétrique réelle et donc diagonalisable d'après le théorème spectral. De plus, la question précédente montre que $\forall A\in \M_{n,1}(\R),\ A^\top\Sigma A\geq 0$.\\
		Donc $\Sigma \in S_n^+(\R)$ c'est à dire $\operatorname{Sp}(\Sigma)\subset \R_+$.
		\item $0\notin \operatorname{Sp}(\Sigma)\iff \forall A=\begin{pmatrix} a_1\\ \vdots\\ a_n \end{pmatrix}\in \M_{n,1}(\R),\ A^\top\Sigma A=\V(A^\top X)\ne 0$.\\
		Or, pour toute variable aléatoire discrète réelle $Y$, $\V(Y)=0\iff \E((Y-\E(Y)^2)=0$. Et puisque la variable aléatoire $(Y-\E(Y))^2$ est positive, cela équivaut à ce que $(Y-\E(Y))^2=0$ c'est à dire à ce que $Y=\E(Y)$ et donc à ce que $Y$ soit certaine (constante).\\
		On peut alors donner la condition : $\Sigma$ est inversible si et seulement s'il n'existe aucune combinaison linéaire de $X_1,\dots,X_n$ qui soit presque sûrement constante.\\
		Ou encore qu'il n'existe aucune combinaison affine de $X_1,\dots,X_n$ qui soit presque sûrement nulle.\\
		\textit{\underline{Remarque}: On pourra apprécier la ressemblance, dans une certaine mesure, entre cette condition et celle pour la matrice de Gram.}
	\end{enumerate}
	
	
	\subsection{Maximisation de la variance sous-contrainte}
	\textcolor{blue}{\hyperref[Maximisation de la variance sous contrainte]{[Enoncé]}}\\
	\label{Maximisation de la variance sous contrainte corrigé}
	\begin{enumerate}[leftmargin=*]
		\item On note $X_i$ la variable aléatoire qui vaut $1$ si la $i$-ème ampoule est allumée et $0$ sinon.\\
		On a donc $\displaystyle Y=\sum_{i=1}^{n}X_i$.\\
		Par linéarité de l'espérance, \[\E(Y)=\sum_{i=1}^{n}\E(X_i)=\sum_{i=1}^{n}p_i\]
		Puisque les $X_i$ sont indépendants par hypothèse, on a : \[\V(Y)=\sum_{i=1}^{n}\V(X_i)=\sum_{i=1}^{n}p_i(1-p_i)\]
		\item On peut voir ce problème comme un problème d'optimisation sous contrainte.\\
		On pose $p=(p_1,\dots,p_n)$.\\
		On pose également $f(p)=\displaystyle\sum_{i=1}^{n}p_i(1-p_i)$ et $g(p)=\displaystyle\sum_{i=1}^{n}p_i-m$.\\
		On a donc : \[\begin{cases}
			\nabla f(p)=(1-2p_1,\dots, 1-2p_n)\\
			\nabla g(p)=(1,\dots, 1)
		\end{cases}\]
		Cherchons $\lambda\in\R$ tel que $\nabla f(p)=\lambda \nabla g(p)$ quand $g(p)=0$.
		Pour qu'un tel $\lambda$ existe il faut que les $p_i$ soient égaux, on a donc : \[p_i=\frac{m}{n}\]
		pour tout $i\in\crblanc{1}{n}$.
		Ainsi, la variance est maximale pour $p=\left(\frac{m}{n},\dots,\frac{m}{n}\right)$ et : \[\V(Y)=\sum_{i=1}^n\frac{m}{n}\left(1-\frac{m}{n}\right)=m\left(1-\frac{m}{n}\right)\]
		On en déduit que : $Y\sim \mathcal{B}\left(n,\frac{m}{n}\right)$.
	\end{enumerate}
	\subsection{Inégalité de Kosmanek \etoile{1}}
	\label{Inégalité de Kosmanek corrigé}
		\textcolor{blue}{\hyperref[Inégalité de Kosmanek]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $\mathbbm{1}_C\sim\B(\p(C))$ donc $\V(\mathbbm{1}_C)=\p(C)(1-\p(C))$.\\
		Or la fonction $p\mapsto p(1-p)$ admet un maximum sur $[0,1]$ en $p=\displaystyle\frac{1}{2}$ qui vaut $\displaystyle\frac{1}{4}$.\\
		On en déduit que $\V(\mathbbm{1}_C)\leq \displaystyle\frac{1}{4}$.
		\item D'après l'inégalité de Cauchy-Schwarz, $\displaystyle\left|\operatorname{Cov}(\mathbbm{1}_A,\mathbbm{1}_B)\right|\leq\sqrt{\V(\mathbbm{1}_A)\V(\mathbbm{1}_B)}\leq\frac{1}{4}$.
		\item $\displaystyle\frac{1}{4}\geq|\operatorname{Cov}(\mathbbm{1}_A,\mathbbm{1}_B)|=|\E(\mathbbm{1}_A\mathbbm{1}_B)-\E(\mathbbm{1}_A)\E(\mathbbm{1}_B)|=|\E(\mathbbm{1}_{A\cap B})-\E(\mathbbm{1}_A)\E(\mathbbm{1}_B)|=|\p(A\cap B)-\p(A)\p(B)|$.\\
		Il y a égalité lorsque $\displaystyle\left|\operatorname{Cov}(\mathbbm{1}_A,\mathbbm{1}_B)\right|=\sqrt{\V(\mathbbm{1}_A)\V(\mathbbm{1}_B)}=\frac{1}{4}$.\\
		La deuxième égalité impose $\p(A)=\p(B)=\displaystyle\frac{1}{2}$.\\
		Alors la première égalité devient $\displaystyle\left|\p(A\cap B)-\frac{1}{4}\right|=\frac{1}{4}$ c'est à dire $\p(A\cap B)=\displaystyle\frac{1}{2}$ ou $\p(A\cap B)=0$.
	\end{enumerate}
	
	\subsection{Inégalité de Cantelli \etoile{3}}
	\label{Inégalité de Cantelli corrigé}
		\textcolor{blue}{\hyperref[Inégalité de Cantelli]{[Enoncé]}}\\
	\begin{enumerate}
		\item \begin{enumerate}
			\item Soit $u\in \R_+$.\\
			$\E\left((X+u)^2\right)=\E(X^2+2uX+u^2)=\E(X^2)+2u\E(X)+\E(u^2)=\E(X^2)-\E(X)^2+u^2=\V(X)+u^2$.
			\item Soit $u\in \R_+$.\\
			$\p(X\geq \lambda)\leq \p(X+u\geq \lambda+u)$. Or $\lambda+u\geq 0$ donc, $\p(|X+u|\geq \lambda+u)=\p\left((X+u)^2\geq (\lambda+u)^2\right)$.\\
			Enfin, $\{X+u\geq \lambda+u\}\subset \{|X+u|\geq \lambda+u\}$.\\
			Donc d'après l'inégalité de Markov, $\p(X\geq \lambda)\leq \p\left((X+u)^2\geq (\lambda+u)^2\right)\leq \displaystyle\frac{\E\left((X+u)^2\right)}{(\lambda+u)^2}=\frac{\V(X)+u^2}{(\lambda+u)^2}$.
			\item Posons $\fonction{f}{\R_+}{\R}{u}{\displaystyle\frac{\V(X)+u^2}{(\lambda+u)^2}}$. $f$ est dérivable sur $\R_+$ et pour tout $u\in \R_+$,
			\begin{align*}
				f'(u)&=\displaystyle\frac{2u(\lambda+u)^2-2(\V(X)+u^2)(\lambda+u)}{(\lambda+u)^4}\\
				&=\frac{2(u\lambda^2+2\lambda u^2+u^3-\lambda\V(X)-\V(X)u-\lambda u^2-u^3)}{(\lambda+u)^4}\\
				&=\frac{2(\lambda u^2+(\lambda^2-\V(X))u-\lambda\V(X))}{(\lambda+u)^4}
			\end{align*}
			\\
			Ainsi, $\forall u\in \R_+,\ f'(u)\geq 0\iff \lambda u^2+(\lambda^2-\V(X))u-\lambda\V(X)\geq 0$.\\\\
			$\Delta=(\lambda^2-\V(X))^2+4\lambda^2\V(X)=(\lambda^2+\V(X))^2>0$. On pose $u_1=\displaystyle\frac{\V(X)-\lambda^2+\displaystyle\sqrt{\Delta}}{2\lambda}=\frac{\V(X)}{\lambda}$ et $u_2=\displaystyle\frac{\V(X)-\lambda^2-\displaystyle\sqrt{\Delta}}{2\lambda}=-\lambda<0$.\\\\
			En résumé,\\\\
			\begin{tikzpicture}
				\tkzTabInit{$x$ / 1 , $f'(x)$ / 1,Variations de $f$/2}{$0$, $u_1$,$+\infty$}
				\tkzTabLine{,-,z,+}
				\tkzTabVar{+/,-/$f(u_1)$,+/}
			\end{tikzpicture}\\
			Ainsi, l'inégalité optimale que l'on peut atteindre est donnée par $u=u_1$ :\\
			$\p(X\geq \lambda)\leq \displaystyle\frac{\V(X)+\left(\displaystyle\frac{\V(X)}{\lambda}\right)^2}{\left(\lambda+\displaystyle\frac{\V(X)}{\lambda}\right)^2}=\frac{\lambda^2\V(X)+\V(X)^2}{(\lambda^2+\V(X))^2}=\frac{\V(X)(\lambda^2+\V(X))}{(\lambda^2+\V(X))^2}=\frac{\V(X)}{\lambda^2+\V(X)}$.
		\end{enumerate}
		\item La variable aléatoire $Y=X-\E(X)$ est réelle discrète et centrée ($\E(Y)=0)$. Elle possède un moment d'ordre $2$ puisque d'après le cours, $\V(Y)=\V(X)$.\\
		Par conséquent d'après ce qui a été fait précédemment,\\
		$$\p(X-\E(X)\geq \lambda)=\p(Y\geq \lambda)\leq \displaystyle\frac{\V(Y)}{\lambda^2+\V(Y)}=\frac{\V(X)}{\lambda^2+\V(X)}$$
	\end{enumerate}
	
	\subsection{Inégalité de Hoeffding \etoile{3}}
	\label{Inégalité de Hoeffding corrigé}
		\textcolor{blue}{\hyperref[Inégalité de Hoeffding]{[Enoncé]}}\\
	\begin{enumerate}
		\item Soient $t\in \R$ et $x\in [-1,1]$. On sait que la fonction exp est convexe sur $\R$ et on remarque que $\displaystyle\frac{1}{2}(1-x)+\frac{1}{2}(1+x)=1$ et que $\displaystyle\frac{1}{2}(1-x)(-t)+\frac{1}{2}(1+x)t=tx$.\\
		Donc $e^{tx}\leq \displaystyle\frac{1}{2}(1-x)e^{-t}+\frac{1}{2}(1+x)e^t$.
		\item Soit $t\in \R$.\\
		On sait que $\ch(t)=\displaystyle\sum\limits_{n=0}^{+\infty}\frac{t^{2n}}{(2n!)}$ et $e^{t^2/2}=\displaystyle\sum\limits_{n=0}^{+\infty}\frac{\left(\frac{t^2}{2}\right)^n}{n!}=\sum\limits_{n=0}^{+\infty}\frac{t^{2n}}{2^nn!}$.\\
		Or, $\forall n\in \N^*,\ \displaystyle\frac{(2n)!}{2^nn!}\geq \frac{(2n)!}{(n!)^2}=\binom{2n}{n}\geq 1$. Donc $\forall n\in \N^*,\ (2n)!\geq 2^nn!$ .\\
		Ainsi, $\ch(t)-e^{t^2/2}=\displaystyle\sum\limits_{n=1}^{+\infty}\left(\frac{1}{(2n)!}-\frac{1}{2^nn!}\right)t^{2n}\leq 0$.
		\item Soit $t\in \R$. $\forall \omega\in \Omega,\ X(\omega)\in [-1,1]$. Donc d'après la question $1$ et par croissance de l'espérance,\\
		$\E(e^{tX})\leq \displaystyle\E\left(\frac{e^{-t}}{2}(1-X)+\frac{e^t}{2}(1+X)\right)=\frac{e^{-t}+e^t}{2}-\frac{1}{2}\E(X)+\frac{1}{2}\E(X)=\ch(t)\leq e^{t^2/2}$.
		\item Soit $(t,\varepsilon)\in \left(\R^*_+\right)^2$. La variable aléatoire discrète $e^{tY}$ est réelle et positive donc d'après l'inégalité de Markov,\\
		$\p(Y\geq \varepsilon)=\p(tY\geq t\varepsilon)=\p\left(e^{tY}\geq e^{t\varepsilon}\right)\leq e^{-t\varepsilon}\E\left(e^{tY}\right)$.
		\item Soit $\varepsilon\in \R^*_+$. Notons $a=\displaystyle\sqrt{\displaystyle\sum\limits_{k=1}^nc_k^2}$.\\
		$\displaystyle\frac{S}{a}$ et $-\displaystyle\frac{S}{a}$ sont des variables aléatoires discrètes centrées à valeurs dans $[-1,1]$ donc $\forall t\in \R^*_+$,\\
		\begin{align*}
			\p(|S|\geq \varepsilon)&=\p(S\geq \varepsilon)+\p(-S\geq \varepsilon)\\
			&=\p\left(\frac{S}{a}\geq \frac{\varepsilon}{a}\right)+\p\left(-\frac{S}{a}\geq \frac{\varepsilon}{a}\right)\\
			&\leq e^{-t\varepsilon/a}\E\left(e^{tS/a}\right)+e^{-t\varepsilon/a}\E\left(e^{-tS/a}\right)\\
			&\leq e^{-t\varepsilon/a}\left(e^{t^2/2}+e^{(-t)^2/2}\right)\\
			&=2e^{-t\varepsilon/a+t^2/2}
		\end{align*}
		En particulier pour $t=\displaystyle\frac{\varepsilon}{a}$ :
		$$\p(|S|\geq \varepsilon)\leq 2\operatorname{exp}\left(-\frac{\varepsilon^2}{2a^2}\right)=2\operatorname{exp}\left(-\frac{\varepsilon^2}{2\displaystyle\sum\limits_{k=1}^nc_k^2}\right)$$
	\end{enumerate}
	
	\subsection{Théorème d'approximation de Weierstrass \etoile{3}}
	\label{Théorème d'approximation de Weierstrass corrigé}
		\textcolor{blue}{\hyperref[Théorème d'approximation de Weierstrass]{[Enoncé]}}\\
	\begin{enumerate}
		\item D'après le cours, $S_n\sim \B(n,t)$ et donc, $\forall k\in \crblanc{0}{n},\ \p(S_n=k)=b_k^n(t)$. Posons $R_n=\displaystyle\frac{S_n}{n}$.\\
		$R_n$ est une variable aléatoire comme somme de variables aléatoires. De plus $R_n(\Omega)=\left\{\displaystyle\frac{k}{n},\ k\in\crblanc{0}{n}\right\}$ est fini. Ainsi, $R_n$ est une variable aléatoire discrète et d'après la formule de transfert,\\
		$\E(f(R_n))=\displaystyle\sum\limits_{k=0}^nf\left(\frac{k}{n}\right)\p\left(R_n=\frac{k}{n}\right)=\sum\limits_{k=0}^nf\left(\frac{k}{n}\right)\p(S_n=k)=B_n(f)(t)$.
		\item $f$ est continue sur le segment $I$ donc d'après le théorème de Heine, $f$ est uniformément continue sur $I$.\\
		Par conséquent, $\exists \delta>0,\ \forall (x,y)\in\R^2,\ |x-y|\leq \delta\implies |f(x)-f(y)|\leq \displaystyle\frac{\varepsilon}{2}$\\
		Comme $\{|t-R_n|\leq \delta\}\ \sqcup\ \{|t-R_n|>\delta\}=\Omega$, on peut écrire\\
		$|f(t)-f(R_n)|=|f(t)-f(R_n)|\mathbbm{1}_{\{|t-R_n|\leq \delta\}}+|f(t)-f(R_n)|\mathbbm{1}_{\{|t-R_n|>\delta\}}$.\\\\
		Or, $|f(t)-f(R_n)|\mathbbm{1}_{\{|t-R_n|\leq \delta\}}\leq \displaystyle\frac{\varepsilon}{2}\mathbbm{1}_{\{|t-R_n|\leq \delta\}}$ et $|f(t)-f(R_n)|\mathbbm{1}_{\{|t-R_n|> \delta\}}\leq 2{||f||}_\infty\mathbbm{1}_{\{|t-R_n|> \delta\}}$.\\
		Aussi on rappelle que pour tout évènement $A,\ \E(\mathbbm{1}_A)=\p(A)$.\\
		Ainsi par croissance de l'espérance,
		\begin{align*}
			\E(|f(t)-f(R_n)|)&=\E\left(|f(t)-f(R_n)|\mathbbm{1}_{\{|t-R_n|\leq \delta\}}\right)+\E\left(|f(t)-f(R_n)|\mathbbm{1}_{\{|t-R_n|>\delta\}}\right)\\
			&\leq \displaystyle\frac{\varepsilon}{2}\E\left(\mathbbm{1}_{\{|t-R_n|\leq \delta\}}\right)+2{||f||}_\infty\E\left(\mathbbm{1}_{\{|t-R_n|>\delta\}}\right)\\
			&\leq \displaystyle\frac{\varepsilon}{2}+2{||f||}_\infty\p\left(|t-R_n|>\delta\right)
		\end{align*}
		\item D'après l'inégalité de Cauchy-Schwarz,\\
		$\p(|R_n-t|>\delta)=\p(|R_n-\E(R_n)|>\delta)\leq \displaystyle\frac{\V(R_n)}{\delta^2}=\frac{\V(S_n)}{n^2\delta^2}=\frac{t(1-t)}{n\delta^2}$.\\\\
		Etudions $g:x\mapsto x(1-x)$ sur $I$. $g$ est dérivable sur $I$ et $\forall x\in I,\ g'(x)=1-2x$.\\\\
		\begin{tikzpicture}
			\tkzTabInit{$x$ / 1 , $g'(x)$ / 1,Variations de $g$/2}{$0$, $\displaystyle\frac{1}{2}$,$1$}
			\tkzTabLine{,+,z,-}
			\tkzTabVar{-/$0$,+/$\displaystyle\frac{1}{4}$,-/$0$}
		\end{tikzpicture}\\\\
		On en déduit la majoration $\p(|R_n-t|>\delta)\leq \displaystyle\frac{1}{4n\delta^2}$.
		\item Soit $\varepsilon>0$. Fixons $n\in \N^*$.\\
		$|f(t)-B_n(f)(t)|=|f(t)-\E(R_n)|=|\E(f(t)-R_n)|$.\\
		Or, $f(t)-R_n\leq |f(t)-R_n|$ et $R_n-f(t)\leq |f(t)-R_n|$ donc par croissance de l'espérance,\\
		$|\E(f(t)-R_n)|=\max(\E(f(t)-R_n),\E(R_n-f(t)))\leq \E(|f(t)-R_n|)$.\\
		Ainsi d'après les questions $2$ et $3$,\\
		$|f(t)-B_n(f)(t)|\leq \displaystyle\frac{\varepsilon}{2}+\frac{{||f||}_\infty}{2n\delta^2}$.\\
		Et donc, $\forall n\geq N=\left\lfloor \displaystyle\frac{{||f||}_\infty}{\varepsilon\delta^2}\right\rfloor,\ |f(t)-B_n(f)(t)|\leq \varepsilon$\\
		d'où, $\forall n\geq N,\ {||f-B_n(f)||}_\infty\leq \varepsilon$.
		\item Soit $[a,b]\subset \R$. Soit $f\in\mathcal{C}^0([a,b],\C)$.\\
		Posons $\fonction{\gamma}{[a,b]}{I}{t}{\displaystyle\frac{t-a}{b-a}}$.\\ $\gamma$ est continue et bijective donc $f\circ\gamma^{-1}\in \mathcal{C}$ et on pose alors, pour tout $n\in \N^*,\ A_n=B_n(f\circ\gamma^{-1})\circ\gamma$.\\
		$(A_n)_{n\in \N^*}$ est une suite de fonctions de $[a,b]$ dans $\C$ polynomiales.\\
		Fixons $\varepsilon>0$.\\
		D'après la question $4,\ \exists N\in \N^*,\ \forall n\geq N,\ \forall t\in [0,1],\ |f\circ\gamma^{-1}(t)-B_n(f\circ\gamma^{-1})(t)|\leq \varepsilon$.\\
		C'est à dire, $\exists N\in \N^*,\ \forall n\geq N,\ \forall t\in [a,b],\ |f(t)-A_n(t)|\leq \varepsilon$.\\
		Ainsi, ${||f-A_n||}_{\infty,[a,b]}\unfty{\longrightarrow}0$ ce qui termine la preuve.
	\end{enumerate}
	
	
	\newpage
\section{Correction Endomorphismes d'un espace euclidien}
	
	\subsection{Equations matricielles \etoile{1}}
	\label{Equations matricielles corrigé}
	\textcolor{blue}{\hyperref[Equations matricielles]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Supposons qu'il existe $A$ et $B$ deux telles matrices.\\
		Alors $n=\Tr(I_n)=\Tr(AB-BA)=\Tr(AB)-\Tr(BA)=\Tr(AB)-\Tr(AB)=0$ ce qui est absurde.
		\item Supposons qu'il existe une telle matrice $M$.\\
		Alors $\det(M^2)=\det(M)^2=\det\begin{pmatrix}0&1\\1&0\end{pmatrix}=-1$.\\
		Or $M$ est à coefficients réels donc $\det(M)\in \R$ ce qui est absurde.
		\item Supposons qu'il existe une telle matrice $N$.\\
		Alors $N^4=\begin{pmatrix}0&0\\0&0\end{pmatrix}$ et donc $N$ est nilpotente. On en déduit que $\chi_N=X^2$. Mais alors d'après le théorème de Cayley-Hamilton, $N^2=\begin{pmatrix}0&0\\0&0\end{pmatrix}$ ce qui est absurde.
	\end{enumerate}
	
	\subsection{Equation matricielle faisant intervenir la comatrice}
	\label{Equation matricielle faisant intervenir la comatrice corrigé}
	\textcolor{blue}{\hyperref[Equation matricielle faisant intervenir la comatrice]{[Enoncé]}}\\
	Soit $A\in\M_n(\R)$ telle que $\Com(A)=A$.\\
	On rappelle que \[A\Com(A)^\top=\det(A)I_n\]
	Ainsi, on en déduit que : $AA^\top=\det(A)I_n$.\\
	Par passage au déterminant, $\det(A)^2=\det(A)$.Donc $\det(A)=0$ ou $\det(A)=1$.\\
	\textbullet Si $\det(A)=0$ alors $AA^\top=0$. Ainsi, $\Tr(A^\top A)=0$ ce qui implique que : $A=0$.\\
	\textbullet Si $\det(A)=1$ alors $AA^\top=I_n$ donc $A\in\mathcal{O}_n(\R)$.\\
	Réciproquement, il est clair que si $A=0$ alors $\Com(A)=A$.\\
	Si $A\in\mathcal{SO}_n(\R)$ alors $A^{-1}=A^\top$ et $\det(A)=1$.\\ 
	Donc puisque \[A\Com(A)^\top=I_n\] par unicité de l'inverse $\Com(A)^\top=A^\top$.
	D'où $A=\Com(A)$.\\
	Donc l'ensemble des solutions des matrices à coefficients réels vérifiant $A=\Com(A)$ est : \[\mathcal{SO}_n(\R)\cup\{0\}\]
	
	\subsection{Matrice de rotation}
	\label{Matrice de rotation corrigé}
		\textcolor{blue}{\hyperref[Matrice de rotation]{[Enoncé]}}\\
	On pose $M=\text{Mat}_\mathcal{B}(u)$.
	\[M^\top M=\frac{1}{9}\begin{pmatrix}
		2&-2&1\\
		-2&1&2\\
		1&-2&2
	\end{pmatrix}\begin{pmatrix}
	2&2&1\\
	-2&1&2\\
	1&-2&2
	\end{pmatrix}=\frac{1}{9}\begin{pmatrix}
	9&0&0\\
	0&9&0\\
	0&0&9
	\end{pmatrix}=I_n\]
	Donc $u\in O(E)$.
	Calculons son déterminant à l'aide de la formule de Sarrus : \[\det(M)=\frac{1}{27}\left(4+4+4-1+8+8\right)=1\]
	Donc $u$ est une rotation. \\
	\textit{Solution à la question bonus} :\\
	Pour déterminer l'axe, il suffit de trouver un vecteur $x$ tel que $Mx=x$.
	On cherche donc à résoudre le système suivant : 
	\[\begin{cases}
		2x+2y+z=3x\\
		-2x+y+2z=3y\\
		x-2y+2z=3z
	\end{cases}\]
	Après résolution, on obtient que $\Vect\left(\begin{pmatrix}
		1\\0\\1
	\end{pmatrix}\right)$ est l'axe de rotation de $u$.\\
	Trouvons maintenant son angle $\theta$.\\
	On sait que : \[
		\Tr(M)=1+2\cos(\theta)=5\\
	\]
	Donc $\cos(\theta)=\frac{\frac{5}{3}-1}{2}=\frac{1}{3}$\\
	Donc $\theta=\pm \arccos\left(\frac{1}{3}\right)$.\\
	Pour trouver le signe de l'angle, il suffit de regarder le signe le déterminant de la famille $\left(\begin{pmatrix}
		1\\0\\1
	\end{pmatrix}, x, Mx\right)$ avec $x$ un vecteur non colinéaire à $\begin{pmatrix}
	1\\0\\1
	\end{pmatrix}$
	On prenant $x=\begin{pmatrix}
		0\\1\\0
	\end{pmatrix}$, on obtient que $\det(\begin{pmatrix}
	1\\0\\1
	\end{pmatrix}, x, Mx)<0$.\\
	Par conséquent, on a : $\theta<0$ i.e $\theta=-\arccos\left(\frac{1}{3}\right)$.
	\subsection{Produit mixte et produit vectoriel}
	\label{Produit mixte et produit vectoriel corrigé}
		\textcolor{blue}{\hyperref[Produit mixte et produit vectoriel]{[Enoncé]}}\\
	
	\subsection{Une caractérisation de la trace}
	\label{Une caractérisation de la trace corrigé}
	\textcolor{blue}{\hyperref[Une caractérisation de la trace]{[Enoncé]}}\\
	$\M_n(\R)$ est un espace euclidien donc d'après le théorème de représentation de Riesz, $\exists A\in \M_n(\R),\ \forall B\in \M_n(\R),\ \varphi(B)=\Tr(AB)$. Il suffit de montrer que $A$ est une matrice d'homothétie. Notons $E_{ij}$ la matrice dont tous les coefficients sont nuls, sauf celui en position $(i,j)$ qui vaut 1.\\
	On vérifie que $\forall i,j,k,l\in \crblanc{1}{n},\ k\ne j\implies E_ijE_kl=0$ et $E_{ij}E_{jl}=E_{il}$.\\
	Par hypothèse si $i\ne l,\ \varphi(E_{ij}E_{jl})=\varphi(E_{jl}E_{ij})=\varphi(0)=0$.\\
	D'autre part $\varphi(E_{ij}E_{jl})=\varphi(E_{ij})=\Tr(AE_{il})=\displaystyle\sum_{k=1}^n(AE_{il})_{kk}$.\\
	On calcule $(AE_{il})_{kk}=\displaystyle\sum_{r=1}^nA_{kr}(E_{il})_{rk}=\begin{cases}
		0&\mbox{si }k\ne l\\
		A_{li}&\mbox{sinon}
	\end{cases}$\\
	Donc $A_{li}=0$.\\
	Et de plus, $A_{11}=\varphi(E_{11})=\varphi(E_{1i}E_{i1})=\varphi(E_{i1}E_{1i})=\varphi(E_{ii})=A_{ii}$. Ainsi $A=\lambda I_n$ avec $\lambda=A_{11}$.\\
	Finalement, $\varphi=\lambda\Tr$.
	
	\subsection{Hyperplan de $\M_n(\K)$}
	\label{Hyperplan de Mn(K) corrigé}
	\textcolor{blue}{\hyperref[Hyperplan de Mn(K)]{[Enoncé]}}\\
	Soit $H$ un hyperplan de $\M_n(\K)$.\\
	\underline{$1^{\T{ère}}$ méthode :}\\
	Supposons que $H$ ne contient aucune matrice inversible.\\
	Ainsi puisque $I_n$ est inversible, on a \[H\bigoplus \Vect{I_n}=\M_n(\K)\]
	Montrons que $H$ contient toutes les matrices nilpotentes.\\
	Soit $N$ une matrice nilpotente.\\
	D'après ce qui précède, il existe $A\in H$ et $\lambda\in \K$ tel que \[N=A+\lambda I_n\]
	Puisque $A\in H$, en particulier elle n'est pas inversible donc il existe $X\in \M_n(\K)$ non nulle tel que $AX=0$.\\
	Cela implique que \[NX=\lambda X\]
	Donc $X$ est un vecteur propre de $N$, on a a fortiori $\lambda=0$, d'où $N=A\in H$.\\
	En particulier $H$ contient toutes les matrices $E_{i,j}$ pour $i\ne j$.\\
	Donc la matrice $J$ définie par \[J=\begin{pmatrix}
		0 & 1 &  &(0)\\
		&\ddots&\ddots&\\
		&(0)&\ddots&1\\
		1&&&0
	\end{pmatrix}\]
	est dans $H$.\\
	Or cette matrice est inversible (c'est l'identité à permutations près) donc on a montré par l'absurde que tout hyperplan de $\M_n(\K)$ rencontre $\text{GL}_n(\K)$.\\\\
	\underline{$2^{\T{nd}}$ méthode :}\\
	Soit $\varphi:\M_n(\K)\to \K$ une forme linéaire de noyau $H$. Dans $\K=\R$ le théorème de Riesz nous donne l'existence d'une matrice $A\in \M_n(\R)$ telle que $\forall B\in \M_n(\R),\ \varphi(B)=\Tr(AB)$. Montrons que le résultat reste vrai avec $\K=\C$.\\
	Pour $A\in \M_n(\C)$ on pose $\fonction{f_A}{\M_n(\C)}{\C}{B}{\Tr(\overline{A}^\top B)}$.\\
	Montrons que $\fonction{\Phi}{\M_n(\C)}{\M_n(\C)^*}{A}{f_A}$ est un isomorphisme.\\
	Par linéarité de la trace, de la transposition et de la conjugaison complexe $\Phi$ est une application linéaire. De plus $\dim(\M_n(\C))=\dim(\M_n(\C)^*)$. Il suffit donc de montrer que $\Phi$ est injective.\\
	Soit $A\in \ker\Phi$. $0=f_A(A)=\Tr(\overline{A}^\top A)=\displaystyle\sum_{1\leq i,j\leq n}|A_{ij}|^2$. Donc $A=0$.\\
	Ainsi dans le cas général, $\exists A\in \M_n(\K),\ \forall B\in \M_n(\K),\ \varphi(B)=\Tr(AB)$.\\
	Soient $P,Q\in \T{GL}_n(\K)$ telles que $\begin{pmatrix}
		I_r&0\\
		0&0
	\end{pmatrix}:=J_r=PAQ$ avec $r=\rg(A)$. Alors $\forall B\in \M_n(\K),\ \varphi(QBP)=\Tr(AQBP)=\Tr(PAQB)=\Tr(J_rB):=\tilde\varphi(B)$. Comme $P$ et $Q$ sont inversibles, le noyau de $\varphi$ contient une matrice inversible ssi le noyau de $\tilde\varphi$ en contient une.\\
	Pour $B=\begin{pmatrix}
		X&Y\\
		Z&T
	\end{pmatrix},\ J_rB=\begin{pmatrix}
	X&Y\\
	0&0
	\end{pmatrix}$. Il suffit de trouver une matrice inversible qui n'a que des $0$ sur sa diagonale. Par exemple la matrice $J$ définie par \[J=\begin{pmatrix}
	0 & 1 &  &(0)\\
	&\ddots&\ddots&\\
	&(0)&\ddots&1\\
	1&&&0
	\end{pmatrix}\] fonctionne (elle est inversible car c'est l'identité à permutations près).\\
	On a montré que $QJP\in H\cap\T{GL}_n(\K)$.
	
	\subsection{Exemple de symétrie orthogonale}
	\label{Exemple de symétrie orthogonale corrigé}
		\textcolor{blue}{\hyperref[Exemple de syméttrie orthogonale]{[Enoncé]}}\\
	D'après le cours, il est évident que $s$ est un symétrie.\\
	Montrons qu'elle est orthogonale.\\
	On remarque que $\Ker(s-Id)=\mathcal{S}_n(\R)$ et que $\Ker(s+Id)=\mathcal{A}_n(\R)$.\\
	Il suffit de montrer que $\mathcal{S}_n(\R)$ et $\mathcal{A}_n(\R)$ sont orthogonaux.
	Soit $S\in\mathcal{S}_n(\R)$ et $A\in\mathcal{A}_n(\R)$.\\
	\[\begin{cases}
		\Tr(A^\top S)=-\Tr(AS)\\
		\Tr(S^\top A)=\Tr(SA)=\Tr(AS)
	\end{cases}\]
	Donc, par symétrie du produit scalaire, on en déduit que $\langle A, S\rangle =0$.
	Donc $\mathcal{S}_n(\R)$ et $\mathcal{A}_n(\R)$ sont orthogonaux.\\
	Ainsi $s$ est une symétrie orthogonale.
	\subsection{Caractérisations des projections orthogonales}
	\label{Caractérisations des projections orthogonales corrigé}
	\textcolor{blue}{\hyperref[Caractérisations des projections orthogonales]{[Enoncé]}}\\
	On pose $F=\Ker(p-Id_E)$
	Montrons que i. $\Rightarrow$ ii.\\
	On sait que pour tout $x\in E$ $(p(x)-x)\in F^\perp$.\\
	Ainsi pour tout $x,y\in E$, \[\langle p(x)-x,p(y)\rangle=0 \text{ i.e } \langle p(x),p(y)\rangle=\langle x,p(y)\rangle\]
	Ainsi par symétrie du produit scalaire : 
	\begin{align*}
		\langle x,p(y)\rangle&=\langle p(x), p(y)\rangle\\
		&=\langle p(y), p(x)\rangle\\
		&=\langle y,p(x)\rangle\\
		&=\langle p(x), y\rangle
	\end{align*}
	Montrons que ii. $\Rightarrow$ iii.\\
	On a pour tout $x\in E$, \[\norme{p(x)}^2=\langle p(x),p(x)\rangle=\langle x, p(x)\rangle\]
	D'après l'inégalité de Cauchy-Schwarz 
	\[\norme{p(x)}^2\leq \norme{x}\norme{p(x)}\]
	Si $p(x)=0$, l'inégalité de vérifié automatiquement, sinon l'inégalité précédente donne le résultat.\\
	Montrons que iii. $\Rightarrow$ i.\\
	Soient $x\in \Ima(p)$ et $y\in\Ker(p)$.\\
	Si $y=0$, $x\perp y=0$.\\
	Supposons que $y\ne 0$.\\
	Soit $\lambda\in\R$.
	D'une part, \[\norme{p(x+\lambda y)}^2=\norme{x}^2\]
	D'autre part, \[\norme{x+\lambda y}^2)=\norme{x}^2+2\lambda\langle x, y\rangle+\lambda^2\norme{y}^2\]
	D'après ii., on a pour tout $\lambda\in\R$,
	\[\lambda\langle x,y\rangle+\lambda^2\norme{y}^2\geq 0\]
	On pose $P=\norme{y}^2X^2+\langle x,y\rangle X$.\\
	Ce polynôme est positif sur $\R$, il possède donc un discriminant négatif, ce qui correspond : \[\langle x,y\rangle^2\leq 0\]
	Donc \[\langle x,y\rangle=0\]
	Donc $p$ est orthogonale.
	
	\subsection{Norme d'une base orthogonale}
	\label{Norme d'une base orthogonale corrigé}
		\textcolor{blue}{\hyperref[Norme d'une base orthogonale]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Posons $\displaystyle u=\sum_{i=1}^{n}\frac{e_i}{\norme{e_i}^2}$.\\
		Notons $p_u$ le projecteur orthogonal sur $\Vect(u)$.\\
		On a : \[p_u(e_i)=\left\langle \frac{u}{\norme{u}}, e_i\right\rangle\frac{u}{\norme{u}}\]
		Par conséquent, on a : 
		\[\norme{p_u(e_i)}=\frac{|\langle u,e_i\rangle}{\norme{u}}\]
		Puisque $(e_1,\dots,e_n)$ est une base orthogonale, on a :
		\[\langle u,e_i\rangle=1\]
		Ainsi, pour tout $i\in\crblanc{1}{n}$ \[\norme{p_u(e_i)}=\frac{1}{\norme{u}}\]
		Les projetés orthogonaux de $e_1,\dots, e_n$ sur $\Vect(u)$ ont donc toute la même norme.
		\item Soit $u$ un vecteur répondant aux conditions de l'énoncé.\\
		On note $N$ la norme commune des vecteurs $p_u(e_i)$.\\
		On a donc pour tout $i\in\crblanc{1}{n}$ \[N=\frac{|\langle u,e_i\rangle}{\norme{u}}\]
		Comme la base $(e_1,\dots,e_n)$ est orthogonal, on a :
		\[\norme{u}^2=\sum_{i=1}^{n}\frac{\langle e_i,u\rangle^2}{\norme{e_i}^2}=\sum_{i=1}^{n}\frac{N^2\norme{u}^2}{\norme{e_i}^2}\]
		Comme $u$ est non nul, 
		\[N^2 = \left(\sum_{i=1}^{n}\frac{1}{\|e_i\|^2}\right)^{-1}\]
		Finalement, $N$ ne dépend du choix du vecteur $u$ et on a obtenu une expression de $N$ en fonction de $e_1,\dots, e_n$.\\
		
	\end{enumerate}
	\subsection{Matrice de Hilbert}
	\label{Matrice de Hilbert corrigé}
	\textcolor{blue}{\hyperref[Matrice de Hilbert]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item \begin{itemize}
			\item \textit{Symétrie} :\\ Soit $(P,Q)\in(\R_{n-1}[X])^2$.\\
			\[\langle P;Q\rangle=\int_{0}^{1}P(t)Q(t)dt=\int_{0}^{1}Q(t)P(t)dt=\langle Q ; P\rangle\]
			\item \textit{Bilinéarité} :\\ Soient $(P_1, P_2, Q)\in(\R_{n-1}[X])^3$ et $\lambda\in\R$.\\
			\[\langle P_1+\lambda P_2 ; Q\rangle=\int_{0}^{1}(P_1+\lambda P_2)(t)Q(t)dt=\int_{0}^{1}P_1(t)Q(t)dt+\lambda \int_{0}^{1}P_2(t)Q(t)=\langle P_1 ; Q\rangle +\lambda P_2 ; Q\rangle\]
			\item \textit{Positivité} :\\Soit $P\in\R_{n-1}[X]$.
			\[\langle P; P\rangle=\int_{0}^{1}P^2(t)dt \geq 0\] car $t\mapsto P(t)^2$ est positive sur $[0,1]$.\\
			\item \textit{Définie positive}:\\ Soit $P\in\R_{n-1}[X]$ tel que $\langle P ; P\rangle$.
			La fonction $t\mapsto P(t)^2$ est continue et positive sur $[0,1]$ donc puisque l'intégrale est nulle, on en déduit que la fonction est nulle.
		\end{itemize}
		Donc $\langle ; \rangle$ est un produit scalaire.
		\item 
		On en déduit aisément que $H$ est symétrique réelle.\\
		On pose $P_i=X^i$ pour tout $i\in\N$.\\
		On remarque que pour tout $(i,j)\in\N^2$, \[\langle P_i, P_j\rangle =\frac{1}{i+j+1}\]
		Ainsi, on peut donc réécrire $H$ comme étant la matrice des coefficients $\left(\langle P_i ; P_j \rangle\right)$.\\
		Soit $X=\begin{pmatrix}
			x_1\\ \vdots\\x_n
		\end{pmatrix}\in\M_{n,1}(\R)$
		\begin{align*}
			X^\top H X&=\sum_{i=1}^{n}\sum_{j=0}^{n}x_ix_j\langle P_i, P_j\rangle\\
			&=\langle \sum_{i=1}^{n}x_iP_i, \sum_{j=1}^{n}x_jP_j\rangle &\text{par bilinéarité du produit scalaire}\\
			&=\norme{P}^2 &\text{où} P=\sum_{i=0}^{n}x_iP_i\\
		\end{align*}
		Ainsi, on en déduit que $H$ est définie positive.
		
	\end{enumerate}
	\subsection{Racine carrée d'un endomorphisme auto-adjoint positif}
	\textcolor{blue}{\hyperref[Racine carrée d'un endomorphisme auto-adjoint positif]{[Enoncé]}}\\
	\label{Racine carrée d'un endomorphisme auto-adjoint positif corrigé}
	
	\subsection{Inégalité de convexité}
	\label{Inégalité de convexité corrigé}
		\textcolor{blue}{\hyperref[Inégalité de convexité]{[Enoncé]}}\\
	On note $\lambda_1, \dots, \lambda_n$ les valeurs à $M$ notée avec répétition.\\
	Puisque $M\in\mathcal{S}^+_n(\R)$, on sait que pour tout $i\in\crblanc{1}{n}$, $\lambda_i\geq 0$.\\
	Si l'un des $\lambda_i$ est nul, le résultat est vérifié immédiatement.\\
	Supposons qu'ils sont tous non nuls, on a donc d'après l'inégalité arithmético-géométrique (Tome I : I-47)que :
	\[\frac{1}{n}\sum_{i=1}^{n}\lambda_i\geq (\lambda_1\dots\lambda_n)^{1/n}\]
	i.e \[\frac{\Tr(M)}{n}\geq \det(M)^{1/n}\]
	\subsection{Inégalité à propos des matrices orthogonales}
	\label{Inégalité à propos des matrices orthogonales corrigé}
		\textcolor{blue}{\hyperref[Inégalité à propos des matrices orthogonales]{[Enoncé]}}\\
	
	\subsection{Inégalité de la trace}
	\label{Inégalité de la trace corrigé}
		\textcolor{blue}{\hyperref[Inégalité de la trace]{[Enoncé]}}\\
	
	\subsection{Inégalité de Hadamard}
	\label{Inégalité de Hadamard corrigé}
		\textcolor{blue}{\hyperref[Inégalité de Hadamard]{[Enoncé]}}\\
	
	\subsection{Perturbations}
	\label{Perturbations corrigé}
	\textcolor{blue}{\hyperref[Perturbations]{[Enoncé]}}\\
	
	\subsection{Somme de Cesàro de matrices orthogonales}
	\label{Somme de Cesaro de mattrices orthogonales corrigé}
	\textcolor{blue}{\hyperref[Somme de Cesàrp de matrices orthogonales]{[Enoncé]}}\\
	
	\subsection{Transformation de Cayley}
	\label{Transformation de Cayley corrigé}
		\textcolor{blue}{\hyperref[Transformation de Cayley]{[Enoncé]}}\\
		
	
	

	
	\subsection{Optimisation (1)}
	\label{Optimisation 1 corrigé}
		\textcolor{blue}{\hyperref[Optimisation 1]{[Enoncé]}}\\
	
	\subsection{Optimisation (2)}
	\label{Optimisation 2 corrigé}
		\textcolor{blue}{\hyperref[Optimisation 2]{[Enoncé]}}\\
	
	\subsection{Optimisation (3)}
	\label{Optimisation 3 corrigé}
		\textcolor{blue}{\hyperref[Optimisation 3]{[Enoncé]}}\\
	
	\subsection{Caractérisation des isométries anti-involutives}
	\label{Caractérisation des isométries anti-involutives corrigé}
	\textcolor{blue}{\hyperref[Caractérisation des isométries anti-involutives]{[Enoncé]}}\\
	
	\subsection{Symétrie de l'espace}
	\label{Symétrie de l'espace corrigé}
	\textcolor{blue}{\hyperref[Symétrie de l'espace]{[Enoncé]}}\\
	
	\subsection{Réflexion et rotation dans un plan \etoile{3}}
	\label{Réflexion et rotation dans un plan corrigé}
		\textcolor{blue}{\hyperref[Réflexion et rotation dans un plan]{[Enoncé]}}\\
	\begin{enumerate}
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $s_1$ et $s_2$ deux réflexions.
			Puisque $(\O(E),\circ)$ est un groupe $s_1\circ s_2\in \O(E)$
			\\On a: $\det(s_1)=\det(s_2)=-1$.
			\\Donc, $\det(s_1\circ s_2)=1$.
			Et puisque $E$ est de dimension 2, on en déduit que $s_1\circ s_2$ est une rotation.
			\item On note $\B=(e_1,e_2)$ une base orthonormée de $E$.
			\\Soit $r$ une rotation d'angle $\theta$ du plan $E$.
			\\On pose $s_1$ la réflexion par rapport à $D_1=\Vect(\cos(\theta/2)e_1
			+\sin(\theta/2)e_2)$ et $s_2$ la réflexion par rapport à $D_2=\Vect(\cos(3\theta/2)e_1
			+\sin(3\theta/2)e_2)$.
			En étudiant les matrices de $r$ et de $s_1\circ s_2$ dans la base $\mathcal{B}$, on a $r=s_1\circ s_2$.
		\end{enumerate}
		\item Puisque $\O(E)$ est un groupe, $r\circ s$ est une isométrie vectorielle.
		\\ On a: $\det(r\circ s)=-1$ donc $r\circ s$ est une réflexion.
		\\ Ainsi, $(r\circ s)^2=\Id_E$ et donc on en déduit que: 
		$\begin{cases}
			&r\circ s\circ r=s^{-1}=s\\
			&s\circ r \circ s=r^{-1}
		\end{cases}$
		\item Soient $s$ une réflexion de $E$ et $r$ une rotation de $E$ tel que $s\circ r=r\circ s$.
		Puisque $s$ est une réflexion, on a $s=s^{-1}$, donc $s\circ r \circ s=r$.
		\\Or d'après la question précédente, $s\circ r \circ s=r^{-1}$, d'où $r=r^{-1}$.
		Et donc, $r=\Id_E$  ou $r=-\Id_E$.
	\end{enumerate}
	
	\subsection{Similitude entre matrices orthogonales}
	\label{Similitude entre matrices orthogonales corrigé}
		\textcolor{blue}{\hyperref[Similitude entre matrices orthogonales]{[Enoncé]}}\\
	
	\subsection{Propriété de l'adjoint}
	\label{Propriété de l'adjoint corrigé}
	\textcolor{blue}{\hyperref[Propriété de l'adjoint]{[Enoncé]}}\\
	
	\subsection{Autour de l'adjoint}
	\label{Autour de l'adjoint corrigé}
		\textcolor{blue}{\hyperref[Autour de l'adjoint]{[Enoncé]}}\\
	
	\subsection{Somme d'une matrice orthogonale et de sa transposée}
	\label{Somme d'une matrice orthogonale et de sa transposée corrigé}
		\textcolor{blue}{\hyperref[Somme d'une matrice orthogonale et de sa transposée]{[Enoncé]}}\\
	
	\subsection{Déterminant d'une exponentielle}
	\label{Déterminant d'une exponentielle corrigé}
		\textcolor{blue}{\hyperref[Déterminant d'une exponentielle]{[Enoncé]}}\\
		On pose $Sp(A)=\{\lambda_1,\dots,\lambda_n\}$.
	On trigonalise $A$ dans $\M_n(\C)$ donc il existe $P\in \text{GL}_n(\C)$ et $T\M_n(\C)$ triangulaire supérieure tel que $A=PTP^{-1}$.
	On sait que $e^A$ est semblable à $e^T$ par continuité de l'exponentielle matricielle.\\
	Ainsi, $\det(e^A)=\det(e^T)$.
	Or $\det(e^T)=\prod_{i=1}^ne^{\lambda_i}=e^{\sum_{i=1}^{n}\lambda_i}=e^{\Tr(T)}$
	Et pour finir, puisque $A$ et $T$ sont semblables, on en déduit le résultat : \[\det(e^A)=e^{\Tr(A)}\]
	\subsection{Exponentielle de matrices antisymétrique}
	\label{Exponentielle de matrices antisymétriques corrigé}
	\textcolor{blue}{\hyperref[Exponentielle de matrices antisymétriques]{[Enoncé]}}\\
	
	\subsection{Théorème spectral}
	\label{Théorème spectral corrigé}
		\textcolor{blue}{\hyperref[Théorème spectral]{[Enoncé]}}\\
	
	\subsection{Réduction simultanée}
	\label{Réduction simultanée corrigé}
	\textcolor{blue}{\hyperref[Réduction simultanée]{[Enoncé]}}\\
	
	\subsection{Matrices antisymétriques réelles}
	\subsubsection{Rang et déterminant}
	\label{Rang et déterminant corrigé}
	\textcolor{blue}{\hyperref[Rang et déterminant]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item $\det A=\det(A^\top)=\det(-A)=(-1)^n\det A=-\det A$ donc $\det A=0$ càd $A$ n'est pas inversible.
		\item Soit $\lambda$ une valeur propre réelle de $A$ et $X$ un vecteur propre associé.\\
		$AX=\lambda X\implies X^\top A^\top=\lambda X^\top$ donc $X^\top A=-\lambda X^\top$.\\
		Ainsi d'une part $X^\top AX=\lambda X^\top X$ et d'autre part $X^\top AX=-\lambda X^\top X$.\\
		Or $X^\top X\ne 0$ car $X\ne 0$ donc $\lambda=-\lambda$ d'où $\lambda=0$.
		\item Comme $A$ est à coefficients réels, son polynôme caractéristique aussi. Donc les valeurs propres non réelles de $A$ sont conjugués deux à deux. Ainsi le rang de $A$, qui est le nombre de valeur propre non nulle et donc ici non réelle, est pair. On en déduit de plus que si $0$ n'est pas valeur propre de $A$ alors en notant $\lambda_1,\dots,\lambda_p\in \C$ tels que $\T{Sp}(A)=\{\lambda_1,\overline{\lambda_1},\dots,\lambda_p,\overline{\lambda_p}\}$, $\det A=\displaystyle\prod_{i=1}^p\lambda_i\prod_{i=1}^p\overline{\lambda_i}=\prod_{i=1}^p|\lambda_i|^2>0$. D'où $\det A\geq 0$.
	\end{enumerate}
	
	\subsubsection{Spectre}
	\label{Spectre corrigé}
	\textcolor{blue}{\hyperref[Spectre]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $\lambda$ une valeur propre réelle de $A$ et $X$ un vecteur propre associé.\\
		$AX=\lambda X\implies X^\top A^\top=\lambda X^\top$ donc $X^\top A=-\lambda X^\top$.\\
		Ainsi d'une part $X^\top AX=\lambda X^\top X$ et d'autre part $X^\top AX=-\lambda X^\top X$.\\
		Or $X^\top X\ne 0$ car $X\ne 0$ donc $\lambda=-\lambda$ d'où $\lambda=0$.
		\item En trigonalisant $A$ dans $\M_n(\C)$ on remarque que les valeurs propres de $A^2$ sont les carrés des valeurs propres de $A$.
		\item On remarque que $A^2$ est symétrique réelle donc d'après le théorème spectral $\T{Sp}_\C(A^2)\subset \R$. Ainsi $\forall \lambda\in \T{Sp}_\C(A),\ \lambda^2\in \R$. Donc $\lambda\in \R\cup i\R$. Mais puisque la seule valeur propre potentiellement réelle de $A$ est $0$, $\T{Sp}_\C(A)\subset i\R$.
	\end{enumerate}
	
	\subsubsection{Diagonalisabilité (1)}
	\label{Diagonalisabilité (1) corrigé}
	\textcolor{blue}{\hyperref[Diagonalisabilité (1)]{[Enoncé]}}\\
	\begin{enumerate}
		\item $(A^2)^\top=(A^\top)^2=(-A)^2=A^2$ donc $A^2\in \S_n(\R)$. D'après le théorème spectral elle est diagonalisable.
		\item Il est clair que $\Ker A\subset \Ker A^2$. Réciproquement fixons $X\in \Ker A^2$.\\
		$A^2X=0\implies -A^\top AX=0\implies X^\top A^\top AX=0\implies \proscal{AX}{AX}=0\implies AX=0\implies X\in \Ker A$.
		\item D'après les questions précédentes $\C^n=\Ker(A^2)\oplus\displaystyle\bigoplus_{\lambda\in \T{Sp}(A^2)\backslash\{0\}}\Ker(A^2-\lambda I_n)$.\\
		Or $\forall \lambda\in \T{Sp}(A^2)\backslash\{0\}$, $\exists \mu_\lambda\in \C^*,\ \lambda=\mu_\lambda^2$.\\
		Dans ce cas les polynômes $X-\mu_\lambda$ et $X+\mu_\lambda$ sont premiers entre eux et de produit $X^2-\lambda$. Et donc d'après le lemme des noyaux :\\
		$\C^n=\Ker(A)\oplus\displaystyle\bigoplus_{\lambda\in \T{Sp}(A^2)\backslash\{0\}}\Ker(A-\mu_\lambda I_n)\oplus\Ker(A+\mu_\lambda I_n)=\bigoplus_{\mu\in \T{Sp}(A)}\Ker(A-\mu I_n)$.\\
		càd $A$ est diagonalisable (il n'y a pas trop de termes, certains des noyaux en jeu sont nuls).
	\end{enumerate}
	
	\subsubsection{Diagonalisabilité (2)}
	\label{Diagonalisabilité (2) corrigé}
	\textcolor{blue}{\hyperref[Diagonalisabilité (2)]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item D'après le théorème du rang $\dim\Ker A=n-\rg A=\dim(\Ima A)^\bot$. Ensuite si $X\in \Ker A$ alors pour tout $Z=AY\in \Ima A$, $\proscal{X}{Z}=Z^\top X=Y^\top A^\top X=-Y^\top AX=0$. Donc $\Ker A\subset (\Ima A)^\bot$.\\
		Ainsi $\Ker A=(\Ima A)^\bot$.
		\item D'après la question précédente on se donne une base orthonormée $\B=(f_1,\dots,f_{n-r},e_1,\dots,e_r)$ adaptée à la décomposition $E=\Ker A\oplus \Ima A$. On note $B=\Mat_(e_1,\dots,e_p)$ et $O$ la matrice de passage de la base canonique à la base $\B$. Alors $O\in \O_n(\R)$ et
		$$A=O\begin{pmatrix}
			0&0\\
			0&B
		\end{pmatrix}O^\top$$
		En outre, $O\begin{pmatrix}
			0&0\\
			0&-B
		\end{pmatrix}O^\top-A=A^\top=O\begin{pmatrix}
			0&0\\
			0&B^\top
		\end{pmatrix}O^\top$ donc $B^\top=-B$ càd $B\in \A_r(\R)$.\\
		Enfin $r=\rg B$ donc $B$ est inversible.
		\item On sait que $\det B=\det(B^\top)=\det(-B)=(-1)^r\det B$ donc comme $B$ est inversible, $(-1)^r=1$ càd $r=\rg A$ est pair.
		\item D'après la question 2 il suffit de traiter le cas où $A$ est inversible.\\
		On remarque que $A^2$ est symétrique réelle. D'après le théorème spectral elle admet un polynôme annulateur $P$ scindé à racines simples.\\
		On pose $Q(X)=P(X^2)$. $Q$ est un polynôme annulateur de $A$ et $Q'(X)=2XP'(X^2)$ n'a pas de racine commune avec $Q$ donc $Q$ est scindé à racines simples dans $\C[X]$. Ainsi $A$ est diagonalisable dans $\M_n(\C)$.
	\end{enumerate}
	
	\subsubsection{Réduction}
	\label{Réduction corrigé}
	\textcolor{blue}{\hyperref[Réduction]{[Enoncé]}}\\
	On montre (cf.\ref{Diagonalisabilité (2)}) qu'on peut se ramener au cas où $A$ est inversible et de taille paire. Ensuite on montre (cf.\ref{Spectre}) que $\T{Sp}_\C(A)\subset i\R$.\\
	On montre par récurrence sur $n\in \N^*$ le résultat suivant : "Si $M\in \A_{2n}(\R)$ est inversible alors elle est orthogonalement semblable à une matrice diagonale par blocs avec différents blocs de la forme $M_\alpha=\begin{pmatrix}
		0&-\alpha\\
		\alpha&0
	\end{pmatrix}$ où $\alpha\in \R^*_+$".\\
	$n=2 :$ Fixons $A=\begin{pmatrix}
		a&b\\
		c&d
	\end{pmatrix}\in \A_2(\R)$ inversible et notons $f\in \L(\R^n)$ l'endomorphisme canoniquement associé. $A^\top=-A\implies a=d=0$ et $b=-c$ et $\det A\ne 0\implies c^2\ne 0\implies c\ne 0$. Si $c>0$ alors c'est bon avec $O=I_2$, sinon on pose $O=\begin{pmatrix}
		0&1\\
		1&0
	\end{pmatrix}\in \O_2(\R)$ et $A=O\begin{pmatrix}
		0&c\\
		-c&0
	\end{pmatrix}O^\top$ conclut.\\
	Supposons le résultat pour des matrices de taille $2n$ pour un certain $n\in \N^*$. Fixons $A\in \A_{2n+2}(\R)$ inversible et notons $f\in \L(\R^{2n+2})$ l'endomorphisme canoniquement associé.\\
	Comme on a supposé $A$ inversible et comme $\chi_A$ est scindé sur $\C$, il existe $\alpha\in \R$ tel que $i\alpha$ soit valeur propre de $A$. En outre comme $\chi_A\in \R[X]$, $-i\alpha$ est aussi valeur propre de $A$. On peut donc choisir $\alpha>0$.\\
	On fixe $Z\in \C^n$ un vecteur propre de $A$ associé à la valeur propre $i\alpha$. On note $Z=X+iY$ avec $X,Y\in \R^n$.\\
	$AZ=i\alpha Z\iff AX+iAY=i\alpha X-\alpha Y\iff \begin{cases}
		AX=-\alpha Y\\
		AY=\alpha X
	\end{cases}\implies (X\ne 0 \T{ et } Y\ne 0)$.\\
	Ceci montre que le sous-espace réel $F=\Vect_\R(X,Y)$ est stable par $f$. $f^*=-f$ stabilise alors aussi $F$. Donc $f=(f^*)^*$ stabilise $F^\bot$.\\
	Or $\dim F=2$, en effet s'il existe $(a,b)\in \R^2\backslash\{(0,0)\}$ tel que $aX+bY=0$ alors $\displaystyle A\left(\frac{a}{\alpha}Y-\frac{b}{\alpha}X\right)=aX+bY=0$ d'où $bY-aX=0$ puis en faisant la somme/différence des deux relations $a=b=0$ (en utilisant $X\ne 0$ et $Y\ne 0$).\\
	Ainsi $A$ est semblable par la matrice de passage de la base canonique à n'importe quelle base orthonormée adaptée à la décomposition $\R^n=F\oplus F^\bot$ à une matrice $A'=\begin{pmatrix}
		U&0\\
		0&V
	\end{pmatrix}$ où $U\in \A_2(\R)$ et $V\in \A_{2n}(\R)$. On conclut en appliquant l'hypothèse de récurrence à $V$ et l'initialisation à $U$.
	
	\subsection{Endomorphismes normaux}
	\label{Endomorphismes normaux corrigé}
	\textcolor{blue}{\hyperref[Endomorphismes normaux]{[Enoncé]}}\\
	
	\subsection{Réduction des normaux}
	\label{Réduction des normaux corrigé}
	\textcolor{blue}{\hyperref[Réduction des normaux]{[Enoncé]}}\\
	
	\subsection{Caractérisation des matrices symétriques positives}
	\label{Caractérisation des matrices symétriques positives corrigé}
	\textcolor{blue}{\hyperref[Caractérisation des matrices symétriques positives]{[Enoncé]}}\\
	
	\subsection{Matrices entières positives}
	\label{Matrices entières positives corrigé}
	\textcolor{blue}{\hyperref[Matrices entières positives]{[Enoncé]}}\\
	
	\subsection{Matrices binaires positives}
	\label{Matrices binaires positives corrigé}
	\textcolor{blue}{\hyperref[Matrices binaires positives]{[Enoncé]}}\\
	
	\subsection{Inégalité de Hoffman-Wielandt}
	\label{Inégalité de Hoffman-Wielandt corrigé}
	\textcolor{blue}{\hyperref[Inégalité de Hoffman-Wielandt]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item Soit $M\in\M_n(\R)$ et pour tout $P,Q\in\O_n(\R)$ donc :
		\[\Tr((PMQ)^\top PMQ)=\Tr(Q^\top M^\top P^\top PMQ)=\Tr(Q^\top M^\top M Q)=\Tr(MQQ^\top M^\top)=\Tr(MM^\top)\]
		On a bien : $\normep{F}{PMQ}=\normep{F}{M}$.
		\item D'après le théorème spectral, il existe $P,Q\in\O_n(\R)$, : $A=P^\top D_A P$ et $B=Q^\top D_B Q$.
		Par conséquent, 
		\begin{align*}
			\normep{F}{A-B}&=\normep{F}{P^\top D_A P-Q^\top D_B Q}\\
			&=\normep{F}{D_APQ^\top-PQ^\top D_B} &\text{d'après la question 1}
		\end{align*}
		On a bien le résultat souhaité car $PQ^\top$ est orthogonale.
		\item On prend la notation de la question précédente. Ainsi,
		\[\normep{F}{A-B}^2=\Tr((D_AP-PD_B)^\top(D_AP-PD_B))=\sum_{1\leq i,j\leq n}(D_AP-PD_B)_{i,j}^2=\sum_{1\leq i,j\leq n}p_{i,j}^2(\lambda_i(A)-\lambda_j(B))^2\]
		\item Soit $M\in\mathcal{B}_n(\R)$.\\
		On a pour tout $i,j\in\crblanc{1}{n}$, 
		\[0\leq m_{i,j}\leq \sum_{i=1}^{n}m_{i,j}=1\]
		On pose les fonctions suivantes :
		\begin{align*}
			f_{i,j}:M\mapsto m_{i,j}
			\varphi_i:M\mapsto \sum_{j=1}^{n}m_{i,j}
			\psi_{j}:M\mapsto \sum_{i=1}^{n}m_{i,j}
		\end{align*}
		On remarque que \[\mathcal{B}_n(\R)=\bigcap_{1\leq i,j\leq n}f^{-1}([0,1])\bigcap_{1\leq i\leq n}\varphi_{i}(\{1\})\bigcap_{1\leq j\leq n}\psi_{j}(\{1\})\]
		Les fonctions $f_{i,j}$, $\phi_i$ et $\psi_j$ sont continues car polynomiales.
		$\{1\}$ et $[0,1]$ sont des fermés de $\R$ donc par intersection de fermés (image réciproque de fermé par des applications continues), $\mathcal{B}_n(\R)$ est fermé.
		De plus, $\mathcal{B}_n(\R)$ est borné pour la norme $\norme: M\mapsto \max_{1\leq i,j\leq n}m_{i,j}$ par $1$.\\
		Puisque $\mathcal{M}_n(\R)$ est de dimension finie, $\mathcal{B}_n{\R}$ est un compact.\\
		On a que $f$ est continue (linéarité sur un espace de dimension finie) et que $\mathcal{B}_n(\R)$ est compact, ainsi d'après le théorème des bornes atteintes, $f$ admet un minimum sur $\B_n(\R)$.
		\item Soit $M\in\mathcal{M}_n(\R)$ et soit $x\in\R^+$.\\
		Par linéarité de $f$, on a : \\
		\begin{align*}
			&f(M+xE_{i,i}+xE_{j,k}-xE_{i,k}-xE_{j,k})-f(M)=x\left(f(E_{i,i}+f(E_{j,k}+f(E_{i,k})+f(E_{j,k})\right)\\
			&=x\left((\lambda_i(A)-\lambda_i(B))^2+(\lambda_j(A)-\lambda_k(B))^2-(\lambda_i(A)-\lambda_k(B))^2-(\lambda_j(A)-\lambda_k(B))^2\right)\\
			&=2x\left(-\lambda_i(A)\lambda_i(B)-\lambda_j(A)\lambda_k(B)+\lambda_i(A)\lambda_k(B)+\lambda_j(A)\lambda_k(B)\right)\\
			&=2x(\lambda_i(A)-\lambda_j(A))(\lambda_k(B)-\lambda_i(B))
		\end{align*}
		Puisque $j\geq i$ et $k\leq i$, on a :
		\begin{align*}f(M+xE_{i,i}+xE_{j,k}-xE_{i,k}-xE_{j,k})-f(M)&=x\left(f(E_{i,i}+f(E_{j,k}+f(E_{i,k})+f(E_{j,k})\right)\\&=2x(\lambda_i(A)-\lambda_j(A))(\lambda_k(B)-\lambda_i(B))\\ &\leq 0
		\end{align*}
		\item Montrons que $i_0$ est bien définie.\\
		On sait que $M\ne I_n$ ainsi il existe $i\in\crblanc{1}{n}$ tel que $m_{i,i}\ne 1$ par conséquent, $\{i\in\crblanc{1}{n}, m_{i,i}\ne 1\}$ est une partie de $\N$ non vide. De plus, cet ensemble est minoré par $1$. Donc, $i_0=\min \{i\in\crblanc{1}{n}, m_{i,i}\ne 1\}$ existe.\\
		Par définition, $m_{i_0,i_0}\ne 1$, ainsi puisque $M\in\mathcal{B}_n(\R)$, il existe $j,k\geq i_0$ tel que $m_{i_0,j},m_{k,i_0}>0$.\\
		On pose $x=\min m_{i_0,j},m_{k,i_0}$ et $M_1=M+xE_{i,i}+xE_{j,k}-xE_{i,k}-xE_{j,i}$.\\
		Les coefficients de $M_1$ sont tous positifs et les sommes des coefficients pour chaque colonne et chaque ligne fait $1$. Ainsi $M_1\in\mathcal{B}_n(\R)$.\\
		De plus, $f(M_1)\leq f(M)$ et on remarque que soit $m^1_{i_0,j}=0$ soit $m^1_{k,i_0}=0$.\\
		En itérant l'opération, on obtient une suite de matrices de $B_n(\R)$ telle que $f(M)\geq f(M_1)\geq \dots \geq f(M_p)$.\\
		Puisque le nombre de coefficients sur la colonne $i_0$ et la ligne $i_0$ est finie, le procédé va s'arrêter sur une matrice $M'\in\mathcal{B}_n(\R)$ tel que $f(M)\geq f(M')$  et $m_{j,j}=1$ pour tout $j\in\crblanc{1}{i_0}$.
		
		\item Soit $M\in\mathcal{B}_n(\R)$.\\
		On construit par récurrence $M^(k+1)=(M^k)'$ tant que $M^k\ne I_n$.\\
		L'algorithme va s'arrêter car le nombre de coefficients distincts de 1 est strictement décroissant. Et il s'arrêter sur la matrice $M^p=I_n$\\
		De plus, d'après la question précédente, on a $f(M^p)\leq\dots\leq f(M)$.\\
		Par conséquent, pour tout $M\in\mathcal{B}_n(\R)$ $f(I_n)\leq f(M)$.\\
		De plus, $I_n\in\B_n(\R)$.\\
		Par conséquent, \[f(I_n)=\min\{f(M), M\in\mathcal{B}_n(\R)\}\].
		\item On note $Q=(p_{i,j}^2)_{1\leq i,j\leq n}$. Comme $P\in\O_n(\R)$, on a $Q\in\mathcal{B}_n(\R)$.\\
		De plus, on a $\normep{F}{A-B}=f(Q)$ selon la question 3, or $f(Q)\geq f(I_n)$ selon la question 7.\\
		On en déduit donc que pour tout $A,B\in S_n(\R)$ : 
		\[\sum_{i=1}^{n}(\lambda_i(A)-\lambda_j(A))^2\leq \normep{F}{A-B}^2\]
		\end{enumerate}
	\subsection{Distance aux matrices de rang au plus r}
	\label{Distance aux matrices de rang au plus r corrigé}
	\textcolor{blue}{\hyperref[Distance aux matrices de rang au plus r]{[Enoncé]}}
	\subsection{Théorème de Courant-Fischer \etoile{3}}
	\label{Théorème de Courant-Fischer corrigé}
	\textcolor{blue}{\hyperref[Théorème de Courant-Fischer]{[Enoncé]}}\\
	D'après le théorème spectral il existe une base orthonormée $\B_0=(x_1,\dots,x_n)$ de $E$ formée de vecteurs propres de $u$. On note pour $i\in \crblanc{1}{n},\ \lambda_i$ la valeur propre associée à $x_i$. Fixons $p\in \crblanc{1}{n}$ et notons $V_p=\Vect(x_1,\dots,x_p)$.\\
	$\forall x=\displaystyle\sum_{i=1}^pa_ix_i\in V_p\cap S,\ \langle u(x),x\rangle=\left\langle\sum_{i=1}^pa_i\lambda_ix_i,\sum_{j=1}^pa_jx_j\right\rangle=\sum_{i=1}^p\sum_{j=1}^p\lambda_ia_ia_j\langle x_i,x_j\rangle=\sum_{k=1}^p\lambda_ka_k^2\geq \lambda_p\sum_{k=1}^pa_k^2=\lambda_p$.\\
	Donc $\lambda_p=\inf\limits_{x\in V_p\cap S}\langle u(x),x\rangle$.\\
	Soit maintenant $V\in \mathcal F_p$. Il faut montrer que $\inf\limits_{x\in V\cap S}\langle u(x),x\rangle\leq\inf\limits_{x\in V_p\cap S}\langle u(x),x\rangle$, il suffit donc de montrer qu'il existe un vecteur unitaire $z\in V$ tel que $\langle u(z),z\rangle\leq \lambda_p$.\\
	Notons $G_p=\Vect(x_p,\dots,x_n)$. D'après la formule de Grassmann,\\
	$\dim(V\cap G_p)=\dim(V)+\dim(G_p)-\dim(V+G_p)=n+1-\dim(V+G_p)$.\\
	Or $V+G_p\subset E$ donc $\dim(V+G_p)\leq n$. Ainsi $\dim(V\cap G_p)\geq 1$ : $\exists y\in V\cap G_p\backslash \{0\}$. Posons $z=\displaystyle\frac{y}{\norme{y}}$.\\
	$z\in G_p$ donc il existe des scalaires $b_p,\dots,b_n$ tels que $z=\displaystyle\sum_{i=p}^nb_ix_i$. Alors $\langle u(z),z\rangle=\displaystyle\sum_{i=p}^n\lambda_ib_i^2\leq \lambda_p\sum_{i=p}^nb_i^2=\lambda_p$.\\
	On a donc montré que :
	$$\lambda_p=\sup\limits_{F\in\mathcal F_p}\left(\inf\limits_{x\in F\cap S}\langle u(x),x\rangle\right)$$
	Pour la deuxième égalité on peut refaire un raisonnement similaire ou alors on applique le résultat à $v=-u$.\\
	$v$ est autoadjoint et ses valeurs propres sont $-\lambda_n\geq\dots\geq-\lambda_1$.\\
	On sait alors que $-\lambda_p=\sup\limits_{F\in\mathcal F_{n+1-p}}\left(\inf\limits_{x\in F\cap S}\langle v(x),x\rangle\right)=\sup\limits_{F\in\mathcal F_{n+1-p}}\left(-\sup\limits_{x\in F\cap S}\langle u(x),x\rangle\right)=-\inf\limits_{F\in\mathcal F_{n+1-p}}\left(\sup\limits_{x\in F\cap S}\langle u(x),x\rangle\right)$.\\
	Ainsi $\lambda_p=\inf\limits_{F\in\mathcal F_{n+1-p}}\left(\sup\limits_{x\in F\cap S}\langle u(x),x\rangle\right)$
	
	\subsection{Principe de Ky-Fan}
	\label{Principe de Ky-Fan corrigé}
	\textcolor{blue}{\hyperref[Principe de Ky-Fan]{[Enoncé]}}\\
	
	\subsection{Théorème de Cartan-Dieudonnée}
	\label{Théorème de Cartan-Dieudonnée corrigé}
		\textcolor{blue}{\hyperref[Théorème de Cartan-Dieudonnée]{[Enoncé]}}\\
	
	\subsection{Relation d'ordre des matrices symétriques}
	\label{Relation d'ordre des matrices symétriques corrigé}
	\textcolor{blue}{\hyperref[Relation d'ordre des matrices symétriques]{[Enoncé]}}\\
	
	\newpage
\section{Correction Décompositions matricielles}
	
	\subsection{Décomposition de Dunford \etoile{2}}
	\label{Décomposition de Dunford corrigé}
		\textcolor{blue}{\hyperref[Décomposition de Dunford]{[Enoncé]}}\\
	\begin{enumerate}
		\item On sait que $\chi_M=\displaystyle\prod_{\lambda\in \text{Sp}(M)}(X-\lambda)^{m_\lambda}$.\\
		Si $\lambda$ et $\mu$ sont deux valeurs propres distinctes de $M$ alors $(X-\lambda)^{m_\lambda}$ et $(X-\mu)^{m_\mu}$ sont premiers entre eux.\\
		Ainsi d'après le lemme des noyaux, $\Ker(\chi_M(M))=\displaystyle\bigoplus_{\lambda\in \text{Sp(M)}}\Ker((M-\lambda I_n)^{m_\lambda})$.\\
		De plus d'après le théorème de Cayley-Hamilton, $\chi_M(M)=0_n$. Ainsi,
		$$\C^n=\bigoplus_{\lambda\in \text{Sp}(M)}F_\lambda$$
		\item Notons $u$ l'endomorphisme canoniquement associé à $M$. Si $\lambda\in \text{Sp}(M)$, $M$ et $(M-\lambda I_n)^{m_\lambda}$ commutent donc $F_\lambda$ est stable par $u$. On note $u_\lambda$ l'endomorphisme induit par $u$ sur $F_\lambda$.\\
		On écrit alors $u_\lambda=\lambda\Id_{F_\lambda}+(u_\lambda-\lambda\Id_{F_\lambda})$. Par définition de $F_\lambda, (u_\lambda-\lambda\Id_{F_\lambda})^{m_\lambda}=0$. Donc $n_\lambda=u-\lambda\Id_{F_\lambda}$ est nilpotent. On note $N_\lambda$ la matrice de $n_\lambda$ dans une base $\B_\lambda$ de $F_\lambda$.\\
		On note enfin $\text{Sp}(M)=\{\lambda_1,\dots,\lambda_p\}$, $\alpha_i=\dim(F_{\lambda_i})$ pour tout $i\in \crblanc{1}{p}$ et $\B$ la base de $\C^n$ obtenue par concaténation des bases $\B_{\lambda_1},\dots,\B_{\lambda_p}$ dans cet ordre.\\
		On peut écrire $M=D+N$ avec les matrices diagonales par blocs $$D=P\begin{pmatrix}
			\lambda_1I_{\alpha_1}& & & & \\
			& \ddots & & 0 & \\
			& & \lambda_iI_{\alpha_i} & & \\
			& 0 & & \ddots & \\
			& & & & \lambda_pI_{\alpha_p}
		\end{pmatrix}P^{-1}\text{ et } N=P\begin{pmatrix}
			N_{\lambda_1}& & & & \\
			& \ddots & & 0 & \\
			& & N_{\lambda_i} & & \\
			& 0 & & \ddots & \\
			& & & & N_{\lambda_p}
		\end{pmatrix}P^{-1}$$ où $P$ désigne la matrice de passage de la base $\B$ à la base canonique de $\C^n$.
		On peut alors affirmer que $D$ est diagonalisable ($P^{-1}DP$ est diagonale), que $N$ est nilpotente car $N^k=P\begin{pmatrix}
			N_{\lambda_1}^k& & & & \\
			& \ddots & & 0 & \\
			& & N_{\lambda_i}^k & & \\
			& 0 & & \ddots & \\
			& & & & N_{\lambda_p}^k
		\end{pmatrix}P^{-1}=0$ pour $k$ le $\ppcm$ des indices de nilpotence de $N_{\lambda_1},\dots,N_{\lambda_p}$ par exemple, et que $N$ et $D$ commutent :
		$$DN=P\begin{pmatrix}
			\lambda_1N_{\lambda_1}& & & & \\
			& \ddots & & 0 & \\
			& & \lambda_iN_{\lambda_i} & & \\
			& 0 & & \ddots & \\
			& & & & \lambda_pN_{\lambda_p}
		\end{pmatrix}P^{-1}=ND$$
		\item Supposons que $M=D+N=D'+N'$ soient deux décompositions de Dunford de $M$. Alors $D-D'=N-N'$.\\
		$D$ et $D'$ commutent car ce sont des polynômes en $M$. De même pour $N$ et $N'$.\\
		Alors comme $D$ et $D'$ sont diagonalisables, elle sont simultanément diagonalisables (cf. \ref{Codiagonalisation}). On en déduit que $D-D'$ est diagonalisable.\\
		D'autre part, $N-N'$ est nilpotente. En effet comme $N$ et $N'$ commutent, si l'on note $k$ et $k'$ deux entiers naturels non nuls pour lesquels $N^k={N'}^{k'}=0$ alors on a :
		$$(N-N')^{k'+k}=\sum_{i=0}^{k'+k}(-1)^i\binom{k'+k}{i}{N'}^iN^{k'+k-i}=\sum_{i=0}^{k'}(-1)^i\binom{k'+k}{i}{N'}^iN^{k'+k-i}+\sum_{i=k'+1}^{k'+k}(-1)^i\binom{k'+k}{i}{N'}^iN^{k'+k-i}$$
		Or $\forall i\in \crblanc{0}{k'},\ k+k'-i\geq k\implies N^{k+k'-i}=0$ et $\forall i\in \crblanc{k'+1}{k'+k},\ i\geq k'\implies {N'}^i=0$.\\
		Donc $(N-N')^{k'+k}=0$.\\
		Ainsi $N-N'=D-D'$ est diagonalisable et nilpotente ce qui implique $D=D'$ et $N=N'$.
	\end{enumerate}
	
	\subsubsection{Diagonalisabilité de l'exponentielle d'une matrice \etoile{3}}
	Supposons que $A$ est diagonalisable. Alors il existe une matrice inversible $P$ et une matrice diagonale $D=\diag(\lambda_1,\dots,\lambda_n)$ telles que $A=PDP^{-1}$.\\
	Posons pour $p\in \N,\ S_p=\displaystyle\sum_{k=0}^p\frac{A^k}{k!}$.\\
	$\forall p\in \N,\ S_p=\displaystyle\sum_{k=0}^p\frac{PD^kP^{-1}}{k!}=P\left[\sum_{k=0}^p\diag\left(\displaystyle\frac{\lambda_1^k}{k!},\dots,\frac{\lambda_n^k}{k!}\right)\right]P^{-1}$.\\
	Or l'application $M\in \M_n(\C)\mapsto PMP^{-1}$ est continue car linéaire sur $\M_n(\C)$ qui est de dimension finie. Ainsi, $S_p\upfty{\longrightarrow}e^A$ et $\displaystyle\sum_{k=0}^p\diag\left(\displaystyle\frac{\lambda_1^k}{k!},\dots,\frac{\lambda_n^k}{k!}\right)\upfty{\longrightarrow}\diag\left(e^{\lambda_1},\dots,e^{\lambda_n}\right)$ donnent par passage à la limite $e^A=Pe^DP^{-1}=P\diag\left(e^{\lambda_1},\dots,e^{\lambda_n}\right)P^{-1}$.\\
	D'où $e^A$ est diagonalisable.\\
	Réciproquement supposons que $e^A$ est diagonalisable. Par unicité de la décomposition de Dunford $e^A=D'+N'$ on sait que $D'=e^A$ et $N'=0$.\\
	On écrit la décomposition de Dunford $A=D+N$ de $A$ et on va essayer de montrer que $N=0$. Pour cela on veut exprimer la décomposition de $e^A$ en fonction de celle de $A$.\\
	Comme $D$ et $N$ commutent on sait que $e^A=e^De^N=e^D+e^D(e^N-I_n)=e^D+e^D\displaystyle\sum_{k=1}^{+\infty}\frac{N^k}{k!}$.\\
	On sait déjà d'après ce qui a été fait précédemment que $e^D$ est diagonalisable. De plus $e^D$ et $e^D(e^N-I_n)$ commutent, il est alors naturel d'essayer de montrer que le deuxième terme est nilpotent.\\
	Tout d'abord, en notant $p\geq 1$ l'indice de nilpotence de $N$ on sait que $\displaystyle\sum_{k=1}^{+\infty}\frac{N^k}{k!}=\sum_{k=1}^{p-1}\frac{N^k}{k!}$. Donc $e^D(e^N-I_n)=e^DN\displaystyle\sum_{k=1}^{p-1}\frac{N^{k-1}}{k!}$. Comme $D$ et $N$ commutent, $e^D$ et $N$ commutent et donc $e^D$ commute avec tous les polynômes en $N$. Aussi, $N$ commute avec tous les polynômes en $N$. Ainsi $\left(e^D(e^N-I_n)\right)^p=\left(e^D\right)^pN^p\left(\displaystyle\sum_{k=1}^{p-1}\frac{N^{k-1}}{k!}\right)^p=0$.\\
	Ainsi par unicité de la décomposition de Dunford, $e^D(e^N-I_n)=N'=0$. Or on sait que $\det\left(e^D\right)=\det\left(\diag\left(e^{\lambda_1},\dots,e^{\lambda_n}\right)\right)=\displaystyle\prod_{i=1}^ne^{\lambda_i}=\exp\left(\sum_{i=1}^n\lambda_i\right)=e^{\Tr(A)}>0$. Donc $e^D$ est inversible.\\
	On en déduit que $e^N-I_n=0$ i.e $e^N=I_n$ i.e $\displaystyle\sum_{k=1}^{p-1}\frac{N^k}{k!}=0$. Or $p$ étant l'indice de nilpotence de $N$ on sait que le polynôme minimal de $N$ est $X^p$. Le polynôme $P=\displaystyle\sum_{k=1}^{p-1}\frac{X^k}{k!}$ annule $N$ et est de degré inférieur strictement à $p$, c'est donc le polynôme nul i.e $p=1$ i.e $N=0$.\\
	Ainsi $A=D$ est diagonalisable.
	
	\subsubsection{Surjectivité de l'exponentielle matricielle \etoile{4}}
	\begin{enumerate}[leftmargin=*]
		\item Notons $M=D+N$ la décomposition de Dunford de $M$. On sait que $\text{Sp}(M)=\text{Sp}(D)$ donc $D$ est inversible.\\
		Posons $U=I_n+D^{-1}N$ de sorte que $M=DU$. $N$ et $D$ commutent donc $\forall k\in \N^*,\ (D^{-1}N)^k=D^{-k}N^k$. Donc $D^{-1}N=U-I_n$ est nilpotente c'est à dire $U$ est unipotente.\\
		D'après le résultat sur la décomposition de Dunford on sait que $D$ et $U$ commutent et que $D$ est diagonalisable.\\
		Enfin, si $M=DU=D'U'$ sont deux décompositions de $M$ alors $\det(M)\ne 0\implies \det(U')\ne 0$ d'où $D{D'}^{-1}=U'U^{-1}$. De plus, on sait aussi que $D,U,D',U'$ sont des polynômes en $M$. Ils commutent donc tous deux à deux. Ainsi $D$ et $D'$ sont codiagonalisables (cf. \ref{Codiagonalisation}) d'où $D{D'}^{-1}$ est diagonalisable puis $I_n-D{D'}^{-1}$ est diagonalisable. De plus, $I_n-U'U^{-1}=(I_n-U')(U^{-1}-I_n)+2I_n-U'-U^{-1}=(I_n-U')(I_n-U)U^{-1}+(I_n-U')+(U-I_n)U^{-1}$. Chacun des termes est une matrice nilpotente : $\forall k\in \N^*,\ \left((I_n-U')(I_n-U)U^{-1}\right)^k=(I_n-U')^k(I_n-U)^kU^{-k}$ et $\left((U-I_n)U^{-1}\right)^k=(-1)^k(I_n-U)^kU^{-k}$.\\
		Montrons que la somme de deux matrices nilpotentes $A$ et $B$ est nilpotente. On note $p,q\in \N^*$ tels que $A^p=B^q=0$. Alors,
		$$(A+B)^{p+q}=\sum_{i=0}^{p+q}\binom{p+q}{i}A^iB^{p+q-i}=\sum_{i=0}^{p}\binom{p+q}{i}A^iB^{p+q-i}+\sum_{i=p+1}^{p+q}\binom{p+q}{i}A^iB^{p+q-i}$$
		Or $\forall i\in \crblanc{0}{p},\ p+q-i\geq q\implies B^{p+q-i}=0$ et $\forall i\in \crblanc{p+1}{p+q},\ i\geq p\implies A^i=0$. Donc $(A+B){p+q}=0$.\\
		Ainsi $(I_n-U')+(U-I_n)U^{-1}$ est nilpotente puis $I_n-U'U^{-1}$ est nilpotente.\\
		Ainsi $I_n-D{D'}^{-1}=I_n-U'U^{-1}$ est diagonalisable et nilpotente d'où $D{D'}^{-1}=I_n$ et $U'U^{-1}=I_n$ c'est à dire $D=D'$ et $U=U'$.
		\item Soit $M\in \M_n(\C)$. $M$ est trigonalisable donc il existe une matrice inversible $P$ et une matrice triangulaire supérieure $T$ telles que $M=PTP^{-1}$. On note $\lambda_1,\dots,\lambda_n$ les coefficients diagonaux de $T$. Par continuité de l'application linéaire $A\in \M_n(\C)\mapsto PAP^{-1}$, $e^M=Pe^TP^{-1}$. Donc $\det(e^M)=\det(e^T)$ est le produit des coefficients diagonaux de $e^T$. Or le calcul montre que les coefficients diagonaux de $e^T$ sont $e^{\lambda_1},\dots,e^{\lambda_n}$.\\
		Ainsi $\det(M)=\displaystyle\prod_{i=1}^ne^{\lambda_i}=\exp\left(\sum_{i=1}^n\lambda_i\right)=e^{\Tr(M)}>0$. D'où $e^M\in \text{GL}_n(\C)$ et $\det$ est bien une application de $\M_n(\C)$ dans GL$_n(\C)$.\\
		Ensuite, donnons nous une matrice $M\in \text{GL}_n(\C)$. On écrit $M=\Delta U=U\Delta$ avec $\Delta$ diagonalisable et $U$ unipotente. Supposons qu'il existe $A\in \M_n(\C)$ telle que $e^A=M$. On écrit $A=D+N$. Comme $D$ et $N$ commutent on sait que $e^A=e^De^N=e^Ne^D$. On sait aussi qu'en notant $D=P\diag(\mu_1,\dots,\mu_n)P^{-1}$ on a $e^D=P\diag(e^{\mu_1},\dots,e^{\mu_n})P^{-1}$ qui est donc diagonalisable.\\
		Enfin, notons $p$ l'indice de nilpotence de $N$. on calcule $e^N=\displaystyle\sum_{k=0}^{+\infty}\frac{N^k}{k!}=I_n+\sum_{k=1}^{p-1}\frac{N^k}{k!}$. Donc $I_n-e^N=-N\displaystyle\sum_{k=1}^{p-1}\frac{N^{k-1}}{k!}$. $N$ commute avec $\displaystyle\sum_{k=1}^{p-1}\frac{N^{k-1}}{k!}$ qui est un polynôme en $N$. donc $I_n-e^N$ est nilpotente c'est à dire $e^N$ est unipotente.\\
		Par unicité de la décomposition de la question $1$, $e^D=\Delta$ et $e^N=U$.\\
		Il faut et suffit donc de choisir $D$ et $N$ qui commutent telles que $e^D=\Delta$ et $e^N=U$ puis de poser $A=D+N$.\\
		$\Delta$ est diagonalisable on peut donc noter $\Delta=Q\diag(\lambda_1,\dots,\lambda_n)Q^{-1}$. Comme $\det(M)\ne 0,\ \det(\Delta)\ne 0$ c'est à dire $\forall i\in \crblanc{1}{n},\ \lambda_i\ne 0$. Or on sait que $\exp:\C\to\C^*$ est surjective. On peut donc choisir pour tout $i\in \crblanc{1}{n},\ \mu_i\in \C$ tel que $e^{\mu_i}=\lambda_i$. $D=P\diag(\mu_1,\dots,\mu_n)P^{-1}$ vérifie $e^D=\Delta$.\\
		Ensuite, on écrit $U=I_n-(I_n-U)$. On va s'inspirer du développement du logarithme dans $\R$ : $\forall x\in ]-1,1[,\ \ln(1-x)=-\displaystyle\sum_{k=1}^{+\infty}\frac{x^k}{k}$.\\
		On pose $N=\displaystyle\sum_{k=1}^{p-1}\frac{(I_n-U)^k}{k}=(I_n-U)\displaystyle\sum_{k=1}^{p-1}\frac{(I_n-U)^{k-1}}{k}$ où $p$ désigne l'indice de nilpotence de $I_n-U$. Montrons que $e^N=U$.\\
		On pose $F:x\mapsto \displaystyle\sum_{k=1}^{p-1}\frac{x^k}{k}$ et $G:x\mapsto \displaystyle\sum_{k=0}^{p-1}\frac{x^k}{k!}$.\\
		On sait que $\ln(1-x)\uxzero{=}F(x)+\smallo{x^p}$ et $e^x\uxzero{=}G(x)+\smallo{x^p}$. Donc par composition $e^{\ln(1-x)}=1-x\uxzero{=}G\circ F(x)+\smallo{x^p}$ i.e $G\circ F(x)\uxzero{=}1-x+\smallo{x^p}$. Cette expression permet de connaître le développement du polynôme $G\circ F$ jusqu'au terme en $X^p$.\\
		Ainsi, comme $(I_n-U)^p=0$ on en déduit $e^N=G\circ F(I_n-U)=U$.\\
		On pose enfin $A=D+N$, on sait que $D$ et $N$ commutent car $e^N=U$ commute avec $e^D=\Delta$ et par conséquent $e^A=e^De^N=\Delta U=M$.\\
		\textit{\underline{Remarque :} On peut montrer de même que $F\circ G(I_n-U)=U$ et donc montrer que $M\mapsto\displaystyle\sum_{k=1}^{+\infty}\frac{M^k}{k}$ réalise une bijection des matrices nilpotentes dans les matrices unipotentes.}
	\end{enumerate}
	
	\subsubsection{Opérateur de commutation \etoile{3}}
	\begin{enumerate}[leftmargin=*]
		\item On note $E_{ij}$ la matrice dont tous les coefficients sont nuls sauf celui en position $(i,j)$ qui vaut $1$. Fixons $(i,j)\in \crblanc{1}{n}$.\\
		$\operatorname{Comm}_A(PE_{ij}P^{-1})=APE_{ij}P^{-1}-PE_{ij}P^{-1}A=PDE_{ij}P^{-1}-PE_{ij}P^{-1}DP^{-1}=P(DE_{ij}-E_{ij}D)P^{-1}=P(\lambda_iE_{ij}-\lambda_jE_{ij})P^{-1}=(\lambda_i-\lambda_j)PE_{ij}P^{-1}$.\\
		Ainsi $(PE_{ij}P^{-1})_{1\leq i,j\leq n}$ est une famille de vecteurs propres de $\operatorname{Comm}_A$. De plus, l'application $M\in \M_n(\C)\mapsto PMP^{-1}$ est un isomorphisme (de réciproque $M\in \M_n(\C)\mapsto P^{-1}MP$).\\
		Donc comme $(E_{ij})_{1\leq i,j\leq n}$ est une base de $\M_n(\C)$, $(PE_{ij}P^{-1})_{1\leq i,j\leq n}$ est une base de $\M_n(\C)$.\\
		Par conséquent $\operatorname{Comm}_A$ est diagonalisable.
		\item On note $A=D+N$ la décomposition de Dunford de $A$.\\
		On vérifie aisément que $\operatorname{Comm}_A=\operatorname{Comm}_D+\operatorname{Comm}_N$ et que $\operatorname{Comm}_D\operatorname{Comm}_N=\operatorname{Comm}_N\operatorname{Comm}_D$ (conséquence directe du fait que $D$ et $N$ commutent). On sait de plus d'après ce qui a été fait précédemment que $D$ diagonalisable entraîne $\operatorname{Comm}_D$ diagonalisable.\\
		Montrons que $\operatorname{Comm}_N$ est nilpotente. Fixons $B\in \M_n(\C)$.\\
		Montrons par récurrence sur $p\in \N^*$ qu'il existe des entiers $\beta_{p,0},\dots,\beta_{p,p}$ (qui ne dépendent pas de $B$) tels que $\operatorname{Comm}_N^p(B)=\displaystyle\sum_{i=0}^p\beta_{p,i}N^{p-i}BN^i$.\\
		$p=1:$\\
		$\operatorname{Comm}_N(B)=NB-BN=\displaystyle\sum_{i=0}^1N^{1-i}BN^i$. La propriété est vraie en posant $\beta_{1,0}=\beta_{1,1}=1$.
		Supposons la propriété vraie à un certain rang $p\in \N^*$.
		\begin{align*}
			\operatorname{Comm}_N^{p+1}(B)&=N\left(\sum_{i=0}^p\beta_{p,i}N^{p-i}BN^i\right)-\left(\sum_{i=0}^p\beta_{p,i}N^{p-i}BN^i\right)N\\
			&=\sum_{i=0}^p\beta_{p,i}N^{p+1-i}BN^i-\sum_{i=0}^p\beta_{p,i}N^{p-i}BN^{i+1}\\
			&=\sum_{i=0}^p\beta_{p,i}N^{p+1-i}BN^i-\sum_{i=1}^{p+1}\beta_{p,i-1}N^{p+1-i}BN^i\\
			&=\beta_{p,0}N^{p+1}B+\sum_{i=1}^p(\beta_{p,i}+\beta_{p,i-1})N^{p-i}BN^i+\beta_{p,p}BN^{p+1}
		\end{align*}
		On pose $\beta_{p+1,0}=\beta_{p,0},\ \beta_{p+1,p+1}=\beta_{p,p}$ et, $\forall i\in \crblanc{1}{p},\ \beta_{p+1,i}=\beta_{p,i}+\beta_{p,i-1}$.\\
		On a bien $\operatorname{Comm}_N^{p+1}(B)=\displaystyle\sum_{i=0}^{p+1}\beta_{p+1,i}N^{p+1-i}BN^i$ et $\forall i\in \crblanc{0}{p+1},\ \beta_{p+1,i}\in \Z$.\\
		On en déduit par récurrence simple que $\forall p\in \N^*,\ \exists (\beta_{p,0},\dots,\beta_{p,p})\in \Z^{p+1},\ \operatorname{Comm}_A^p=B\in \M_n(\C)\mapsto \displaystyle\sum_{i=0}^{p}\beta_{p,i}N^{p-i}BN^i$.\\
		Ainsi pour $k$ l'indice de nilpotence de $N$, si $B\in \M_n(\C),\ \operatorname{Comm}_N^{2k}(B)=\displaystyle\sum_{i=0}^{k-1}\beta_{2k,i}N^{2k-i}BN^i+\sum_{i=k}^{2k}\beta_{2k,i}N^{2k-i}BN^i$.\\
		Or $\forall i\in \crblanc{1}{k-1},\ 2k-i\geq k\implies N^{2k-i}=0$ et $\forall i\in \crblanc{k}{2k},\ i\geq k\implies N^i=0$.\\
		Finalement $\operatorname{Comm}_N^{2k}(B)=0$ quel que soit $B\in \M_n(\C)$.\\
		On en déduit que $\operatorname{Comm}_N^{2k}=0$ et en particulier que $\operatorname{Comm}_N$ est nilpotente.\\
		Ceci montre que $\operatorname{Comm}_A=\operatorname{Comm}_D+\operatorname{Comm}_N$ est la décomposition de Dunford de $\operatorname{Comm}_A$.\\
		Ainsi comme $\operatorname{Comm}_A$ est diagonalisable, $\operatorname{Comm}_N$ doit être nul.\\
		Ceci impose que $N$ doit commuter avec toutes les matrices de $\M_n(\C)$.\\
		$N$ est donc une matrice scalaire $N=\alpha I_n$ (cf. \ref{Centre de Mn(K)}). Etant nilpotente, elle est nulle et $A=D$ est diagonalisable.
	\end{enumerate}
	
	\subsubsection{Deux applications non continues}
	
	\subsection{Décomposition polaire \etoile{3}}
	\label{Décomposition polaire corrigé}
		\textcolor{blue}{\hyperref[Décomposition polaire]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item \begin{enumerate}[label=\alph*.]
			\item On remarque que $vu=v^3=uv$. D'après le cours les sous-espaces propres de $u$ sont stables par $v$. Notons pour $\lambda\in \text{Sp}(u),\ E_\lambda=\Ker(u-\lambda\Id_E)$ et $v_\lambda$ l'endomorphisme induit par $v$ sur $E_\lambda$. $u$ est diagonalisable d'après le théorème spectral donc $\C^n=\displaystyle\bigoplus_{\lambda\in \text{Sp}(u)}E_\lambda$. Fixons $\lambda\in \text{Sp(u)}$.\\
			d'après le théorème spectral $v$ est diagonalisable et donc $v_\lambda$ est diagonalisable. Soit $\mu$ une valeur propre de $v_\lambda$ et $x$ un vecteur propre associé.\\
			$\mu^2x=v_\lambda^2(x)=u(x)=\lambda x$. $\mu^2=\lambda$. Or $u$ et $v$ sont autoadjoints définis positifs donc $\mu>0$ et $\lambda>0$. Ceci impose $\mu=\sqrt{\lambda}$.\\
			Finalement $v_\lambda$ est diagonalisable et admet pour seule valeur propre $\sqrt{\lambda}$. On en déduit que $v_\lambda=\sqrt{\lambda}I_{\dim(E_\lambda)}$.\\
			On a déterminé $v$ sur des sous-espaces supplémentaires dans $\C^n$. $v$ est donc déterminé sur $\C^n$.
			\item Notons $\text{Sp}(u)=\{\lambda_1,\dots,\lambda_p\}$. On utilise les polynômes interpolateurs de Lagrange.\\
			On pose pour $1\leq j\leq p,\ L_j(X)=\displaystyle\prod_{\substack{1\leq i\leq n\\i\ne j}}\frac{X-\lambda_i}{\lambda_j-\lambda_i}$ de sorte que $\forall j\in \crblanc{1}{p},\ L_j(\lambda_i)=\begin{cases}1&\mbox{si }i=j\\0&\mbox{sinon}\end{cases}$.\\
			On vérifie alors que $v=\displaystyle\left(\sum_{j=1}^p\sqrt{\lambda_j}L_j\right)(u)$.
		\end{enumerate}
		\item \begin{enumerate}[label=\alph*.]
			\item Tout d'abord, $\left(A^\top A\right)^\top=A^\top A$. Ensuite, si $X\in \R^n\backslash\{0\}$,\\
			$X^\top A^\top AX=\norme{AX}^2$ pour $\norme{.}:Y\in \R^n\mapsto Y^\top Y$ la norme euclidienne usuelle sur $\R^n$.\\
			Or $A\in \text{GL}_n(\R)$ donc $X\ne 0\implies AX\ne 0\implies \norme{AX}^2>0$. Ainsi $A^\top A$ est une matrice symétrique définie positive.
			\item Si $A=OS$ avec $(O,S)\in \O_n(\R)\times S_n^{++}(\R)$ alors $A^\top A=S^\top O^\top OS=S^\top S=S^2$. Donc $S$ est l'unique matrice de $S_n^{++}(\R)$ qui vérifie $A^\top A=S^2$. On a alors forcément $O=AS^{-1}$.\\
			On calcule $O^\top O=\left(S^{-1}\right)^\top A^\top AS^{-1}=\left(S^\top\right)^{-1}S=S^{-1}S=I_n$ donc $O$ est bien une matrice orthogonale.
		\end{enumerate}
		\item On sait que GL$_n(\R)$ est dense dans $\M_n(\R)$ (cf. VIII-53 tome Analyse). On peut donc poser une suite $(A_k)_{k\in \N}$ de matrice inversible qui converge vers $A$. Pour tout $k\in \N$ on note $A_k=O_kS_k$ la décomposition polaire de $A_k$.\\
		Comme $\O_n(\R)$ est compact on peut extraire une suite $(O_{\varphi(k)})_{k\in \N}$ de $(O_k)_{k\in \N}$ qui converge. On note $O$ sa limite.\\
		$\forall k\in \N,\ S_{\varphi(k)}=O_{\varphi(k)}^{-1}A_{\varphi(k)}$.\\
		On sait que l'application $M\in \M_n(\R)\mapsto M^{-1}$ est continue puisque les coefficients de $M^{-1}$ sont polynomiaux en ceux de $M$. De plus $A_{\varphi(k)}\ukfty{\longrightarrow}A$ comme suite extraite de $(A_k)_{k\in \N}$. Enfin l'application $(M,N)\in \M_n(\R)^2\mapsto MN$ est continue car bilinéaire sur $\M_n(\R)$ qui est de dimension finie.\\
		Ainsi par passage à la limite, $S_{\varphi(k)}\ukfty{\longrightarrow}O^{-1}A=S$.\\
		Or $(S_{\varphi(k)})_{k\in \N}\in S_n^+(\R)^\N$. On sait que si $X\in \R^n$ alors $\forall k\in \N,\ X^\top S_{\varphi(k)}X\geq 0$ et $S_{\varphi(k)}^\top=S_{\varphi(k)}$. Donc comme les applications $M\in \M_n(\R)\mapsto X^\top MX$ et $M\in \M_n(\R)\mapsto M^\top$ sont continues car linéaires, on obtient par passage à la limite : $X^\top SX\geq 0$ et $S^\top=S$. C'est à dire $S\in S_n^+(\R)$.\\
		Finalement, $A=OS$ où $(O,S)\in \O_n(\R)\times S_n^+(\R)$.\\
		Il n'y a pas unicité. Par exemple la matrice nulle peut s'écrire $0_n=I_n\times 0_n=-I_n\times 0_n$ où $(I_n,-I_n)\in \O_n(\R)^2$ et $0_n\in S_n^+(\R)$.
	\end{enumerate}
	
	\subsubsection{Sous-groupe compact maximal de GL$_n(\R)$ \etoile{3}}
	\begin{enumerate}[leftmargin=*]
		\item Soit $A\in \M_n(\R)$. Remarquons que $A^\top A\in S_n^+(\R)$. En effet, $A^\top A$ est clairement symétrique et $\forall X\in \R^n,\ X^\top A^\top AX=\normep{2}{AX}^2\geq 0$. On note alors $\lambda_1,\dots,\lambda_n$ les valeurs propres de $A^\top A$ comptées avec multiplicités et $e_1,\dots,e_n$ des vecteurs propres associés formant une base de $\R^n$. On sait que $\forall i\in \crblanc{1}{n},\ \lambda_i\in \R_+$. Fixons $X=\displaystyle\sum_{i=1}^na_ie_i\in \R^n$ tel que $\normep{2}{X}=1$. On a\\
		$$0\leq \normep{2}{AX}^2=X^\top A^\top AX=\sum_{i=1}^n\lambda_ia_i^2\leq \rho(A^\top A)\sum_{i=1}^na_i^2=\rho(A^\top A)\normep{2}{X}^2=\rho(A^\top A)$$
		De plus, en notant $i_0\in \crblanc{1}{n}$ tel que $\lambda_{i_0}=\rho(A^\top A)$ on a $X^\top e_{i_0}X=\rho(A^\top A)$.\\
		Ainsi ${|||A|||}_2=\sqrt{\rho(A^\top A)}$.
		\item $\O_n(\R)$ est un sous-groupe de GL$_n(\R)$ :
		\begin{itemize}
			\item $\O_n(\R)\subset \text{GL}_n(\R)$ ($\forall M\in \O_n(\R),\ M^{-1}=M^\top$);
			\item $I_n\in \O_n(\R)$;
			\item $\forall (A,B)\in \O_n(\R)^2,\ (AB^{-1})^\top AB^{-1}=BA^\top AB^\top=BB^\top=I_n$.
		\end{itemize}
		$\O_n(\R)$ est compact :
		\begin{itemize}
			\item $\O_n(\R)\subset \M_n(\R)$ avec $\dim(\M_n(\R))=n^2<+\infty$;
			\item $\O_n(\R)=f^{-1}(\{I_n\})$ pour $f:M\mapsto M^\top M$ qui est continue car les coefficients de $f(M)$ sont polynomiaux en ceux de $M$. Donc $\O_n(\R)$ est fermé;
			\item $\forall M\in \O_n(\R),\ {|||M|||}_2=\sup\limits_{\substack{X\in \R^n\\\normep{2}{X}=1}}\normep{2}{MX}=\sup\limits_{\substack{X\in \R^n\\\normep{2}{X}=1}}\normep{2}{X}=1$.
		\end{itemize}
		$\O_n(\R)$ est un fermé borné de $\M_n(\R)$ qui est de dimension finie, c'est donc un compact.
		\item \begin{enumerate}[label=\alph*.]
			\item Supposons que $S$ ait une valeur propre $\lambda>1$.\\
			Comme $G$ est un groupe qui contient $\O_n(\R)$, $\forall n\in \N^*,\ S^n=(O^{-1}A)^n\in G$.\\
			Nonobstant, $\forall n\in \N^*,\ \lambda^{2n}\in \text{Sp}(S^{2n})\implies \lambda^{2n}\leq \rho(S^{2n})={|||S^n|||}_2$. Ainsi ${|||S^n|||}_2\unfty{\longrightarrow}+\infty$ ce qui est absurde puisque $G$ est borné.
			\item Supposons que $S$ ait une valeur propre $\lambda<1$. Alors comme $G$ est un groupe qui contient $\O_n(\R)$, $S^{-1}=A^{-1}O\in G$. Or $\displaystyle\frac{1}{\lambda}\in \text{Sp}(S^{-1})$ et $\displaystyle\frac{1}{\lambda}>1$. Par le même raisonnement qu'à la question précédente ceci est absurde. On en déduit que Sp$(S)=\{1\}$.
		\end{enumerate}
		\item Le théorème spectral donne que $S$ est diagonalisable. On a donc d'après la question précédente $S=I_n$.\\
		Ainsi $A=O\in \O_n(\R)$ et finalement $G=\O_n(\R)$.
	\end{enumerate}
	
	\subsubsection{Enveloppe convexe des matrices orthogonales}
	\begin{enumerate}[leftmargin=*]
		\item Remarquons que $A^\top A\in S_n^+(\R)$. En effet, $A^\top A$ est clairement symétrique et $\forall X\in \M_{n,1}(\R),\ X^\top A^\top AX=\norme{2}{AX}^2\geq 0$. On note alors $\lambda_1,\dots,\lambda_n$ les valeurs propres de $A^\top A$ comptées avec multiplicités et $e_1,\dots,e_n$ des vecteurs propres associés formant une base de $\M_{n,1}(\R)$. On sait que $\forall i\in \crblanc{1}{n},\ \lambda_i\in \R_+$. Fixons $X=\displaystyle\sum_{i=1}^na_ie_i\in \M_{n,1}(\R)$ tel que $\normep{2}{X}=1$. On a\\
		$$0\leq \normep{2}{AX}^2=X^\top A^\top AX=\sum_{i=1}^n\lambda_ia_i^2\leq \rho(A^\top A)\sum_{i=1}^na_i^2=\rho(A^\top A)\normep{2}{X}^2=\rho(A^\top A)$$
		De plus, en notant $i_0\in \crblanc{1}{n}$ tel que $\lambda_{i_0}=\rho(A^\top A)$ on a $X^\top e_{i_0}X=\rho(A^\top A)$.\\
		Ainsi ${|||A|||}_2=\sqrt{\rho(A^\top A)}$.
		\item Soit $A\in \text{Conv}(\O_n(\R))$. Il existe $A_1,\dots,A_p$ des matrices orthogonales ainsi que $t_1,\dots,t_p$ des réels positifs de somme $1$ tels que $A=\displaystyle\sum_{i=1}^pt_iA_i$.\\
		On remarque que $\forall M\in \O_n(\R),\ {|||M|||}_2=\sup\limits_{\substack{X\in \M_n(\R)\\\normep{2}{X}=1}}\normep{2}{MX}=\sup\limits_{\substack{X\in \M_n(\R)\\\normep{2}{X}=1}}\normep{2}{X}=1$.\\
		Donc par inégalité triangulaire ${|||A|||}_2\leq \displaystyle\sum_{i=1}^pt_i{|||A_i|||}_2=\sum_{i=1}^pt_i=1$ d'où $A\in \B$.
		\item \begin{enumerate}[label=\alph*.]
			\item Soit $V\in \text{Conv}(\O_n(\R))$.\\
			D'une part, $\Tr(AV)-\Tr(AN)=\Tr(A(V-N))=\proscal{M-N}{V-N}\leq 0$ d'après le théorème de projection sur un convexe compact.\\
			D'autre part, $\Tr(AM)-\Tr(AN)=\Tr((M-N)^\top(M-N))=\proscal{M-N}{M-N}\geq 0$. De plus $N\in \text{Conv}(\O_n(\R))$ et $M\notin \text{Conv}(\O_n(\R))$ donc $M\ne N$ d'où $\proscal{M-N}{M-N}>0$.
			\item D'après la question précédente, pour $V=U^{-1}\in \O_n(\R)\subset \text{Conv}(\O_n(\R)),\ \Tr(S)=\Tr(U^{-1}US)=\Tr(U^{-1}A)\leq \Tr(AN)<\Tr(AM)=\Tr(USM)$.
			\item D'après le théorème spectral il existe une base $(e_1,\dots,e_n)$ de $\M_{n,1}(\R)$ formée de vecteurs propres de $S$. On note $\lambda_i$ la valeur propre associée à $e_i$.\\
			
		\end{enumerate}
	\end{enumerate}
	
	\subsubsection{Points extrémaux des matrices orthogonales}
	\subsubsection{Matrice extraite d'une matrice orthogonale}
	\subsubsection{Matrices qui respectent le volume de $k$-parallélépipèdes rectangles}
	
	\subsection{Décomposition QR}
	\label{Décomposition QR corrigé}
		\textcolor{blue}{\hyperref[Décomposition QR]{[Enoncé]}}\\
	
	\subsubsection{Matrice de Householder}
	
	
	\subsection{Décomposition LU}
	\label{Décomposition LU corrigé}
		\textcolor{blue}{\hyperref[Décomposition LU]{[Enoncé]}}\\
	
	\subsubsection{$\K=\R$ ou $\C$} 
	
	
	\subsubsection{$\K=\Z/p\Z$}
	
	
	\subsubsection{Algorithme de calcul}
	
	
	\subsubsection{Décomposition de Cholesky}
	
	\subsection{Lemme de Fitting}
	\label{Lemme de Fitting corrigé}
	\textcolor{blue}{\hyperref[Lemme de Fitting]{[Enoncé]}}\\
	\begin{enumerate}[leftmargin=*]
		\item On écrit le polynôme caractéristique de $u$ \[\chi_u=X^p\times Q\] de sorte que $0$ n'est pas racine de $Q$.\\
		Ainsi, $X^p\wedge Q=1$, donc d'après le lemme des noyaux, \[E=\Ker(u^p)\oplus\Ker(Q(u))\]
		Montrons que $\Ker(Q(u))=\Ima(u^p)$.\\
		Comme $\Ker(Q(u))$ est stable par $u^p$, et $\Ker(u^p)\cap\Ker(Q(u))$, l'endomorphisme induit $u^p_{\Ker(Q(u))}$ est injectif donc réalise un isomorphisme de $\Ker(Q(u))$ dans lui-même.\\
		D'où $\Ker(Q(u))=u^p(\Ker(Q(u)))\subset \Ima(u^p)$.\\
		De plus, d'après le théorème du rang, on en déduit que $\Ker(Q(u))=\Ima(u^p)$.\\
		Et donc \[E=\Ker(u^p)\oplus \Ima(u^p)\]
		\item On pose $u$ l'endomorphisme canoniquement associé à $M$.\\
		D'après la question précédente,il existe $p\in\N$ tel que $E=\Ker(u^p)\oplus\Ima(u^p)$.\\
		On pose $F=\Ker(u^p)$ et $G=\Ima(u^p)$.\\
		Montrons que $F$ et $G$ sont stables par $u$.\\
		Puisque $u$ commute avec $u^p$, on en déduit que $F$ est stable par $u$.\\
		De même puisque $u$ commute avec $Q(u)$ (cf. question 1), on a $G$ est stable par $u$.\\
		Par conséquent, la matrice $u$ dans la base $\B$ adaptée à $E=F\oplus G$ est de la forme \[\begin{pmatrix}
			N&0\\0&P
		\end{pmatrix}\]
		Montrons que $N$ est nilpotente.\\
		Pour tout $x\in F$, $u^p(x)=0$.\\
		Donc $u^p=0$ sur $F$, par conséquent $N$ est nilpotente.\\
		Montrons que $P$ est inversible.\\
		On rappelle que $\Ima(u^p)=\Ker(Q(u))$.\\
		Puisque $0$ n'est pas racine de $Q$, on en déduit aisément que $u_{|G}$ est inversible, donc $P$ l'est également.\\
		Finalement, on a montré que $M$ est semblable à une matrice de la forme $\begin{pmatrix}
			N&0\\0&P
		\end{pmatrix}$ où $N$ est nilpotente et $P$ est inversible.		
		 
	\end{enumerate}
	\subsection{Réduction de Jordan}
	\label{Réduction de Jordan corrigé}
		\textcolor{blue}{\hyperref[Réduction de Jordan]{[Enoncé]}}\\
	\begin{enumerate}
		\item On a : $\displaystyle\chi_M=\prod_{\lambda\in\text{Sp}(M)}(X-\lambda)^{m_\lambda}$.
		\\Les polynômes $((X-\lambda)^{m_\lambda})_{\lambda\in\text{Sp}(M)}$ sont premiers deux à deux. \\Donc d'après le lemme des noyaux, $\displaystyle\M_{n,1}(\K)=\bigoplus_{\lambda\in\text{Sp}(M)}F_\lambda$.
		\item Soit $(\alpha_0,\dots,\alpha_{p-1})\in\K^p$ tel que $$\sum_{i=0}^{p-1}\alpha_if^i(x)=0$$
		On note $d$ le minimum de l'ensemble $\left\{ i\in\crblanc{0}{p-1} \, \alpha_i\ne0\right\}$. Ce minimum existe car cet ensemble est une partie de $\N$.
		\\ On a donc $\displaystyle\sum_{i=d}^{p-1}\alpha_if^i(x)=0$.
		En composant par $f^{p-1-d}$, on a $\alpha_df^{p-1}(x)=0$. \\Ainsi $\alpha_d=0$, ce qui est absurde par définition de $d$.
		\\Donc la famille $\mathcal{F}$ est libre.
		On a : $$\text{Mat}_\mathcal{F}(f)=J_p(0)$$
		\item Soit $M\in\M_n(\K)$ de polynôme caractéristique $\displaystyle\chi_M=\prod_{\lambda\in\text{Sp}(M)}(X-\lambda)^{m_\lambda}$.
		\\ On note $f$ l'endomorphisme canoniquement associé à $M$.
		\\On remarque que $\varphi_\lambda=f_{|F_\lambda}-\lambda \text{Id}$ est nilpotent d'indice $m_\lambda$ sur $F_\lambda$ pour tout $\lambda\in\text{Sp}(M)$.
		\\ Donc $(\varphi_\lambda^{p-1}(x),\dots,\varphi_\lambda (x),x)$ est une base de $F_\lambda$. On a donc : $\text{Mat}(\varphi_\lambda)=J_p(0)$ donc $\text{Mat}(f_{|F_\lambda})=J_p(\lambda)$.
		\\On en déduit que $M$ est semblable à une matrice diagonale par blocs de Jordan.
	\end{enumerate}
	\subsubsection{Application}
	
	\subsection{Réduction de Frobenius}
	\label{Réduction de Frobenius corrigé}
	\textcolor{blue}{\hyperref[Réduction de Frobenius]{[Enoncé]}}\\
	\subsubsection{$M\sim M^\top$}
	\subsubsection{Matrices semblables à leur inverse}
	\subsubsection{Bicommutant}
	
	\newpage
\section{Correction Divers}

\subsection{Fonction $\R$-linéaire mais pas $\C$-linéaire \etoile{1}}
\label{Fonction R linéaire mais pas C linéaire corrigé}
\textcolor{blue}{\hyperref[Fonction R linéaire mais pas C linéaire]{[Enoncé]}}\\
On pose $\fonction{f}{\C}{\C}{z}{\overline{z}}$.\\
$\forall x,y\in \C,\ \forall \lambda\in \R,\ f(x+\lambda y)=\overline{x+\lambda y}=\overline{x}+\overline{\lambda}\overline{y}=f(x)+\lambda f(y)$.\\
Ainsi $f$ est $\R$-linéaire.\\
Nonobstant $f(i\times 1)=-i\ne i=i\times f(1)$ donc $f$ n'est pas $\C$-linéaire.

\subsection{Relation d'ordre}
\label{Relation d'ordre corrigé}
\textcolor{blue}{\hyperref[Relation d'ordre]{[Enoncé]}}\\

\subsection{Ordre lexicographique}
\label{Ordre lexicographique corrigé}
\textcolor{blue}{\hyperref[Ordre lexicographique]{[Enoncé]}}
\subsection{Sous-groupes de GL$_n(\C)$ d'exposant fini \etoile{5}}
\label{Sous-groupes de Gl_n(C) d'exposant fini corrigé}
	\textcolor{blue}{\hyperref[Sous-groupes de GL_n(C) d'exposant fini]{[Enoncé]}}\\
Soit $M\in G$.\\
$M^k=I_n$ donc les valeurs propres de $M$ sont incluses dans l'ensemble des racines de $X^k-1$ c'est à dire $\U_k$. On remarque aussi que $M$ est diagonalisable ce qui servira plus tard.\\
Alors $T=\{\Tr(M),\ M\in G\}=\displaystyle\left\{\sum\limits_{\lambda\in \text{Sp}(M)}m_\lambda(M)\lambda,\ M\in G\right\}\subset \left\{\sum\limits_{p=1}^n\lambda_p,\ (\lambda_1,\dots,\lambda_n)\in \U_k^n\right\}$.\\
Ce dernier ensemble étant fini car $\U_k$ l'est, $T$ est fini.\\\\
Ensuite, Considérons $V=\Vect(G)$. $V$ est de dimension finie comme sous-espace vectoriel de $\M_n(\C)$ on peut donc en prendre une base $(M_1,\dots,M_d)$.\\
Posons alors l'application $\fonction{f}{G}{T^d}{M}{(\Tr(MM_1),\dots,\Tr(MM_d))}$ et montrons que $f$ est injective.\\
Tout d'abord, $f$ est bien une application. En effet, $\forall M\in G,\ \forall p\in \crblanc{1}{d},\ M_p\in G\implies MM_p\in G$.\\
Fixons un couple de matrice $(A,B)\in G^2$ tel que $f(A)=f(B)$ c'est à dire $\forall p\in \crblanc{1}{d},\ \Tr(AM_p)=\Tr(BM_p)$.\\
Alors par linéarité de la trace, pour toute combinaison linéaire $M=\displaystyle\sum\limits_{p=1}^dx_pM_p,\ \Tr(AM)=\Tr(BM)$.\\
En particulier pour $B^{-1}\in G\subset V,\ \Tr(AB^{-1})=\Tr(I_n)=n$.\\
Supposons alors par l'absurde que $A\ne B$ et donc que $C=AB^{-1}\ne I_n$. Alors le spectre de $C$ n'est pas réduit à $1$ (car sinon, $C$ étant diagonalisable on aurait $C=I_n$). On note $\lambda_0$ une valeur propre de $C$ différente de $1$.\\
On a $\operatorname{Re}(\Tr(C))=\operatorname{Re}(\lambda_0)+\displaystyle\sum\limits_{\lambda\in \text{Sp}(C)\backslash\{\lambda_0\}}\operatorname{Re}(\lambda)\leq \operatorname{Re}(\lambda_0)+\sum\limits_{\lambda\in \text{Sp}(C)\backslash\{\lambda_0\}}1=\operatorname{Re}(\lambda_0)+n-1<n$.\\
Ceci est absurde donc $A=B$.\\
On a donc construit une injection de $G$ dans $T^d$ qui est un ensemble fini. On en déduit que $G$ est fini.
	
	
\subsection{Formule de Burnside \etoile{4}}
\label{Formule de Burnside corrigé}
	\textcolor{blue}{\hyperref[Formule de Burnside]{[Enoncé]}}\\
	Toutes les sommes écrites sont des sommes \textbf{finies}.\\
	Posons $\varphi=\displaystyle\frac{1}{|G|}\sum\limits_{g\in G}g$. On veut montrer que $\Tr(\varphi)=\dim(V^G)$.\\
	Le résultat paraît improbable dans le sens où $\Tr(\varphi)\in \K$. Et pourtant on doit montrer que $\Tr(\varphi)$ est un entier positif. Parmi les endomorphismes connus on sait que les projecteur ont une trace entière (égale à leur rang). On va donc essayer de montrer que $\varphi$ est un projecteur.\\
	On calcule $\varphi^2=\displaystyle\left(\frac{1}{|G|}\sum\limits_{g\in G}g\right)\left(\frac{1}{|G|}\sum\limits_{h\in G}h\right)=\frac{1}{|G|^2}\sum\limits_{g\in G}\sum\limits_{h\in G}gh$.\\
	Or pour tout $g\in G$ l'application $f_g:h\in G\mapsto gh$ est bijective. En effet, sa bijection réciproque est $f_{g^{-1}}$.\\
	Donc $\varphi^2=\displaystyle\frac{1}{|G|^2}\sum\limits_{g\in G}\sum\limits_{h\in G}f_g(h)=\frac{1}{|G|^2}\sum\limits_{g\in G}\sum\limits_{h\in G}h=\frac{1}{|G|^2}\sum\limits_{h\in G}\sum\limits_{g\in G}h=\frac{1}{|G|}\sum\limits_{h\in G}h=\varphi$.\\
	On sait alors que $\Tr(\varphi)=\rg(\varphi)$ et que Im$(\varphi)=\text{Ker}(\varphi-\Id_V)$.
	Ensuite si $x\in V^G$ alors $\varphi(x)=\displaystyle\frac{1}{|G|}\sum\limits_{g\in G}g(x)=\frac{1}{|G|}\sum\limits_{g\in G}x=x$ c'est à dire $x\in \text{Ker}(\varphi-\Id_V)$. Donc $V^G\subset \text{Im}(\varphi)$.\\
	Réciproquement, donnons nous $x\in \text{Im}(\varphi)$ et fixons $h\in G$.\\
	On a $x=\displaystyle\frac{1}{|G|}\sum\limits_{g\in G}g(x)$ donc $h(x)=\displaystyle\frac{1}{|G|}\sum\limits_{g\in G}f_h(g)(x)=\frac{1}{|G|}\sum\limits_{g\in G}g(x)=x$.\\
	Ainsi Im$(\varphi)=V^G$.\\
	Finalement $\dim(V^G)=\rg(\varphi)=\Tr(\varphi)=\displaystyle\frac{1}{|G|}\sum\limits_{g\in G}\Tr(g)$.
	
\subsection{Caractères algébriques de $GL_n(\K)$}
\label{Caractères algébriques de GLn(K) corrigé}
\textcolor{blue}{\hyperref[Caractères algébriques de GLn(K)]{[Enoncé]}}\\
D'après l'énoncé, $\exists R\in \K[X_1,\dots,X_{n^2}],\ \forall A=(a_{ij})_{1\leq i,j\leq n}\in \T{GL}_n(\K),\ \chi(A)=R((a_{ij})_{1\leq i,j\leq n})$.\\
On sait que GL$_n(\K)$ est engendré par les matrices de dilatations $D_n(\lambda)=\left(\begin{array}{c|ccc}
	\lambda & 0 & \cdots & 0\\
	\hline
	0 & & &\\
	\vdots& & I_{n-1} &\\
	0 & & &
\end{array}\right),\lambda\in \K^*$ et les matrices de transvections $T_{ij}(\lambda)=I_n+\lambda E_{ij},\ \lambda\in \K,i\ne j$. Il suffit donc de montrer que $\chi$ coïncide avec une puissance du déterminant sur ces matrices.
\begin{itemize}
	\item Fixons $\lambda\in \K^*$.\\
	On note $P=\chi(D_n(X))\in \K[X]$ et on note $d$ son degré. $P$ n'est pas le polynôme nul puisque $P(1)=\chi(I_n)=1$ donc $d$ est un entier naturel.\\
	$\forall \lambda,\mu\in \K^*,\ P(\lambda\mu)=\chi(D_n(\lambda\mu))=\chi(D_n(\lambda)D_n(\mu))=\chi(D_n(\lambda))\chi(D_n(\mu))=P(\lambda)P(\mu)$. Donc comme $\K$ est infini, $P(X)P(\mu)=P(\mu X)$. Par dérivation successive on obtient $P(\mu)P^{(d)}(X)=\mu^d P^{(d)}(\mu X)$. Or $P^{(d)}$ est un polynôme constant non nul, donc $\forall \mu\in \K^*,\ P(\mu)=\mu^d$.\\
	Autrement dit $\exists d\in \N,\ \forall \lambda\in \K^*,\ \chi(D_n(\lambda))=\lambda^d=\det(D_n(\lambda))^d$.
	\item Fixons $i\ne j\in \crblanc{1}{n}$ et $\lambda,\mu\in \K$.\\
	On note $Q_{ij}=\chi(T_{ij}(X))\in \K[X]$. $T_{ij}(\lambda)T_{ij}(\mu)=I_n+(\lambda+\mu)E_{ij}=T_{ij}(\lambda+\mu)$. On en déduit la relation $Q_{ij}(X+\mu)=Q_{ij}(\mu)Q_{ij}(X)$. Or le coefficients dominants de $Q_{ij}(X+\mu)$ et $Q_{ij}(X)$ sont égaux, donc $Q_{ij}(\mu)=1$. Ainsi $\forall \lambda\in \K,\ \forall i\ne j\in\crblanc{1}{n},\ \chi(T_{ij}(\lambda))=1=\det(T_{ij}(\lambda))^d$.
\end{itemize}
Finalement, $\chi=\det^d$.\\

Pour $\K=\C$ on peut donner la preuve alternative suivante.
\begin{itemize}
	\item Fixons $\lambda\in \C^*$.\\
	On note $P=\chi(D_n(X))\in \C[X]$. $\forall \lambda\in \C^*,\ \chi(D_n(\lambda))=P(\lambda)\ne 0$. Donc $P$ n'a pas de racine non nulle. Si $P$ n'est pas constant alors $\exists d\in \N^*,\ P=X^d$. Sinon, comme $P(1)=\chi(I_n)=1,\ P=1$.
	\item Fixons $i\ne j\in \crblanc{1}{n}$ et $\lambda\in \C$.\\
	On note $Q_{ij}=\chi(T_{ij}(X))\in \C[X]$. $\forall \lambda\in \C,\ \chi(T_{ij}(\lambda))=Q_{ij}(\lambda)\ne 0$. Donc $Q_{ij}$ n'a pas de racine, c'est donc un polynôme constant. Comme $Q_{ij}(0)=\chi(I_n)=1$, $Q_{ij}=1$.
\end{itemize}

\textit{Remarque : La première preuve qu'on a donné fonctionne pour n'importe quel corps infini. Le résultat est aussi vrai sur les corps finis et ce montre toujours en passant par les matrices de transvections/dilatations, en montrant au préalable qu'elles engendrent toujours le groupe linéaire lorsque $\K$ est un corps quelconque.}

\subsection{Théorème de Fermat matriciel}
	\label{Théorème de Fermat matriciel corrigé}
	\textcolor{blue}{\hyperref[Théorème de Fermat matriciel]{[Enoncé]}}\\
	
\subsection{Développement décimal propre d'un réel}
	\label{Développement décimal propre d'un réel corrigé}
	\textcolor{blue}{\hyperref[Développement décimal propre d'un réel]{[Enoncé]}}\\
\subsubsection{Une caractérisation des rationnels}
	
\subsection{Distribution du premier chiffre des puissances de 2}
	\label{Distribution du premier chiffre des puissances de 2 corrigé}
	\textcolor{blue}{\hyperref[Distribution du premier chiffre des puissances de 2]{[Enoncé]}}\\
	
	
	
	
	
	
	
\end{document}